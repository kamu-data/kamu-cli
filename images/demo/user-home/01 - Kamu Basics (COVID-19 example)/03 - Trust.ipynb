{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"https://github.com/kamu-data/kamu-cli\">\n",
    "<img alt=\"kamu\" src=\"https://raw.githubusercontent.com/kamu-data/kamu-cli/master/docs/readme_files/kamu_logo.png\" width=270/>\n",
    "</a>\n",
    "</center>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div align=\"center\">\n",
    "<a href=\"https://docs.kamu.dev/cli/\">Docs</a> | \n",
    "<a href=\"https://docs.kamu.dev/cli/learn/learning-materials/\">Tutorials</a> | \n",
    "<a href=\"https://docs.kamu.dev/cli/learn/examples/\">Examples</a> |\n",
    "<a href=\"https://docs.kamu.dev/cli/get-started/faq/\">FAQ</a> |\n",
    "<a href=\"https://discord.gg/nU6TXRQNXC\">Discord</a> |\n",
    "<a href=\"https://kamu.dev\">Website</a>\n",
    "</div>\n",
    "\n",
    "\n",
    "<center>\n",
    "\n",
    "<br/>\n",
    "    \n",
    "# 3. Trustworthiness of Data\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "If you skipped the previous chapter or continuing after a break, use the following commands to get your environment ready for this chapter:\n",
    "    \n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\">cd \"01 - Kamu Basics (COVID-19 example)\"\n",
    "./init-chapter-3.sh\n",
    "</code>\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem\n",
    "So far we've built a simple but very useful data supply chain, parts of which could be **owned and maintained by different people and organizations**. As long as publishers continue to update source data, we saw how easy it is to keep all of our datasets up-to-date. The ODF protocol is in fact designed to achieve <mark>near real-time latency</mark>.\n",
    "\n",
    "But getting data fast is not the only concern. When multiple parties are involved in publishing and transforming data there is a big question of whether you can **trust** the data you are receiving. When working with data in a decentralized setting you have to account for the worst case - **malicious intent**.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Short on time?</b> See <a href=\"https://www.youtube.com/watch?v=hN_vpHYmwi0&list=PLV91cS45lwVG20Hicztbv7hsjN6x69MJk&index=2\">this video</a> for a quick explainer of trust and collaboration.\n",
    "</div>\n",
    "\n",
    "After incidents like [the Surgisphere scandal](https://www.the-scientist.com/features/the-surgisphere-scandal-what-went-wrong--67955) (where publication based on fake medical data derailed drug research worldwide) the sentiment is changing from assuming that all research is done in good faith, to <mark>considering any research unreliable until proven otherwise</mark>.\n",
    "\n",
    "This is why we've built `kamu` with <mark>complete reproducibility and verifiability</mark> in mind. In this section, we will see how to assess the trustworthiness of any dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Datasets\n",
    "Previously, you've seen the clear separation `kamu` makes between root and derivative datasets. Let's see how trust works in each of them separately, starting with root datasets and source data.\n",
    "\n",
    "### Reproducibility & Verifiability\n",
    "In our example, we used root datasets that directly fetch the COVID-19 data from government websites. Why are we duplicating the data instead of using it directly?\n",
    "\n",
    "In general, data sources like websites should be considered **non-reproducible**:\n",
    "- The website can be down or **unreachable**\n",
    "- Publishers do not provide us any clear guarantees around **availability** of data, so the dataset may disappear at any time\n",
    "- There is no clarity around **preserving history**, so publisher may (intentionally or by accident) destructively update data and alter history\n",
    "\n",
    "We could make some assumptions on a case-by-case basis, but what we really want is a **clear contract** on how data is preserved.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Destructive updates and not preserving history is unfortunately the case for a very large portion of data publishers today. For example http://geonames.org (one of the most widely used GIS data publishers) update their datasets daily by **overwriting** the old data. So the data you download today will be different from the one you download tomorrow. \n",
    "    \n",
    "This makes **copying and versioning data** the only way to share a **stable data reference** with your colleagues and make your data project reproducible, but by doing so you lose the ability to verify that input data actually came from the trusted publisher.\n",
    "\n",
    "</div>\n",
    "\n",
    "The `kamu`'s contract that is enforced by root datasets is simple - **complete reproducibility & verifiability** of source data. It provides all the above guarantees and the certainty that the data observed today will still be available tomorrow, that the history will not be altered, and that you can easily check \"*did this data actually came from this publisher?*\".\n",
    "\n",
    "\n",
    "#### In Practice\n",
    "\n",
    "Since dataset is a ledger - a single metadata block hash is sufficient to create a **stable reference** for data and ensure **preproducibility**. Same effect can be achieved using a fixed position in a ledger (as an offset or using an upper bound for the `system_time` column).\n",
    "\n",
    "<!--div class=\"alert alert-block alert-danger\">\n",
    "// TODO - show how to work with stable references\n",
    "</div-->\n",
    "\n",
    "Since metadata chain is secured cryptographically, to verify that it's authentic it is sufficient to validate its integrity (something that `kamu` always does automatically) and then ask publisher if they have a metadata block with this hash.\n",
    "\n",
    "<!--div class=\"alert alert-block alert-danger\">\n",
    "// TODO - show how to check metadata is authentic\n",
    "</div-->\n",
    "\n",
    "And since metadata chain contains hashes of all data files in the dataset - you can easily verify that your local copy of data corresponds to the metadata and was not tampered.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "To verify data that we previously downloaded from a repository run:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> kamu verify covid19.ontario.case-details\n",
    "</code>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "\n",
    "### Trustworthiness of Root Data\n",
    "While tools above can ensure that your data actually came from a certain publisher, they unfortunately cannot tell you <mark>whether the data itself is trustworthy</mark>.\n",
    "\n",
    "Remember that the source data is always under complete control of the owner of the root dataset, which means that measuring and processing errors and even malicious data can make its way in. Don't trust any random person presenting you data.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Take our first dataset for example (`covid19.british-columbia.case-details`). If you downloaded this root dataset from the repository and inspected metadata - you would see that it fetches data from the official source - the CDC website. That sounds trustworthy, right?\n",
    "    \n",
    "Not quite! The person who published the dataset **could've tampered with the data** when it was being pulled from CDC website. This problem disappears when organizations that publish data also own the corresponding root datasets in ODF, as it removes the middle man.\n",
    "    \n",
    "</div>\n",
    "\n",
    "The amount of trust you can put into the data should be equal to how much you can trust the publisher. You can measure this trust based on following factors:\n",
    "\n",
    "- First factor is <mark>**reputation**</mark>\n",
    "    - Your degree of trust may go up if you know the publisher personally\n",
    "    - Or see that other people in the community trust them\n",
    "    - An affiliation with the government, university, or some reputable organization would also improve their credibility\n",
    "- Another possibility is <mark>**audit**</mark>\n",
    "    - A third party can audit the methodology and tools used to collect data\n",
    "    - This method doesn't scale well and always leaves the question of \"who watches the watchmen?\"\n",
    "- A much better option is <mark>**cross-validation**</mark>\n",
    "    - You can find another independent publisher that provides similar data and see if their measurements are similar\n",
    "- A step above that is <mark>**outlier detection**</mark>\n",
    "    - You can create a derivative dataset that continuously relies on a group of independent publishers, compares their data, while discarding and possibly penalizing the outliers\n",
    "\n",
    "`kamu` helps you to keep track of which data publishers you depend on and lets you implement the above advanced techniques to improve reliability of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative Datasets\n",
    "As you saw before, `kamu` strictly follows \"Data as Code\" philosophy where you never modify data directly and only manipulate data through queries. This makes collaborating on data very similar to collaborating on software.\n",
    "\n",
    "Derivative data in `kamu` is inseparable from queries that produced it, because data provenance of which we cannot establish is useless as it cannot be trusted.\n",
    "\n",
    "The stream processing queries we discussed previously in fact have very **strict properties** - they are fully **deterministic and reproducible**. Meaning that the same query on same inputs will always produce the same result.\n",
    "\n",
    "Thanks to this, verification process takes just three steps:\n",
    "\n",
    "1. Understand **which root datasets you depend on** and whether you trust their publishers\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "For this you can use the already familiar lineage command:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> kamu inspect lineage covid19.canada.daily-cases\n",
    "</code>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "2. Audit the queries to make sure they are not malicious\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Use following command to see the queries used by a dataset:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> kamu inspect query covid19.canada.daily-cases\n",
    "</code>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "\n",
    "3. Verify that data you downloaded matches the declared transformations and was not tampered\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Done by the same command we used previously for root datasets:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> kamu verify covid19.canada.daily-cases\n",
    "</code>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "When called on a derivative dataset, in addition to comparing the hashes of data to metadata, the `verify` command does one extra thing - it **replays all the transformations locally** to guarantee that the resulting data was in fact produced by queries declared in the metadata chain. It re-executes the queries in the same sequence and on the same inputs as recorded in metadata, one block at a time.\n",
    "\n",
    "The last part relies on determinism and reproducibility of the queries.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Here's another interesting way to look at this:**\n",
    "    \n",
    "Because all queries are stored in metadata, and are deterministic, the entirety of derivative data can be deleted and fully re-created from only root datasets and metadata. This makes derivative data **a form of caching**! \n",
    "    \n",
    "This is a pretty amazing property that means that derivative data can be stored cheaply, unreliably, or even not stored at all, potentially resulting in massive savings as amounts of derivative data we produce on the path to insight can often exceed the volume of the original raw data.\n",
    "</div>\n",
    "\n",
    "Since the data pipelines can get quite long you can also use the `--recursive` flag to verify all derivative data starting from the root datasets:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> kamu verify --recursive covid19.canada.daily-cases\n",
    "</code>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Effect\n",
    "\n",
    "We just saw how `kamu` provides a simple step-by-step process to evaluate trustworthiness of data, enabling efficient reuse and collaboration. Most of this work is automated, but remember that the remaining manual bits are a <mark>**community effort**</mark>!\n",
    "\n",
    "Just like in open source software, you will always have <mark>an army of peers on your side</mark>, helping to establish trustworthiness of different publishers and auditing the derivative queries in complex transformation pipelines! Enabling the collaboration effect is the **true superpower** of `kamu`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Well Done! ðŸŽ‰\n",
    "\n",
    "This concludes the introductory tutorial!\n",
    "\n",
    "Next, we invite you to try the **\"Web3 Data\" tutorial** that introduces decentralized storage and more in-depth stream processing concepts.\n",
    "\n",
    "You can also explore our growing collection of:\n",
    "- [Examples](https://docs.kamu.dev/cli/learn/examples/)\n",
    "- [Learning materials](https://docs.kamu.dev/cli/get-started/learning-materials/)\n",
    "- and [external datasets](https://github.com/kamu-data/kamu-contrib).\n",
    "\n",
    "All examples are conveniently located in `~/XX - Other Examples` directory of this Jupyter server.\n",
    "\n",
    "Also, please tell us what you think about this demo:\n",
    "- by [joining our Discord](https://discord.gg/nU6TXRQNXC)\n",
    "- by creating an [issue](https://github.com/kamu-data/kamu-cli/issues)\n",
    "- or e-mailing us at [info@kamu.dev](mailto:info@kamu.dev)\n",
    "\n",
    "Thank you for checking out `kamu`!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

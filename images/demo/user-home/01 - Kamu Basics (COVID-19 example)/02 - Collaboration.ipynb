{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"https://github.com/kamu-data/kamu-cli\">\n",
    "<img alt=\"kamu\" src=\"https://raw.githubusercontent.com/kamu-data/kamu-cli/master/docs/readme_files/kamu_logo.png\" width=270/>\n",
    "</a>\n",
    "</center>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div align=\"center\">\n",
    "<a href=\"https://docs.kamu.dev/cli/\">Docs</a> | \n",
    "<a href=\"https://docs.kamu.dev/cli/learn/learning-materials/\">Tutorials</a> | \n",
    "<a href=\"https://docs.kamu.dev/cli/learn/examples/\">Examples</a> |\n",
    "<a href=\"https://docs.kamu.dev/cli/get-started/faq/\">FAQ</a> |\n",
    "<a href=\"https://discord.gg/nU6TXRQNXC\">Discord</a> |\n",
    "<a href=\"https://kamu.dev\">Website</a>\n",
    "</div>\n",
    "\n",
    "\n",
    "<center>\n",
    "\n",
    "<br/>\n",
    "    \n",
    "# 2. Sharing and Collaboration\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "If you skipped the previous chapter or continuing after a break, use the following commands to get your environment ready for this chapter:\n",
    "    \n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\">cd \"01 - Kamu Basics (COVID-19 example)\"\n",
    "./init-chapter-2.sh\n",
    "</code>\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repositories\n",
    "\n",
    "After you created your dataset, you might want to share it with other people. We can do so by using **repositories**.\n",
    "\n",
    "Repository can be just some storage, e.g.:\n",
    "- Cloud and on-prem like S3, GCS, or Minio\n",
    "- Decentralized storage like IPFS, Arweave (see next tutorial on \"Web3 Data\")\n",
    "- Or even some old FTP server (see [full list](https://docs.kamu.dev/node/deploy/storage/))\n",
    "\n",
    "As a reporitory for this demo we will use [**Kamu Node**](https://docs.kamu.dev/node/) - you can think of it as a small server on top of some storage (AWS S3 or Minio in this case) that speaks ORF protocol and provides a bunch of cool additional features, like highly optimized uploads/downloads, dataset search, and even executing remote SQL queries.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "So let's add the node as a repository:\n",
    "    \n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> kamu repo add kamu-node ${KAMU_NODE_URL}\n",
    "</code>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "To see the list of all known repositories use:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> kamu repo list\n",
    "</code>\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pushing data\n",
    "You happen to already have `write` access to the node, so let's try sharing our dataset.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>push</b> your local dataset into the repository (note that we are storing it under your personal account):\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> kamu push covid19.british-columbia.case-details --to kamu-node/${GITHUB_LOGIN}/covid19.british-columbia.case-details\n",
    "</code>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "This initial push creates an association between your local dataset and the repository, so next time you want to update the remote dataset you can simply run:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> kamu push covid19.british-columbia.case-details\n",
    "</code>\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discovering Data\n",
    "\n",
    "Using the `search` command, we can also search for datasets in known repositories.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Try searching for `covid` datasets:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> kamu search covid\n",
    "</code>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "Firstly, you should see the dataset you pushed in this list, under your account:\n",
    "\n",
    "`kamu-node/${GITHUB_LOGIN}/covid19.british-columbia.case-details`\n",
    "\n",
    "\n",
    "But more excitingly, you will also see some COVID-19 datasets added by other people, some related to other provinces of Canada, like:\n",
    "\n",
    "```kamu-node/kamu/covid19.ontario.case-details```\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Let's pull this dataset into our workspace, and have a look:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> kamu pull kamu-node/kamu/covid19.ontario.case-details\n",
    "</code>\n",
    "</p>\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> kamu tail covid19.ontario.case-details\n",
    "</code>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "Ah! This dataset is very similar to the dataset that we created. It also contains individual case data, but from Ontario province."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative Datasets\n",
    "\n",
    "Canada has 13 provinces and territories, so while we could run data analysis on each of them separately, things would be much simpler if we had one Canada-wide dataset.\n",
    "\n",
    "I'm sure other researchers would also appreciate having such dataset too, so let's create one!\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Let's start with checking the schema of both datasets:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> kamu inspect schema covid19.british-columbia.case-details\n",
    "</code>\n",
    "</p>\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> kamu inspect schema covid19.ontario.case-details\n",
    "</code>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "The datasets are similar but they do have somewhat different schemas, so it looks like we will need to **harmonize the data** to be able to combine them. We can do this using a **derivative dataset**, which we will define in another `.yaml` file.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Add the pre-made derivative dataset to your workspace with:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> kamu add datasets/canada.case-details.yaml\n",
    "</code>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "This is what this file looks like:\n",
    "\n",
    "```yaml\n",
    "kind: DatasetSnapshot\n",
    "version: 1\n",
    "content:\n",
    "  name: covid19.canada.case-details\n",
    "  kind: Derivative\n",
    "  # List of metadata events that get dataset into its initial state\n",
    "  metadata:\n",
    "    - kind: SetTransform\n",
    "      # References the datasets that will be used as sources of data.\n",
    "      inputs:\n",
    "        - datasetRef: covid19.british-columbia.case-details\n",
    "        - datasetRef: covid19.ontario.case-details\n",
    "      # Transformation that will be applied to produce new data\n",
    "      transform:\n",
    "        kind: Sql\n",
    "        engine: spark\n",
    "        query: |\n",
    "          SELECT\n",
    "            \"BC\" as province,\n",
    "            id,\n",
    "            reported_date,\n",
    "            sex as gender,\n",
    "            case when age_group = '<10' then '<20'\n",
    "                 when age_group = '10-19' then '<20' \n",
    "                 when age_group = '20-29' then '20s'\n",
    "                 when age_group = '30-39' then '30s'\n",
    "                 when age_group = '40-49' then '40s'\n",
    "                 when age_group = '50-59' then '50s'\n",
    "                 when age_group = '60-69' then '60s'\n",
    "                 when age_group = '70-79' then '70s'\n",
    "                 when age_group = '80-89' then '80s'\n",
    "                 when age_group = '90+' then '90+'\n",
    "                 else 'UNKNOWN' end as age_group,\n",
    "            ha as location\n",
    "            FROM `covid19.british-columbia.case-details`\n",
    "          UNION ALL\n",
    "          SELECT\n",
    "            \"ON\" as province,\n",
    "            id,\n",
    "            case_reported_date as reported_date,\n",
    "            case when lower(gender) = 'male' then 'M' \n",
    "                 when lower(gender) = 'female' then 'F' \n",
    "                 else 'U' end as gender,\n",
    "            age_group,\n",
    "            city as location\n",
    "            FROM `covid19.ontario.case-details`\n",
    "    - kind: SetVocab\n",
    "      eventTimeColumn: reported_date\n",
    "```\n",
    "\n",
    "Derivative datasets are used to transform, combine, enrich and aggregate data from multiple sources. Unlike root datasets, they work only with data that is already in the system to **guarantee reproducible/verifiable results**.\n",
    "\n",
    "In our case, the inputs are two root COVID-19 datasets from BC and Ontario. Our output is computed via an SQL query that harmonizes the dataset schemas and performs a `UNION ALL` operation.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Just like before, the dataset is empty until we pull it:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> kamu pull covid19.canada.case-details\n",
    "</code>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "This time, we are not fetching any external data while pulling. Instead, `kamu` will check which records from the input datasets have not been processed yet and it will feed them into the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping data up-to-date\n",
    "\n",
    "If you run the pull command again:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> kamu pull covid19.canada.case-details\n",
    "</code>\n",
    "</p>\n",
    "\n",
    "You will see that the dataset is up-to-date, because neither of its two inputs have changed.\n",
    "\n",
    "By adding the `--recursive` flag you can instruct `kamu` to check for updates in every dataset that is part of the dependency tree.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Try running:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> kamu pull --recursive covid19.canada.case-details\n",
    "</code>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "This operation will perform three steps:\n",
    "- For `covid19.british-columbia.case-details` - check the **external website** for updates\n",
    "- For `covid19.ontario.case-details` - check **the repository** for updates (since we pulled this dataset from a repo)\n",
    "- For `covid19.canada.case-details` - **apply the transformation** query to any new data in the above\n",
    "\n",
    "And just like that, <mark>with a single command you can keep a very large number of datasets fully up-to-date</mark>!\n",
    "\n",
    "With `kamu`, you are always only a few clicks away from getting the latest data into your projects, notebooks and dashboards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to Stream Processing!\n",
    "\n",
    "You might not have recognized it, but you've just been doing **stream processing**. \n",
    "\n",
    "As opposed to the traditional batch processing (e.g. classic SQL, Pandas, or most other frameworks) all processing in `kamu` is streaming in nature.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Have not used stream processing before?** It's a fairly young technology, but it's rapidly entering the data science and analytics space. A very simple overview of differences between streaming and batch can be found in [this talk recording](https://youtu.be/XxKnTusccUM?t=574).\n",
    "\n",
    "</div>\n",
    "\n",
    "To understand the benefits of this, let's create one more derivative dataset. This time we will use the Canada-wide dataset from the previous step to create the \"total daily new cases\" dataset, similar to the COVID statistics you used to hear on the news.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Add the pre-made derivative dataset to your workspace with:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> kamu add datasets/canada.daily-cases.yaml\n",
    "</code>\n",
    "</p>\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> kamu pull covid19.canada.daily-cases\n",
    "</code>\n",
    "</p>\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> kamu tail covid19.canada.daily-cases\n",
    "</code>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "This dataset is defined as:\n",
    "\n",
    "```yaml\n",
    "kind: DatasetSnapshot\n",
    "version: 1\n",
    "content:\n",
    "  id: covid19.canada.daily-cases\n",
    "  kind: Derivative\n",
    "  metadata:\n",
    "    - kind: SetTransform\n",
    "      inputs:\n",
    "        - datasetRef: covid19.canada.case-details\n",
    "      transform:\n",
    "        kind: Sql\n",
    "        engine: flink\n",
    "        query: |\n",
    "          SELECT\n",
    "            TUMBLE_START(`reported_date`, INTERVAL '1' DAY) as `reported_date`,\n",
    "            `province`,\n",
    "            COUNT(*) as `total_daily`\n",
    "          FROM `covid19.canada.case-details`\n",
    "          GROUP BY TUMBLE(`reported_date`, INTERVAL '1' DAY), `province`\n",
    "    - kind: SetVocab\n",
    "      eventTimeColumn: reported_date\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Notice that we already used **three different data processing engines**: DataFusion, Spark, and now Flink!\n",
    "    \n",
    "`kamu` allows you to use the individual strengths of [multiple different engines](https://docs.kamu.dev/cli/supported-engines/) and mix them within a single data pipeline.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "Calculating the daily cases per province is an **aggregation**. In a classic SQL we would write it as:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "  reported_date,\n",
    "  province,\n",
    "  COUNT(*) as total_daily\n",
    "FROM `covid19.canada.case-details`\n",
    "GROUP BY reported_date, province\n",
    "```\n",
    "\n",
    "But this type of query is flawed in many ways:\n",
    "- Data from different provinces may be updated on **different cadences** and may **lag** by one or several days (e.g. CDC of BC does not update their dataset on weekends and statutory holidays)\n",
    "- Data may be **out-of-order**, can be **back-filled**, or contain errors that were later **corrected**\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "Naively executing this batch query is guaranteed to constantly produce **inaccurate results**. What's worse, the errors will be concentrated in the most recent data - data everyone cares about the most.\n",
    "\n",
    "</div>\n",
    "\n",
    "Stream processing takes all these temporal problems into account and will delay producing the result before we have certainty that the input data is complete. This is an amazing property that ensures that as we build more and more complex data pipelines we don't end up creating a massive cascade of incorrect data.\n",
    "\n",
    "Stream processing is a truly fascinating topic. We will have to park it for now, but be sure to later check out our examples and tutorials on this topic to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lineage\n",
    "\n",
    "So things are getting a bit complicated. We have datasets building on top of datasets, building on yet more datasets...\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "If you ever lose your bearings - <b>lineage</b> command will help you out:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> kamu inspect lineage covid19.canada.daily-cases\n",
    "</code>\n",
    "</p>\n",
    "</div>\n",
    "    \n",
    "This command shows you a graph of datasets and their dependencies. Thanks to the metadata, `kamu` knows exactly where every single bit of data came from, so lineage is **guaranteed** to be accurate.\n",
    "\n",
    "When you install `kamu` on your machine, you can also display lineage in a browser by running:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> kamu inspect lineage --browse\n",
    "</code>\n",
    "</p>\n",
    "\n",
    "It will look something like this:\n",
    "\n",
    "![](files/lineage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kamu Web UI\n",
    "An even better option for exploring pipelines is [Kamu Web UI](https://docs.kamu.dev/platform/).\n",
    "\n",
    "With `kamu` installed on your machine you can run it as:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> kamu ui\n",
    "</code>\n",
    "</p>\n",
    "\n",
    "Kamu Node that we pushed and pulled data from also comes with this web interface!\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Follow this link to check it out:\n",
    "    \n",
    "${KAMU_WEB_UI_URL}kamu/covid19.canada.case-details?tab=lineage\n",
    "</div>\n",
    "\n",
    "There you will see a COVID data pipeline built by our community that is a bigger version of what we built in this exercise:\n",
    "\n",
    "![](files/web-ui.png)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The above is a taste how `kamu` embraces **\"local-first\" design**:\n",
    "- All features are available in a simple CLI tool\n",
    "- You can build really complex pipelines on your laptop, without entangling your data in proprietary data platforms\n",
    "- When your data becomes large - you can [**deploy your own Kamu Node**](https://docs.kamu.dev/node/quick-start/) in a cloud or on-prem and run your pipelines in a scalable data lake\n",
    "- You can then invite other people to your node or **connect multiple nodes together** to collaborate on data.\n",
    "    \n",
    "This flexibility makes `kamu` the **world's first decentralized data lake and multi-party data processing network**.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Up Next\n",
    "🎉 Well done! 🎉\n",
    "\n",
    "You have now discovered how `kamu` can be used to <mark>share</mark> data with other people, and how to easily keep all datasets <mark>up-to-date</mark>.\n",
    "\n",
    "But besides having recent data - there's one very important component still missing:\n",
    "- How can we **trust data** that we get from others?\n",
    "- How can we reliably reuse their derivative datasets without fear of them introducing some mistakes, or worse, some **malicious** data?\n",
    "\n",
    "Find out how `kamu` enables <mark>true collaboration and reuse</mark> in the next chapter!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

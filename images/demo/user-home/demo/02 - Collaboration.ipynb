{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"https://github.com/kamu-data/kamu-cli\">\n",
    "<img alt=\"kamu\" src=\"https://raw.githubusercontent.com/kamu-data/kamu-cli/master/docs/readme_files/kamu_logo.png\" width=270/>\n",
    "</a>\n",
    "</center>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<center><i>World's first decentralized real-time data warehouse, on your laptop</i></center>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div align=\"center\">\n",
    "<a href=\"https://docs.kamu.dev/cli/\">Docs</a> | \n",
    "<a href=\"https://docs.kamu.dev/cli/learn/learning-materials/\">Tutorials</a> | \n",
    "<a href=\"https://docs.kamu.dev/cli/learn/examples/\">Examples</a> |\n",
    "<a href=\"https://docs.kamu.dev/cli/get-started/faq/\">FAQ</a> |\n",
    "<a href=\"https://discord.gg/nU6TXRQNXC\">Discord</a> |\n",
    "<a href=\"https://kamu.dev\">Website</a>\n",
    "</div>\n",
    "\n",
    "\n",
    "<center>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "    \n",
    "# 2. Sharing and Collaboration\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "If you skipped the previous chapter or continuing after a break, use the following commands to get your environment ready for this chapter:\n",
    "    \n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\">kamu init\n",
    "kamu add demo/datasets/ca.bccdc.covid19.case-details.yaml\n",
    "kamu pull --all\n",
    "</code>\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repositories\n",
    "\n",
    "After you created your dataset, you might want to share it with other people. We can do so by using **repositories**. There are many kinds of repositories in `kamu`, some only provide storage capabilities (S3, GCS, FTP, ...), some also support remote query execution.\n",
    "\n",
    "In this demo environment we have an S3-compatible storage server running under `minio` hostname. This server has a shared S3 bucket called `kamu-hub` that you can read from and write to.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "So let's add this S3 bucket as a repository:\n",
    "    \n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> &dollar; kamu repo add kamu-hub s3+http://minio/kamu-hub\n",
    "</code>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "To see the list of all known repositories use:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> &dollar; kamu repo list\n",
    "</code>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Now we are ready to <b>push</b> your local dataset into the repository:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> &dollar; kamu push ca.bccdc.covid19.case-details --as kamu-hub/ca.bccdc.covid19.case-details\n",
    "</code>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "If you are doing this demo in a **shared environment**, sombody may have already pushed a dataset to the repository using the same name. `kamu` will not let you overwrite this dataset, so simply **prefix the alias with your username** to make it unique:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> &dollar; kamu push ca.bccdc.covid19.case-details --as kamu-hub/&lt;your-github-username&gt;.ca.bccdc.covid19.case-details\n",
    "</code>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "This initial push creates an association between your local dataset and the repository, so next time you want to update the dataset you can simply run:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> &dollar; kamu push ca.bccdc.covid19.case-details\n",
    "</code>\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discovering Data\n",
    "\n",
    "Using the `search` command, we can also search for datasets in known repositories.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Give search command a try:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> &dollar; kamu search covid\n",
    "</code>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "You should see your dataset in this list, and also some other COVID-19 datasets from other provinces that were added by other people. \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Let's pull one of these datasets into our workspace, and have a look:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> &dollar; kamu pull kamu-hub/ca.ontario.data.covid19.case-details\n",
    "</code>\n",
    "</p>\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> &dollar; kamu tail ca.ontario.data.covid19.case-details\n",
    "</code>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "Ah! This dataset is very similar to the dataset that we created. It also contains invidual case data, but from Ontario province."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative Datasets\n",
    "\n",
    "Canada has 13 provinces and territories, so while you could run data analysis on each of them separately, things would be much simpler if we had one Canada-wide dataset. I'm sure other researchers would also appreciate having such dataset too, so let's create one!\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Let's start with checking the schema of both datasets:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> &dollar; kamu inspect schema ca.bccdc.covid19.case-details\n",
    "</code>\n",
    "</p>\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> &dollar; kamu inspect schema ca.ontario.data.covid19.case-details\n",
    "</code>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "The datasets are similar but they do have somewhat different schemas, so it looks like we will need to **harmonize the data** to be able to combine them. We'll do all this using a **derivative dataset**, which we will define in another `.yaml` file. \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Add the pre-made derivative dataset to your workspace with:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> &dollar; kamu add demo/datasets/ca.covid19.case-details.yaml\n",
    "</code>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "This is what this file looks like:\n",
    "\n",
    "```yaml\n",
    "version: 1\n",
    "kind: DatasetSnapshot\n",
    "content:\n",
    "  id: ca.covid19.case-details\n",
    "  source:\n",
    "    kind: derivative\n",
    "    inputs:\n",
    "      - ca.bccdc.covid19.case-details\n",
    "      - ca.ontario.data.covid19.case-details\n",
    "    transform:\n",
    "      kind: sql\n",
    "      engine: spark\n",
    "      query: >\n",
    "        SELECT\n",
    "          \"BC\" as province,\n",
    "          id,\n",
    "          reported_date,\n",
    "          sex as gender,\n",
    "          Case when age_group = '<10' then '<20'\n",
    "               when age_group = '10-19' then '<20' \n",
    "               when age_group = '20-29' then '20s'\n",
    "               when age_group = '30-39' then '30s'\n",
    "               when age_group = '40-49' then '40s'\n",
    "               when age_group = '50-59' then '50s'\n",
    "               when age_group = '60-69' then '60s'\n",
    "               when age_group = '70-79' then '70s'\n",
    "               when age_group = '80-89' then '80s'\n",
    "               when age_group = '90+' then '90+'\n",
    "               else 'UNKNOWN' end as age_group,\n",
    "            ha as location\n",
    "          FROM `ca.bccdc.covid19.case-details`\n",
    "        UNION ALL\n",
    "        SELECT\n",
    "          \"ON\" as province,\n",
    "          id,\n",
    "          case_reported_date as reported_date,\n",
    "          Case when lower(gender) = 'male' then 'M' \n",
    "               when lower(gender) = 'female' then 'F' \n",
    "               else 'U' end as gender,\n",
    "          age_group,\n",
    "          city as location\n",
    "          FROM `ca.ontario.data.covid19.case-details`\n",
    "  vocab:\n",
    "    eventTimeColumn: reported_date\n",
    "```\n",
    "\n",
    "Unlike root datasets, derivative datasets work on data that is **already in the system**. They can transform, combine, enrich and aggregate data from multiple sources. \n",
    "\n",
    "In our case, the inputs are two root Covid-19 datasets from BC and Ontario. Our output is computed via an SQL query that harmonizes the dataset schemas and performs a `UNION ALL` operation.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Just like before, the dataset is empty untill we pull it:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> &dollar; kamu pull ca.covid19.case-details\n",
    "</code>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "This time, we are not fetching any external data while pulling. Instead, `kamu` will check which input records from the dataset have not been processed yet and it will feed them into the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping data up-to-date\n",
    "\n",
    "If you run the pull command again:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> &dollar; kamu pull ca.covid19.case-details\n",
    "</code>\n",
    "</p>\n",
    "\n",
    "You will see that the dataset is up-to-date, because neither of its two inputs have changed.\n",
    "\n",
    "By adding the `--recursive` flag you can instruct `kamu` to check for updates in every dataset that is part of the dependency tree.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Try running:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> &dollar; kamu pull --recursive ca.covid19.case-details\n",
    "</code>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "This operation will perform three steps:\n",
    "- For `ca.bccdc.covid19.case-details` - check the **external website** for updates\n",
    "- For `ca.ontario.data.covid19.case-details` - check **the repository** for updates (since we pulled this dataset from a repo)\n",
    "- For `ca.covid19.case-details` - **apply the transformation** query to any new data in the above\n",
    "\n",
    "And just like that, <mark>with a single command you can keep a very large number of datasets fully up-to-date</mark>!\n",
    "\n",
    "With `kamu`, you are always few clicks away from getting the latest data into your projects, notebooks and dashboards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to Stream Processing!\n",
    "\n",
    "You might not have recognized it, but you've just been doing **stream processing**. \n",
    "\n",
    "As opposed to the traditional batch processing (e.g. classic SQL, Pandas, or most other frameworks) all processing in `kamu` is streaming in nature.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Have not used stream processing before?** It's a fairly young technology, but it's rapidly entering the data science and analytics space. A very simple overview of differences between streaming and batch can be found in [this talk recording](https://youtu.be/XxKnTusccUM?t=574).\n",
    "\n",
    "</div>\n",
    "\n",
    "To understand the benefits of this, let's create one more derivative dataset. This time we will use the Canada-wide dataset from the previous step to create the \"total daily new cases\" dataset, similar to the COVID statistics you hear on the news nowadays.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Add the pre-made derivative dataset to your workspace with:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> &dollar; kamu add demo/datasets/ca.covid19.daily-cases.yaml\n",
    "</code>\n",
    "</p>\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> &dollar; kamu pull ca.covid19.daily-cases\n",
    "</code>\n",
    "</p>\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> &dollar; kamu tail ca.covid19.daily-cases\n",
    "</code>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "This dataset is defined as:\n",
    "\n",
    "```yaml\n",
    "version: 1\n",
    "kind: DatasetSnapshot\n",
    "content:\n",
    "  id: ca.covid19.daily-cases\n",
    "  source:\n",
    "    kind: derivative\n",
    "    inputs:\n",
    "      - ca.covid19.case-details\n",
    "    transform:\n",
    "      kind: sql\n",
    "      engine: flink\n",
    "      query: >\n",
    "        SELECT\n",
    "          TUMBLE_START(`reported_date`, INTERVAL '1' DAY) as `reported_date`,\n",
    "          `province`,\n",
    "          COUNT(*) as `total_daily`\n",
    "        FROM `ca.covid19.case-details`\n",
    "        GROUP BY TUMBLE(`reported_date`, INTERVAL '1' DAY), `province`\n",
    "  vocab:\n",
    "    eventTimeColumn: reported_date\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Notice** that previously we were using Apache Spark but have switched over to Apache Flink engine for this query. `kamu` already supports [multiple different frameworks](https://docs.kamu.dev/cli/transform/supported-engines/) and can be easily extended to support more.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "Calculating the daily cases per province is an **aggregation**. In a classic SQL we would write it as:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "  reported_date,\n",
    "  province,\n",
    "  COUNT(*) as total_daily\n",
    "FROM `ca.covid19.case-details`\n",
    "GROUP BY reported_date, province\n",
    "```\n",
    "\n",
    "But this type of query is flawed in many ways:\n",
    "- Data from different provinces may be updated on **different cadences**\n",
    "- It may **lag** by one or several days (e.g. CDC of BC does not update their dataset on weekends)\n",
    "- Data may be **out-of-order**\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "Executing this batch query is guaranteed to constantly produce **innacurate results**. What's worse, the errors will be concentrated in the most recent data - data everyone cares about the most.\n",
    "\n",
    "</div>\n",
    "\n",
    "Stream processing takes all these temporal problems into account and will delay producing the result before we have certainty that input data is complete. This is an amazing property that ensures that as we build more and more complex data pipelines we don't end up creating a massive cascade of incorrect data.\n",
    "\n",
    "Stream processing is a truly facinating topic. We will have to park it for now, but be sure to later check out our examples and tutorials on this topic to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lineage\n",
    "\n",
    "So things are getting a bit complicated. We have datasets building on top of datasets, building on yet more datasets...\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "If you ever lose your bearings - <b>lineage</b> command will help you out:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> &dollar; kamu inspect lineage ca.covid19.daily-cases\n",
    "</code>\n",
    "</p>\n",
    "</div>\n",
    "    \n",
    "This command shows you a graph of datasets and their dependencies. Thanks to the metadata, `kamu` knows exactly where every single bit of data came from, so lineage is **guaranteed** to be accurate.\n",
    "\n",
    "When you install `kamu` on your desktop, you can also display lineage in a browser by running:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> &dollar; kamu inspect lineage --browse\n",
    "</code>\n",
    "</p>\n",
    "\n",
    "It will look something like this:\n",
    "\n",
    "![](files/lineage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Up Next\n",
    "ðŸŽ‰ Well done! ðŸŽ‰\n",
    "\n",
    "You have now discovered how `kamu` can be used to <mark>share</mark> data with other people, and how to easily keep all datasets <mark>up-to-date</mark>.\n",
    "\n",
    "But besides having recent data - there's one very important component still missing. How can we trust data that we get from others? How can we reliably reuse their derivative datasets without fear of them introducing some mistakes, or worse, some malicious data.\n",
    "\n",
    "Find out how `kamu` enables <mark>true collaboration and reuse</mark> in the next chapter!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"https://github.com/kamu-data/kamu-cli\">\n",
    "<img alt=\"kamu\" src=\"https://raw.githubusercontent.com/kamu-data/kamu-cli/master/docs/readme_files/kamu_logo.png\" width=270/>\n",
    "</a>\n",
    "</center>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div align=\"center\">\n",
    "<a href=\"https://github.com/kamu-data/kamu-cli\">Repo</a> | \n",
    "<a href=\"https://docs.kamu.dev/cli/\">Docs</a> | \n",
    "<a href=\"https://docs.kamu.dev/cli/learn/learning-materials/\">Tutorials</a> | \n",
    "<a href=\"https://docs.kamu.dev/cli/learn/examples/\">Examples</a> |\n",
    "<a href=\"https://docs.kamu.dev/cli/get-started/faq/\">FAQ</a> |\n",
    "<a href=\"https://discord.gg/nU6TXRQNXC\">Discord</a> |\n",
    "<a href=\"https://kamu.dev\">Website</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<br/>\n",
    "    \n",
    "# 2. Watermarks\n",
    "\n",
    "</center>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "If you skipped the previous chapter or continuing after a break - use the following commands to get your environment ready for this chapter:\n",
    "    \n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\">cd \"02 - Web3 Data (Ethereum trading example)\"\n",
    "./init-chapter-2.sh\n",
    "</code>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "# Up-to-date data is harder than you think\n",
    "\n",
    "With `kamu` its very easy to keep all your data up-to-date.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "To refresh your datasets simply run:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\">&dollar; kamu pull --all\n",
    "</code>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "This command will traverse all datasets in the pipeline in the depth-first order and:\n",
    "- pull data from remote repositories\n",
    "- ingest data into root datasets from external sources\n",
    "- advance computations in derivative datasets\n",
    "\n",
    "And just to confirm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import pandas as pd\n",
    "import hvplot.pandas\n",
    "pd.set_option('max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext kamu\n",
    "%import_dataset net.rocketpool.reth.mint-burn\n",
    "%import_dataset account.tokens.portfolio.market-value\n",
    "%import_dataset account.tokens.portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select \n",
    "    event_time, eth_amount, amount\n",
    "from `net.rocketpool.reth.mint-burn`\n",
    "order by 1 desc\n",
    "limit 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above you should see an `event_time` value close to the current time in UTC.\n",
    "\n",
    "# A Bug?\n",
    "\n",
    "But what about the market value dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select\n",
    "    event_time, token_symbol, token_balance, token_market_value_eth, token_market_value_usd\n",
    "from `account.tokens.portfolio.market-value` \n",
    "order by event_time desc\n",
    "limit 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, for some reason the `event_time` is **way behind** (about a month as of this writing).\n",
    "\n",
    "Remember that to get the market value of `rETH` we JOIN'ed our `portfolio` dataset onto every value in the `reth.mint-burn` dataset. Since `reth.mint-burn`, as we just saw, contains recent values - one would expect to get an up-to-date market value too.\n",
    "\n",
    "_\"So much for near real-time\"_ you say...\n",
    "\n",
    "# Time relativity in data\n",
    "Now abandon your database mindset and think about this from decentralized data perspective.\n",
    "\n",
    "The `portfolio` and `reth.mint-burn` datasets **are owned by different people and and operated using independent infrastructures**.\n",
    "\n",
    "You have just ingested the `portfolio` data from Etherscan, but <mark>is it \"latest data\"</mark>? \n",
    "\n",
    "What if Ethereum blockchain was not producing any blocks for some time? What if blocks are being produced, but Etherscan's node we're pulling data from was failing to synchronize them? Similarly, we told you that `reth.mint-burn` is ingested by a recurrent job, but what if that job didn't run for a while due to some technical issue?\n",
    "\n",
    "> The concept of time and \"now\" suddenly becomes very blurry... \n",
    "\n",
    "What we can tell is that data in the datasets appears with some **latency**, and this latency can vary greatly under some circumstances.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "A good mental model is to imagine that both of these datasets have their **own wall clock**, and the time these clock tell might always be different and advance with very **different speeds**.<br/>    \n",
    "For example a stock ticker dataset might advance its clock every second, but some population census dataset might only advance its clock once every four years.<br/><br/>\n",
    "This is as close as data science gets to the theory of relativity :)\n",
    "</div>\n",
    "\n",
    "# Comparing results to batch processing\n",
    "Now that we agree that `portfolio` and `eth-usd` datasets advance at different paces, meaning that one of them can be slightly or even much further ahead of another - what happens if we JOIN them together using a typical relational batch JOIN.\n",
    "\n",
    "Let's write down the batch SQL version of the market value computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql -o mv_batch -q\n",
    "\n",
    "with \n",
    "\n",
    "--## We only care about rETH tokens\n",
    "--## so let's filter out all other types\n",
    "reth_portfolio as (\n",
    "    select * \n",
    "    from `account.tokens.portfolio`\n",
    "    where token_symbol = \"rETH\"\n",
    "),\n",
    "\n",
    "--## Join every exchange rate data point\n",
    "--## with every portfolio transactions that precedes it in time\n",
    "joined_with_preceding_dates as (\n",
    "    select\n",
    "        reth.event_time as event_time,\n",
    "        pf.block_number as last_tx_block_number,\n",
    "        pf.block_time as last_tx_block_time,\n",
    "        pf.transaction_hash,\n",
    "        pf.token_symbol,\n",
    "        pf.token_balance,\n",
    "        reth.eth_amount / reth.amount * pf.token_balance as token_market_value_eth\n",
    "    from `net.rocketpool.reth.mint-burn` as reth\n",
    "    join reth_portfolio as pf\n",
    "    on reth.event_time >= pf.block_time\n",
    "),\n",
    "\n",
    "--## For every exchange rate data point\n",
    "--## rank transactions in reverse order by time\n",
    "with_dates_ranked as (\n",
    "    select\n",
    "        *,\n",
    "        row_number() over (partition by event_time order by last_tx_block_time desc) as rank\n",
    "    from joined_with_preceding_dates\n",
    ")\n",
    "\n",
    "--## Finally, select only the closest preceding transactions\n",
    "--## using computed rank computed in the previous step\n",
    "select\n",
    "    *\n",
    "from with_dates_ranked\n",
    "where rank = 1\n",
    "order by event_time desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways to perform a JOIN based on closest preceding timestamp, but all of them will give you the same result.\n",
    "\n",
    "Let's compare this \"batch\" result to the \"streaming\" result we get from `kamu`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql -o mv_streaming -q\n",
    "select * from `account.tokens.portfolio.market-value`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "mv_batch.hvplot.line(\n",
    "    x=\"event_time\", \n",
    "    y=\"token_market_value_eth\",\n",
    "    line_dash=\"dashed\",\n",
    "    xlabel=\"Time\",\n",
    "    ylabel=\"ETH\",\n",
    "    title=\"Temporal JOIN: Batch vs Streaming\",\n",
    "    height=500,\n",
    "    width=800,\n",
    ") * mv_streaming.hvplot.line(\n",
    "    x=\"event_time\", \n",
    "    y=\"token_market_value_eth\",\n",
    ") * mv_batch.hvplot.scatter(\n",
    "    x=\"last_tx_block_time\", \n",
    "    y=\"token_market_value_eth\",\n",
    "    color=\"red\",\n",
    "    alpha=0.01,\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha! We can clearly see that the batch JOIN (the dashed blue line) **returned us more data**!\n",
    "\n",
    "<mark>So is batch more real-time than stream processing?</mark>\n",
    "\n",
    "# Data (in)completeness\n",
    "In our case the `portfolio` dataset's clock is lagging significantly behind the exchange rate dataset. This means that the interval of data after `max(portfolio.event_time)` for it is still undefined - it might still get any number of new transactions in this interval. And if it does - the results of `market-value` in that interval <mark>will be completely different than what \"batch\" results show us now!</mark>\n",
    "\n",
    "See, batch SQL doesn't know anything about time and doesn't care about these funky dataset \"wall clocks\". The time columns for it is not more significant than any other, so batch query ignores them.\n",
    "\n",
    "> **Batch processing is non-temporal**. It works under the assumption that every input (table) is complete. In reality most data is constantly delayed, reordered, and backfilled and rarely reaches the state of \"completeness\".\n",
    "\n",
    "This makes batch a poor choice for dynamic data - it will constantly produce incorrect results on incomplete data.\n",
    "\n",
    "# Watermarks\n",
    "Stream processing engines used by `kamu` are (bi)temporal. They are aware of these \"dataset clocks\" and keep track of them. \n",
    "\n",
    "They are called **Watermarks**.\n",
    "\n",
    "A watermark `(Ts, Te)` tells us\n",
    "- that at a certain system time `Ts`\n",
    "- with a high probability\n",
    "- the system have observed all events prior to event time `Te`\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "For a brief introduction to stream processing and watermarks see [this video](https://www.youtube.com/watch?v=XxKnTusccUM&t=577s)\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "You can see the current watermarks of every dataset in `kamu`.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Try running:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\">&dollar; kamu list --wide\n",
    "</code>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "Example output:\n",
    "```\n",
    "┌───────────────────────────────────────┬──────────────┬──────────────┬─────────────┐\n",
    "│                 Name                  │     Kind     │    Pulled    │  Watermark  │\n",
    "├───────────────────────────────────────┼──────────────┼──────────────┼─────────────┤\n",
    "│ account.tokens.portfolio              │  Derivative  │ 2 hours ago  │ a month ago │\n",
    "│ account.tokens.portfolio.market-value │  Derivative  │ a minute ago │ a month ago │\n",
    "│ account.tokens.portfolio.usd          │  Derivative  │ a minute ago │ a month ago │\n",
    "│ account.tokens.transfers              │     Root     │ 2 hours ago  │ a month ago │\n",
    "│ account.transactions                  │     Root     │ 2 hours ago  │ a month ago │\n",
    "│ com.cryptocompare.ohlcv.eth-usd       │ Remote(Root) │ an hour ago  │ an hour ago │\n",
    "│ net.rocketpool.reth.mint-burn         │ Remote(Root) │ 3 hours ago  │ 3 hours ago │\n",
    "└───────────────────────────────────────┴──────────────┴──────────────┴─────────────┘\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "You can also see `kamu` adjusting the watermarks after each `ExecuteQuery` step:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\">&dollar; kamu log account.tokens.portfolio\n",
    "</code>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "Despite being pulled just recently, the `account.tokens.transfers` and `account.transactions` have watermarks which are a month old. And since their data is still undefined beyond that period - same goes for all derivative datasets that depend on them (`account.tokens.portfolio.*`).\n",
    "\n",
    "# How watermarks are set\n",
    "Currently `kamu` automatically sets watermarks for root datasets using highest observer event time - so for `portfolio` dataset the watermark matches the `event_time` of the last transaction. This is a safe default choice that works for most datasets.\n",
    "\n",
    "Watermarks can also be assigned using fixed offset from the latest observed event time (to tolerate some degree of out-of-order arrivals), or predictively - being constantly adjusted by the system based on the observed event delays.\n",
    "\n",
    "There is a lot of room for experimentation, but the key idea of this mechanism is to find good **balance between correctness and latency**:\n",
    "- Advancing watermark too eagerly may result in processing taking place before all data have arrived and produce **incorrect results**\n",
    "- Setting watermark too conservatively to let all data \"settle\" will introduce a lot of **latency**\n",
    "\n",
    "As the owners of our crypto wallet - we are in the best position to know when all transactions have made it into the dataset, so we can manually advance the watermark with `kamu`.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Advance the watermark of both datasets manually (adjust command to current date):\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\">kamu pull --set-watermark 2023-09-01T00:00:00Z account.tokens.transfers\n",
    "kamu pull --set-watermark 2023-09-01T00:00:00Z account.transactions\n",
    "</code>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Propagate the new watermarks through the pipeline:\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\">kamu pull --all\n",
    "</code>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "Let's have a look now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%import_dataset account.tokens.portfolio.market-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql -o mv_streaming -q\n",
    "select * from `account.tokens.portfolio.market-value`\n",
    "order by event_time desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "mv_batch.hvplot.line(\n",
    "    x=\"event_time\", \n",
    "    y=\"token_market_value_eth\",\n",
    "    line_dash=\"dashed\",\n",
    "    xlabel=\"Time\",\n",
    "    ylabel=\"ETH\",\n",
    "    title=\"Temporal JOIN: Batch vs Streaming\",\n",
    "    height=500,\n",
    "    width=800,\n",
    ") * mv_streaming.hvplot.line(\n",
    "    x=\"event_time\", \n",
    "    y=\"token_market_value_eth\",\n",
    ") * mv_batch.hvplot.scatter(\n",
    "    x=\"last_tx_block_time\", \n",
    "    y=\"token_market_value_eth\",\n",
    "    color=\"red\",\n",
    "    alpha=0.01,\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we go, the `market-value` dataset is now fully up-to-date and **correct**!\n",
    "\n",
    "Most importantly, our computations did not make any bold assumptions about completeness of data - we achieved it safely by \"hinting\" to the system that our two root datasets are complete, and `kamu` **propagated the watermarks through the entire pipeline**.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "There is an obvious **room for improvement** here:\n",
    "    \n",
    "The data we get from Etherscan API comes from the Ethereum blockchain, which is strongly ordered. If we could tell what the latest block number processed by Etherscan's backend was - we would be able to set the watermark automatically.\n",
    "    \n",
    "This can be achieved using [container-based fetch](XXX) and is left as an exercise for the reader.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "Every dataset in `kamu` has a **watermark** - they serve to prevent processing from producing incorrect results by holding off until data is complete.\n",
    "\n",
    "Ability to **balance latency and correctness** is the <mark>holy grail of data processing</mark>:\n",
    "- It makes data more **composable** - allows us to build complex pipelines without having a compounding effect of correctness issues\n",
    "- It makes processing more **autonomous** - after the query is written it requires minimal maintenance, unlike batch pipelines that require lots of baby sitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// WARNING: This file is auto-generated from Open Data Fabric Schemas
// See: http://opendatafabric.org/
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

struct Timestamp {
  year: int32;
  ordinal: uint16;
  seconds_from_midnight: uint32;
  nanoseconds: uint32;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// OffsetInterval
// Describes a range of data as a closed arithmetic interval of offsets
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#offsetinterval-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table OffsetInterval {
  // Start of the closed interval [start; end].
  start: uint64;
  // End of the closed interval [start; end].
  end: uint64;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// DataSlice
// Describes a slice of data added to a dataset or produced via transformation
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#dataslice-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table DataSlice {
  // Logical hash sum of the data in this slice.
  logical_hash: [ubyte];
  // Hash sum of the data part file.
  physical_hash: [ubyte];
  // Data slice produced by the transaction.
  offset_interval: OffsetInterval;
  // Size of data file in bytes.
  size: uint64;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// Checkpoint
// Describes a checkpoint produced by an engine
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#checkpoint-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table Checkpoint {
  // Hash sum of the checkpoint file.
  physical_hash: [ubyte];
  // Size of checkpoint file in bytes.
  size: uint64;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// SourceState
// The state of the source the data was added from to allow fast resuming.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#sourcestate-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table SourceState {
  // Identifies the source that the state corresponds to.
  source_name: string;
  // Identifies the type of the state. Standard types include: `odf/etag`, `odf/last-modified`.
  kind: string;
  // Opaque value representing the state.
  value: string;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// AddData
// Indicates that data has been ingested into a root dataset.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#adddata-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table AddData {
  // Hash of the checkpoint file used to restore ingestion state, if any.
  prev_checkpoint: [ubyte];
  // Last offset of the previous data slice, if any. Must be equal to the last non-empty `newData.offsetInterval.end`.
  prev_offset: uint64 = null;
  // Describes output data written during this transaction, if any.
  new_data: DataSlice;
  // Describes checkpoint written during this transaction, if any. If an engine operation resulted in no updates to the checkpoint, but checkpoint is still relevant for subsequent runs - a hash of the previous checkpoint should be specified.
  new_checkpoint: Checkpoint;
  // Last watermark of the output data stream, if any. Initial blocks may not have watermarks, but once watermark is set - all subsequent blocks should either carry the same watermark or specify a new (greater) one. Thus, watermarks are monotonically non-decreasing.
  new_watermark: Timestamp;
  // The state of the source the data was added from to allow fast resuming. If the state did not change but is still relevant for subsequent runs it should be carried, i.e. only the last state per source is considered when resuming.
  new_source_state: SourceState;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// ReadStepCsv
// Reader for comma-separated files.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#readstepcsv-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table ReadStepCsv {
  // A DDL-formatted schema. Schema can be used to coerce values into more appropriate data types.
  //
  // Examples:
  // - ["date TIMESTAMP","city STRING","population INT"]
  schema: [string];
  // Sets a single character as a separator for each field and value.
  //
  // Defaults to: ","
  separator: string;
  // Decodes the CSV files by the given encoding type.
  //
  // Defaults to: "utf8"
  encoding: string;
  // Sets a single character used for escaping quoted values where the separator can be part of the value. Set an empty string to turn off quotations.
  //
  // Defaults to: "\""
  quote: string;
  // Sets a single character used for escaping quotes inside an already quoted value.
  //
  // Defaults to: "\\"
  escape: string;
  // Use the first line as names of columns.
  //
  // Defaults to: false
  header: bool = null;
  // Infers the input schema automatically from data. It requires one extra pass over the data.
  //
  // Defaults to: false
  infer_schema: bool = null;
  // Sets the string representation of a null value.
  //
  // Defaults to: ""
  null_value: string;
  // Sets the string that indicates a date format. The `rfc3339` is the only required format, the other format strings are implementation-specific.
  //
  // Defaults to: "rfc3339"
  date_format: string;
  // Sets the string that indicates a timestamp format. The `rfc3339` is the only required format, the other format strings are implementation-specific.
  //
  // Defaults to: "rfc3339"
  timestamp_format: string;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// ReadStepGeoJson
// Reader for GeoJSON files. It expects one `FeatureCollection` object in the root and will create a record per each `Feature` inside it extracting the properties into individual columns and leaving the feature geometry in its own column.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#readstepgeojson-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table ReadStepGeoJson {
  // A DDL-formatted schema. Schema can be used to coerce values into more appropriate data types.
  schema: [string];
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// ReadStepEsriShapefile
// Reader for ESRI Shapefile format.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#readstepesrishapefile-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table ReadStepEsriShapefile {
  // A DDL-formatted schema. Schema can be used to coerce values into more appropriate data types.
  schema: [string];
  // If the ZIP archive contains multiple shapefiles use this field to specify a sub-path to the desired `.shp` file. Can contain glob patterns to act as a filter.
  sub_path: string;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// ReadStepParquet
// Reader for Apache Parquet format.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#readstepparquet-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table ReadStepParquet {
  // A DDL-formatted schema. Schema can be used to coerce values into more appropriate data types.
  schema: [string];
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// ReadStepJson
// Reader for JSON files that contain an array of objects within them.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#readstepjson-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table ReadStepJson {
  // Path in the form of `a.b.c` to a sub-element of the root JSON object that is an array or objects. If not specified it is assumed that the root element is an array.
  sub_path: string;
  // A DDL-formatted schema. Schema can be used to coerce values into more appropriate data types.
  schema: [string];
  // Sets the string that indicates a date format. The `rfc3339` is the only required format, the other format strings are implementation-specific.
  //
  // Defaults to: "rfc3339"
  date_format: string;
  // Allows to forcibly set one of standard basic or extended encodings.
  //
  // Defaults to: "utf8"
  encoding: string;
  // Sets the string that indicates a timestamp format. The `rfc3339` is the only required format, the other format strings are implementation-specific.
  //
  // Defaults to: "rfc3339"
  timestamp_format: string;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// ReadStepNdJson
// Reader for files containing multiple newline-delimited JSON objects with the same schema.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#readstepndjson-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table ReadStepNdJson {
  // A DDL-formatted schema. Schema can be used to coerce values into more appropriate data types.
  schema: [string];
  // Sets the string that indicates a date format. The `rfc3339` is the only required format, the other format strings are implementation-specific.
  //
  // Defaults to: "rfc3339"
  date_format: string;
  // Allows to forcibly set one of standard basic or extended encodings.
  //
  // Defaults to: "utf8"
  encoding: string;
  // Sets the string that indicates a timestamp format. The `rfc3339` is the only required format, the other format strings are implementation-specific.
  //
  // Defaults to: "rfc3339"
  timestamp_format: string;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// ReadStepNdGeoJson
// Reader for Newline-delimited GeoJSON files. It is similar to `GeoJson` format but instead of `FeatureCollection` object in the root it expects every individual feature object to appear on its own line.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#readstepndgeojson-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table ReadStepNdGeoJson {
  // A DDL-formatted schema. Schema can be used to coerce values into more appropriate data types.
  schema: [string];
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// ReadStep
// Defines how raw data should be read into the structured form.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#readstep-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

union ReadStep {
  ReadStepCsv,
  ReadStepGeoJson,
  ReadStepEsriShapefile,
  ReadStepParquet,
  ReadStepJson,
  ReadStepNdJson,
  ReadStepNdGeoJson,
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// SqlQueryStep
// Defines a query in a multi-step SQL transformation.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#sqlquerystep-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table SqlQueryStep {
  // Name of the temporary view that will be created from result of the query. Step without this alias will be treated as an output of the transformation.
  alias: string;
  // SQL query the result of which will be exposed under the alias.
  query: string;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// TemporalTable
// Temporary Flink-specific extension for creating temporal tables from streams.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#temporaltable-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table TemporalTable {
  // Name of the dataset to be converted into a temporal table.
  name: string;
  // Column names used as the primary key for creating a table.
  primary_key: [string];
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// TransformSql
// Transform using one of the SQL dialects.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#transformsql-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table TransformSql {
  // Identifier of the engine used for this transformation.
  engine: string;
  // Version of the engine to use.
  version: string;
  // SQL query the result of which will be used as an output. This is a convenience property meant only for defining queries by hand. When stored in the metadata this property will never be set and instead will be converted into a single-iter `queries` array.
  query: string;
  // Specifies multi-step SQL transformations. Each step acts as a shorthand for `CREATE TEMPORARY VIEW <alias> AS (<query>)`. Last query in the array should have no alias and will be treated as an output.
  queries: [SqlQueryStep];
  // Temporary Flink-specific extension for creating temporal tables from streams.
  temporal_tables: [TemporalTable];
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// Transform
// Engine-specific processing queries that shape the resulting data.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#transform-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

union Transform {
  TransformSql,
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// MergeStrategyAppend
// Append merge strategy.
// 
// Under this strategy new data will be appended to the dataset in its entirety, without any deduplication.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#mergestrategyappend-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table MergeStrategyAppend {
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// MergeStrategyLedger
// Ledger merge strategy.
// 
// This strategy should be used for data sources containing ledgers of events. Currently this strategy will only perform deduplication of events using user-specified primary key columns. This means that the source data can contain partially overlapping set of records and only those records that were not previously seen will be appended.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#mergestrategyledger-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table MergeStrategyLedger {
  // Names of the columns that uniquely identify the record throughout its lifetime
  primary_key: [string];
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// MergeStrategySnapshot
// Snapshot merge strategy.
// 
// This strategy can be used for data state snapshots that are taken periodically and contain only the latest state of the observed entity or system. Over time such snapshots can have new rows added, and old rows either removed or modified.
// 
// This strategy transforms snapshot data into an append-only event stream where data already added is immutable. It does so by performing Change Data Capture - essentially diffing the current state of data against the reconstructed previous state and recording differences as retractions or corrections. The Operation Type "op" column will contain:
//   - append (`+A`) when a row appears for the first time
//   - retraction (`-D`) when row disappears
//   - correction (`-C`, `+C`) when row data has changed, with `-C` event carrying the old value of the row and `+C` carrying the new value.
// 
// To correctly associate rows between old and new snapshots this strategy relies on user-specified primary key columns.
// 
// To identify whether a row has changed this strategy will compare all other columns one by one. If the data contains a column that is guaranteed to change whenever any of the data columns changes (for example a last modification timestamp, an incremental version, or a data hash), then it can be specified in `compareColumns` property to speed up the detection of modified rows.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#mergestrategysnapshot-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table MergeStrategySnapshot {
  // Names of the columns that uniquely identify the record throughout its lifetime.
  primary_key: [string];
  // Names of the columns to compared to determine if a row has changed between two snapshots.
  compare_columns: [string];
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// MergeStrategy
// Merge strategy determines how newly ingested data should be combined with the data that already exists in the dataset.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#mergestrategy-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

union MergeStrategy {
  MergeStrategyAppend,
  MergeStrategyLedger,
  MergeStrategySnapshot,
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// AddPushSource
// Describes how to ingest data into a root dataset from a certain logical source.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#addpushsource-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table AddPushSource {
  // Identifies the source within this dataset.
  source_name: string;
  // Defines how data is read into structured format.
  read: ReadStep;
  // Pre-processing query that shapes the data.
  preprocess: Transform;
  // Determines how newly-ingested data should be merged with existing history.
  merge: MergeStrategy;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// AttachmentEmbedded
// Embedded attachment item.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#attachmentembedded-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table AttachmentEmbedded {
  // Path to an attachment if it was materialized into a file.
  path: string;
  // Content of the attachment.
  content: string;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// AttachmentsEmbedded
// For attachments that are specified inline and are embedded in the metadata.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#attachmentsembedded-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table AttachmentsEmbedded {
  // List of embedded items.
  items: [AttachmentEmbedded];
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// Attachments
// Defines the source of attachment files.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#attachments-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

union Attachments {
  AttachmentsEmbedded,
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// CompressionFormat
// Defines a compression algorithm.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#compressionformat-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

enum CompressionFormat: int32 {
  Gzip,
  Zip,
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// DatasetKind
// Represents type of the dataset.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#datasetkind-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

enum DatasetKind: int32 {
  Root,
  Derivative,
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// ExecuteTransformInput
// Describes a slice of the input dataset used during a transformation
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#executetransforminput-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table ExecuteTransformInput {
  // Input dataset identifier.
  dataset_id: [ubyte];
  // Last block of the input dataset that was previously incorporated into the derivative transformation, if any. Must be equal to the last non-empty `newBlockHash`. Together with `newBlockHash` defines a half-open `(prevBlockHash, newBlockHash]` interval of blocks that will be considered in this transaction.
  prev_block_hash: [ubyte];
  // Hash of the last block that will be incorporated into the derivative transformation. When present, defines a half-open `(prevBlockHash, newBlockHash]` interval of blocks that will be considered in this transaction.
  new_block_hash: [ubyte];
  // Last data record offset in the input dataset that was previously incorporated into the derivative transformation, if any. Must be equal to the last non-empty `newOffset`. Together with `newOffset` defines a half-open `(prevOffset, newOffset]` interval of data records that will be considered in this transaction.
  prev_offset: uint64 = null;
  // Offset of the last data record that will be incorporated into the derivative transformation, if any. When present, defines a half-open `(prevOffset, newOffset]` interval of data records that will be considered in this transaction.
  new_offset: uint64 = null;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// ExecuteTransform
// Indicates that derivative transformation has been performed.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#executetransform-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table ExecuteTransform {
  // Defines inputs used in this transaction. Slices corresponding to every input dataset must be present.
  query_inputs: [ExecuteTransformInput];
  // Hash of the checkpoint file used to restore transformation state, if any.
  prev_checkpoint: [ubyte];
  // Last offset of the previous data slice, if any. Must be equal to the last non-empty `newData.offsetInterval.end`.
  prev_offset: uint64 = null;
  // Describes output data written during this transaction, if any.
  new_data: DataSlice;
  // Describes checkpoint written during this transaction, if any. If an engine operation resulted in no updates to the checkpoint, but checkpoint is still relevant for subsequent runs - a hash of the previous checkpoint should be specified.
  new_checkpoint: Checkpoint;
  // Last watermark of the output data stream, if any. Initial blocks may not have watermarks, but once watermark is set - all subsequent blocks should either carry the same watermark or specify a new (greater) one. Thus, watermarks are monotonically non-decreasing.
  new_watermark: Timestamp;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// Seed
// Establishes the identity of the dataset. Always the first metadata event in the chain.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#seed-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table Seed {
  // Unique identity of the dataset.
  dataset_id: [ubyte];
  // Type of the dataset.
  dataset_kind: DatasetKind;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// EventTimeSourceFromMetadata
// Extracts event time from the source's metadata.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#eventtimesourcefrommetadata-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table EventTimeSourceFromMetadata {
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// EventTimeSourceFromPath
// Extracts event time from the path component of the source.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#eventtimesourcefrompath-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table EventTimeSourceFromPath {
  // Regular expression where first group contains the timestamp string.
  pattern: string;
  // Format of the expected timestamp in java.text.SimpleDateFormat form.
  timestamp_format: string;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// EventTimeSourceFromSystemTime
// Assigns event time from the system time source.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#eventtimesourcefromsystemtime-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table EventTimeSourceFromSystemTime {
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// EventTimeSource
// Defines the external source of data.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#eventtimesource-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

union EventTimeSource {
  EventTimeSourceFromMetadata,
  EventTimeSourceFromPath,
  EventTimeSourceFromSystemTime,
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// SourceCachingForever
// After source was processed once it will never be ingested again.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#sourcecachingforever-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table SourceCachingForever {
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// SourceCaching
// Defines how external data should be cached.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#sourcecaching-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

union SourceCaching {
  SourceCachingForever,
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// RequestHeader
// Defines a header (e.g. HTTP) to be passed into some request.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#requestheader-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table RequestHeader {
  // Name of the header.
  name: string;
  // Value of the header.
  value: string;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// FetchStepUrl
// Pulls data from one of the supported sources by its URL.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#fetchstepurl-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table FetchStepUrl {
  // URL of the data source
  url: string;
  // Describes how event time is extracted from the source metadata.
  event_time: EventTimeSource;
  // Describes the caching settings used for this source.
  cache: SourceCaching;
  // Headers to pass during the request (e.g. HTTP Authorization)
  headers: [RequestHeader];
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// SourceOrdering
// Specifies how input files should be ordered before ingestion.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#sourceordering-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

enum SourceOrdering: int32 {
  ByEventTime,
  ByName,
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// FetchStepFilesGlob
// Uses glob operator to match files on the local file system.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#fetchstepfilesglob-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table FetchStepFilesGlob {
  // Path with a glob pattern.
  path: string;
  // Describes how event time is extracted from the source metadata.
  event_time: EventTimeSource;
  // Describes the caching settings used for this source.
  cache: SourceCaching;
  // Specifies how input files should be ordered before ingestion.
  // Order is important as every file will be processed individually
  // and will advance the dataset's watermark.
  order: SourceOrdering = null;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// EnvVar
// Defines an environment variable passed into some job.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#envvar-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table EnvVar {
  // Name of the variable.
  name: string;
  // Value of the variable.
  value: string;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// FetchStepContainer
// Runs the specified OCI container to fetch data from an arbitrary source.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#fetchstepcontainer-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table FetchStepContainer {
  // Image name and and an optional tag.
  image: string;
  // Specifies the entrypoint. Not executed within a shell. The default OCI image's ENTRYPOINT is used if this is not provided.
  command: [string];
  // Arguments to the entrypoint. The OCI image's CMD is used if this is not provided.
  args: [string];
  // Environment variables to propagate into or set in the container.
  env: [EnvVar];
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// MqttQos
// MQTT quality of service class.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#mqttqos-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

enum MqttQos: int32 {
  AtMostOnce,
  AtLeastOnce,
  ExactlyOnce,
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// MqttTopicSubscription
// MQTT topic subscription parameters.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#mqtttopicsubscription-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table MqttTopicSubscription {
  // Name of the topic (may include patterns).
  path: string;
  // Quality of service class.
  //
  // Defaults to: "AtMostOnce"
  qos: MqttQos = null;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// FetchStepMqtt
// Connects to an MQTT broker to fetch events from the specified topic.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#fetchstepmqtt-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table FetchStepMqtt {
  // Hostname of the MQTT broker.
  host: string;
  // Port of the MQTT broker.
  port: int32;
  // Username to use for auth with the broker.
  username: string;
  // Password to use for auth with the broker (can be templated).
  password: string;
  // List of topic subscription parameters.
  topics: [MqttTopicSubscription];
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// FetchStepEthereumLogs
// Connects to an Ethereum node to stream transaction logs.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#fetchstepethereumlogs-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table FetchStepEthereumLogs {
  // Identifier of the chain to scan logs from. This parameter may be used for RPC endpoint lookup as well as asserting that provided `nodeUrl` corresponds to the expected chain.
  chain_id: uint64 = null;
  // Url of the node.
  node_url: string;
  // An SQL WHERE clause that can be used to pre-filter the logs before fetching them from the ETH node.
  //
  // Examples:
  // - "block_number > 123 and address = X'5fbdb2315678afecb367f032d93f642f64180aa3' and topic1 = X'000000000000000000000000f39fd6e51aad88f6f4ce6ab8827279cfffb92266'"
  filter: string;
  // Solidity log event signature to use for decoding. Using this field adds `event` to the output containing decoded log as JSON.
  signature: string;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// FetchStep
// Defines the external source of data.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#fetchstep-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

union FetchStep {
  FetchStepUrl,
  FetchStepFilesGlob,
  FetchStepContainer,
  FetchStepMqtt,
  FetchStepEthereumLogs,
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// PrepStepDecompress
// Pulls data from one of the supported sources by its URL.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#prepstepdecompress-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table PrepStepDecompress {
  // Name of a compression algorithm used on data.
  format: CompressionFormat;
  // Path to a data file within a multi-file archive. Can contain glob patterns.
  sub_path: string;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// PrepStepPipe
// Executes external command to process the data using piped input/output.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#prepsteppipe-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table PrepStepPipe {
  // Command to execute and its arguments.
  command: [string];
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// PrepStep
// Defines the steps to prepare raw data for ingestion.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#prepstep-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

union PrepStep {
  PrepStepDecompress,
  PrepStepPipe,
}

table PrepStepWrapper {
  value: PrepStep;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// SetPollingSource
// Contains information on how externally-hosted data can be ingested into the root dataset.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#setpollingsource-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table SetPollingSource {
  // Determines where data is sourced from.
  fetch: FetchStep;
  // Defines how raw data is prepared before reading.
  prepare: [PrepStepWrapper];
  // Defines how data is read into structured format.
  read: ReadStep;
  // Pre-processing query that shapes the data.
  preprocess: Transform;
  // Determines how newly-ingested data should be merged with existing history.
  merge: MergeStrategy;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// TransformInput
// Describes a derivative transformation input
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#transforminput-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table TransformInput {
  // A local or remote dataset reference. When block is accepted this MUST be in the form of a DatasetId to guarantee reproducibility, as aliases can change over time.
  dataset_ref: string;
  // An alias under which this input will be available in queries. Will be populated from `datasetRef` if not provided before resolving it to DatasetId.
  alias: string;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// SetTransform
// Defines a transformation that produces data in a derivative dataset.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#settransform-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table SetTransform {
  // Datasets that will be used as sources.
  inputs: [TransformInput];
  // Transformation that will be applied to produce new data.
  transform: Transform;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// SetVocab
// Lets you manipulate names of the system columns to avoid conflicts.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#setvocab-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table SetVocab {
  // Name of the offset column.
  offset_column: string;
  // Name of the operation type column.
  operation_type_column: string;
  // Name of the system time column.
  system_time_column: string;
  // Name of the event time column.
  event_time_column: string;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// SetAttachments
// Associates a set of files with this dataset.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#setattachments-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table SetAttachments {
  // One of the supported attachment sources.
  attachments: Attachments;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// SetInfo
// Provides basic human-readable information about a dataset.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#setinfo-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table SetInfo {
  // Brief single-sentence summary of a dataset.
  description: string;
  // Keywords, search terms, or tags used to describe the dataset.
  keywords: [string];
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// SetLicense
// Defines a license that applies to this dataset.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#setlicense-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table SetLicense {
  // Abbreviated name of the license.
  short_name: string;
  // Full name of the license.
  name: string;
  // License identifier from the SPDX License List.
  spdx_id: string;
  // URL where licensing terms can be found.
  website_url: string;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// SetDataSchema
// Specifies the complete schema of Data Slices added to the Dataset following this event.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#setdataschema-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table SetDataSchema {
  // Apache Arrow schema encoded in its native flatbuffers representation.
  schema: [ubyte];
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// DisablePushSource
// Disables the previously defined source.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#disablepushsource-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table DisablePushSource {
  // Identifies the source to be disabled.
  source_name: string;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// DisablePollingSource
// Disables the previously defined polling source.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#disablepollingsource-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table DisablePollingSource {
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// MetadataEvent
// Represents a transaction that occurred on a dataset.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#metadataevent-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

union MetadataEvent {
  AddData,
  ExecuteTransform,
  Seed,
  SetPollingSource,
  SetTransform,
  SetVocab,
  SetAttachments,
  SetInfo,
  SetLicense,
  SetDataSchema,
  AddPushSource,
  DisablePushSource,
  DisablePollingSource,
}

table MetadataEventWrapper {
  value: MetadataEvent;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// DatasetSnapshot
// Represents a projection of the dataset metadata at a single point in time.
// This type is typically used for defining new datasets and changing the existing ones.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#datasetsnapshot-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table DatasetSnapshot {
  // Alias of the dataset.
  name: string;
  // Type of the dataset.
  kind: DatasetKind;
  // An array of metadata events that will be used to populate the chain. Here you can define polling and push sources, set licenses, add attachments etc.
  metadata: [MetadataEventWrapper];
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// DatasetVocabulary
// Specifies the mapping of system columns onto dataset schema.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#datasetvocabulary-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table DatasetVocabulary {
  // Name of the offset column.
  //
  // Defaults to: "offset"
  offset_column: string;
  // Name of the operation type column.
  //
  // Defaults to: "op"
  operation_type_column: string;
  // Name of the system time column.
  //
  // Defaults to: "system_time"
  system_time_column: string;
  // Name of the event time column.
  //
  // Defaults to: "event_time"
  event_time_column: string;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// Manifest
// An object that wraps the metadata resources providing versioning and type identification. All root-level resources are wrapped with a manifest when serialized to disk.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#manifest-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table Manifest {
  // Type of the resource.
  kind: int64;
  // Major version number of the resource contained in this manifest. It provides the mechanism for introducing compatibility breaking changes.
  version: int32;
  // Resource data.
  content: [ubyte];
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// MetadataBlock
// An individual block in the metadata chain that captures the history of modifications of a dataset.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#metadatablock-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table MetadataBlock {
  // System time when this block was written.
  system_time: Timestamp;
  // Hash sum of the preceding block.
  prev_block_hash: [ubyte];
  // Block sequence number, starting from zero at the seed block.
  sequence_number: uint64;
  // Event data.
  event: MetadataEvent;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// RawQueryRequest
// Sent by the coordinator to an engine to perform query on raw input data, usually as part of ingest preprocessing step
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#rawqueryrequest-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table RawQueryRequest {
  // Paths to input data files to perform query over. Must all have identical schema.
  input_data_paths: [string];
  // Transformation that will be applied to produce new data.
  transform: Transform;
  // Path where query result will be written.
  output_data_path: string;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// RawQueryResponseProgress
// Reports query progress
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#rawqueryresponseprogress-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table RawQueryResponseProgress {
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// RawQueryResponseSuccess
// Query executed successfully
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#rawqueryresponsesuccess-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table RawQueryResponseSuccess {
  // Number of records produced by the query
  num_records: uint64;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// RawQueryResponseInvalidQuery
// Query did not pass validation
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#rawqueryresponseinvalidquery-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table RawQueryResponseInvalidQuery {
  // Explanation of an error
  message: string;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// RawQueryResponseInternalError
// Internal error during query execution
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#rawqueryresponseinternalerror-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table RawQueryResponseInternalError {
  // Brief description of an error
  message: string;
  // Details of an error (e.g. a backtrace)
  backtrace: string;
}

union RawQueryResponse {
  RawQueryResponseProgress,
  RawQueryResponseSuccess,
  RawQueryResponseInvalidQuery,
  RawQueryResponseInternalError,
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// RawQueryResponseRoot
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#rawqueryresponseroot-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table RawQueryResponseRoot {
  value: RawQueryResponse;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// Watermark
// Represents a watermark in the event stream.
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#watermark-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table Watermark {
  // Moment in processing time when watermark was emitted.
  system_time: Timestamp;
  // Moment in event time which watermark has reached.
  event_time: Timestamp;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// TransformRequestInput
// Sent as part of the engine transform request operation to describe the input
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#transformrequestinput-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table TransformRequestInput {
  // Unique identifier of the dataset.
  dataset_id: [ubyte];
  // Alias of the output dataset, for logging purposes only.
  dataset_alias: string;
  // An alias of this input to be used in queries.
  query_alias: string;
  // Vocabulary of the input dataset.
  vocab: DatasetVocabulary;
  // Subset of data that goes into this transaction.
  offset_interval: OffsetInterval;
  // TODO: This will be removed when coordinator will be slicing data for the engine.
  data_paths: [string];
  // TODO: replace with actual DDL or Parquet schema.
  schema_file: string;
  // Watermarks that should be injected into the stream to separate micro batches for reproducibility.
  explicit_watermarks: [Watermark];
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// TransformRequest
// Sent by the coordinator to an engine to perform the next step of data transformation
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#transformrequest-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table TransformRequest {
  // Unique identifier of the output dataset.
  dataset_id: [ubyte];
  // Alias of the output dataset, for logging purposes only.
  dataset_alias: string;
  // System time to use for new records.
  system_time: Timestamp;
  // Vocabulary of the output dataset.
  vocab: DatasetVocabulary;
  // Transformation that will be applied to produce new data.
  transform: Transform;
  // Defines inputs used in this transaction. Slices corresponding to every input dataset must be present.
  query_inputs: [TransformRequestInput];
  // Starting offset to use for new data records.
  next_offset: uint64;
  // TODO: This will be removed when coordinator will be speaking to engines purely through Arrow.
  prev_checkpoint_path: string;
  // TODO: This will be removed when coordinator will be speaking to engines purely through Arrow.
  new_checkpoint_path: string;
  // TODO: This will be removed when coordinator will be speaking to engines purely through Arrow.
  new_data_path: string;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// TransformResponseProgress
// Reports query progress
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#transformresponseprogress-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table TransformResponseProgress {
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// TransformResponseSuccess
// Query executed successfully
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#transformresponsesuccess-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table TransformResponseSuccess {
  // Data slice produced by the transaction, if any.
  new_offset_interval: OffsetInterval;
  // Watermark advanced by the transaction, if any.
  new_watermark: Timestamp;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// TransformResponseInvalidQuery
// Query did not pass validation
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#transformresponseinvalidquery-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table TransformResponseInvalidQuery {
  // Explanation of an error
  message: string;
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// TransformResponseInternalError
// Internal error during query execution
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#transformresponseinternalerror-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table TransformResponseInternalError {
  // Brief description of an error
  message: string;
  // Details of an error (e.g. a backtrace)
  backtrace: string;
}

union TransformResponse {
  TransformResponseProgress,
  TransformResponseSuccess,
  TransformResponseInvalidQuery,
  TransformResponseInternalError,
}

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// TransformResponseRoot
//
// See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#transformresponseroot-schema
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

table TransformResponseRoot {
  value: TransformResponse;
}


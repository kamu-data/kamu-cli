# THIS IS AN AUTOGENERATED FILE. DO NOT EDIT THIS FILE DIRECTLY.
#
# To regenerate this schema from existing code, use the following command:
# ```shell
# make codegen-graphql-schema
# ```

type AccessTokenConnection {
	"""
	A shorthand for `edges { node { ... } }`
	"""
	nodes: [ViewAccessToken!]!
	"""
	Approximate number of total nodes
	"""
	totalCount: Int!
	"""
	Page information
	"""
	pageInfo: PageBasedInfo!
	edges: [AccessTokenEdge!]!
}

type AccessTokenEdge {
	node: ViewAccessToken!
}

scalar AccessTokenID

type Account {
	"""
	Unique and stable identifier of this account
	"""
	id: AccountID!
	"""
	Symbolic account name
	"""
	accountName: AccountName!
	"""
	Account name to display
	"""
	displayName: AccountDisplayName!
	"""
	Account type
	"""
	accountType: AccountType!
	"""
	Account provider
	"""
	accountProvider: AccountProvider!
	"""
	Email address
	"""
	email: String!
	"""
	Avatar URL
	"""
	avatarUrl: String
	"""
	Indicates the administrator status
	"""
	isAdmin: Boolean!
	"""
	Access to the flow configurations of this account
	"""
	flows: AccountFlows!
	"""
	Access to the flow configurations of this account
	"""
	accessTokens: AccountAccessTokens!
}

type AccountAccessTokens {
	listAccessTokens(page: Int, perPage: Int): AccessTokenConnection!
}

type AccountAccessTokensMut {
	createAccessToken(tokenName: String!): CreateTokenResult!
	revokeAccessToken(tokenId: AccessTokenID!): RevokeResult!
}

type AccountConnection {
	"""
	A shorthand for `edges { node { ... } }`
	"""
	nodes: [Account!]!
	"""
	Approximate number of total nodes
	"""
	totalCount: Int!
	"""
	Page information
	"""
	pageInfo: PageBasedInfo!
	edges: [AccountEdge!]!
}

input AccountDatasetRelationOperation {
	accountId: AccountID!
	operation: DatasetRoleOperation!
	datasetId: DatasetID!
}

scalar AccountDisplayName

type AccountEdge {
	node: Account!
}

type AccountFieldNonUnique implements CreateAccountResult {
	field: String!
	message: String!
}

input AccountFlowFilters {
	byFlowType: DatasetFlowType
	byStatus: FlowStatus
	byInitiator: InitiatorFilterInput
	byDatasetIds: [DatasetID!]!
}

type AccountFlowRuns {
	listFlows(page: Int, perPage: Int, filters: AccountFlowFilters): FlowConnection!
	listDatasetsWithFlow: DatasetConnection!
}

type AccountFlowTriggers {
	"""
	Checks if all triggers of all datasets in account are disabled
	"""
	allPaused: Boolean!
}

type AccountFlowTriggersMut {
	resumeAccountDatasetFlows: Boolean!
	pauseAccountDatasetFlows: Boolean!
}

type AccountFlows {
	"""
	Returns interface for flow runs queries
	"""
	runs: AccountFlowRuns!
	"""
	Returns interface for flow triggers queries
	"""
	triggers: AccountFlowTriggers!
}

type AccountFlowsMut {
	triggers: AccountFlowTriggersMut!
}

scalar AccountID

input AccountLookupFilter {
	excludeAccountsByIds: [AccountID!]!
}

type AccountMut {
	"""
	Update account name
	"""
	rename(newName: AccountName!): RenameAccountResult!
	"""
	Update account email
	"""
	updateEmail(newEmail: Email!): UpdateEmailResult!
	"""
	Reset password for a selected account. Allowed only for admin users
	"""
	modifyPassword(password: AccountPassword!): ModifyPasswordResult!
	"""
	Change password with confirmation
	"""
	modifyPasswordWithConfirmation(oldPassword: AccountPassword!, newPassword: AccountPassword!): ModifyPasswordResult!
	"""
	Delete a selected account. Allowed only for admin users
	"""
	delete: DeleteAccountResult!
	"""
	Access to the mutable flow configurations of this account
	"""
	flows: AccountFlowsMut!
	"""
	Access to the mutable flow configurations of this account
	"""
	accessTokens: AccountAccessTokensMut!
}

scalar AccountName

scalar AccountPassword

enum AccountProvider {
	PASSWORD
	OAUTH_GITHUB
	WEB3_WALLET
}

enum AccountType {
	USER
	ORGANIZATION
}

type AccountWithRole {
	account: Account!
	role: DatasetAccessRole!
}

type AccountWithRoleConnection {
	"""
	A shorthand for `edges { node { ... } }`
	"""
	nodes: [AccountWithRole!]!
	"""
	Approximate number of total nodes
	"""
	totalCount: Int!
	"""
	Page information
	"""
	pageInfo: PageBasedInfo!
	edges: [AccountWithRoleEdge!]!
}

type AccountWithRoleEdge {
	node: AccountWithRole!
}

type Accounts {
	"""
	Returns account by its ID
	"""
	byId(accountId: AccountID!): Account
	"""
	Returns account by its name
	"""
	byName(name: AccountName!): Account
}

type AccountsMut {
	"""
	Returns a mutable account by its id
	"""
	byId(accountId: AccountID!): AccountMut
	"""
	Returns a mutable account by its name
	"""
	byName(accountName: AccountName!): AccountMut
	"""
	Create a new account
	"""
	createAccount(accountName: AccountName!, email: Email): CreateAccountResult!
	"""
	Create wallet accounts
	"""
	createWalletAccounts(walletAddresses: [DidPkh!]!): CreateWalletAccountsResult!
}

"""
Indicates that data has been ingested into a root dataset.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#adddata-schema
"""
type AddData {
	"""
	Hash of the checkpoint file used to restore ingestion state, if any.
	"""
	prevCheckpoint: Multihash
	"""
	Last offset of the previous data slice, if any. Must be equal to the
	last non-empty `newData.offsetInterval.end`.
	"""
	prevOffset: Int
	"""
	Describes output data written during this transaction, if any.
	"""
	newData: DataSlice
	"""
	Describes checkpoint written during this transaction, if any. If an
	engine operation resulted in no updates to the checkpoint, but
	checkpoint is still relevant for subsequent runs - a hash of the
	previous checkpoint should be specified.
	"""
	newCheckpoint: Checkpoint
	"""
	Last watermark of the output data stream, if any. Initial blocks may not
	have watermarks, but once watermark is set - all subsequent blocks
	should either carry the same watermark or specify a new (greater) one.
	Thus, watermarks are monotonically non-decreasing.
	"""
	newWatermark: DateTime
	"""
	The state of the source the data was added from to allow fast resuming.
	If the state did not change but is still relevant for subsequent runs it
	should be carried, i.e. only the last state per source is considered
	when resuming.
	"""
	newSourceState: SourceState
}

"""
Describes how to ingest data into a root dataset from a certain logical
source.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#addpushsource-schema
"""
type AddPushSource {
	"""
	Identifies the source within this dataset.
	"""
	sourceName: String!
	"""
	Defines how data is read into structured format.
	"""
	read: ReadStep!
	"""
	Pre-processing query that shapes the data.
	"""
	preprocess: Transform
	"""
	Determines how newly-ingested data should be merged with existing
	history.
	"""
	merge: MergeStrategy!
}

type Admin {
	selfTest: String!
}

type ApplyRolesMatrixResult {
	message: String!
}

"""
Embedded attachment item.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#attachmentembedded-schema
"""
type AttachmentEmbedded {
	"""
	Path to an attachment if it was materialized into a file.
	"""
	path: String!
	"""
	Content of the attachment.
	"""
	content: String!
}

"""
Defines the source of attachment files.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#attachments-schema
"""
union Attachments = AttachmentsEmbedded

"""
For attachments that are specified inline and are embedded in the metadata.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#attachmentsembedded-schema
"""
type AttachmentsEmbedded {
	"""
	List of embedded items.
	"""
	items: [AttachmentEmbedded!]!
}

type Auth {
	enabledProviders: [AccountProvider!]!
}

type AuthMut {
	"""
	Web3-related functionality group
	"""
	web3: AuthWeb3Mut!
	login(loginMethod: AccountProvider!, loginCredentialsJson: String!, deviceCode: DeviceCode): LoginResponse!
	accountDetails(accessToken: String!): Account!
}

type AuthWeb3Mut {
	eip4361AuthNonce(account: EvmWalletAddress!): Eip4361AuthNonceResponse!
}

"""
Base64-encoded binary data (url-safe, no padding)
"""
scalar Base64Usnp

input BatchingInput {
	minRecordsToAwait: Int!
	maxBatchingInterval: TimeDeltaInput!
}

type BlockRef {
	name: String!
	blockHash: Multihash!
}

type BuildInfo {
	appVersion: String!
	buildTimestamp: String
	gitDescribe: String
	gitSha: String
	gitCommitDate: String
	gitBranch: String
	rustcSemver: String
	rustcChannel: String
	rustcHostTriple: String
	rustcCommitSha: String
	cargoTargetTriple: String
	cargoFeatures: String
	cargoOptLevel: String
}

interface CancelScheduledTasksResult {
	message: String!
}

type CancelScheduledTasksSuccess implements CancelScheduledTasksResult {
	flow: Flow!
	message: String!
}

"""
Describes a checkpoint produced by an engine

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#checkpoint-schema
"""
type Checkpoint {
	"""
	Hash sum of the checkpoint file.
	"""
	physicalHash: Multihash!
	"""
	Size of checkpoint file in bytes.
	"""
	size: Int!
}

type CliProtocolDesc {
	pullCommand: String!
	pushCommand: String!
}

type CollaborationMut {
	"""
	Batch application of relations between accounts and datasets
	"""
	applyAccountDatasetRelations(operations: [AccountDatasetRelationOperation!]!): ApplyRolesMatrixResult!
}

type Collection {
	"""
	Latest state projection of the state of a collection
	"""
	latest: CollectionProjection!
	"""
	State projection of the state of a collection at the specified point in
	time
	"""
	asOf(blockHash: Multihash!): CollectionProjection!
}

type CollectionEntry {
	"""
	Time when this version was created
	"""
	systemTime: DateTime!
	"""
	Time when this version was created
	"""
	eventTime: DateTime!
	"""
	File system-like path
	Rooted, separated by forward slashes, with elements URL-encoded
	(e.g. `/foo%20bar/baz`)
	"""
	path: CollectionPath!
	"""
	DID of the linked dataset
	"""
	ref: DatasetID!
	"""
	Extra data associated with this entry
	"""
	extraData: ExtraData!
	"""
	Resolves the reference to linked dataset
	"""
	asDataset: Dataset
}

type CollectionEntryConnection {
	"""
	A shorthand for `edges { node { ... } }`
	"""
	nodes: [CollectionEntry!]!
	"""
	Approximate number of total nodes
	"""
	totalCount: Int!
	"""
	Page information
	"""
	pageInfo: PageBasedInfo!
	edges: [CollectionEntryEdge!]!
}

type CollectionEntryEdge {
	node: CollectionEntry!
}

input CollectionEntryInput {
	"""
	Entry path
	"""
	path: CollectionPath!
	"""
	DID of the linked dataset
	"""
	ref: DatasetID!
	"""
	Json object containing extra column values
	"""
	extraData: ExtraData
}

type CollectionMut {
	"""
	Links new entry to this collection
	"""
	addEntry(entry: CollectionEntryInput!, expectedHead: Multihash): CollectionUpdateResult!
	"""
	Moves or renames an entry
	"""
	moveEntry(pathFrom: CollectionPath!, pathTo: CollectionPath!, extraData: ExtraData, expectedHead: Multihash): CollectionUpdateResult!
	"""
	Remove an entry from this collection
	"""
	removeEntry(path: CollectionPath!, expectedHead: Multihash): CollectionUpdateResult!
	"""
	Execute multiple add / move / unlink operations as a single transaction
	"""
	updateEntries(operations: [CollectionUpdateInput!]!, expectedHead: Multihash): CollectionUpdateResult!
}

"""
Collection entry paths are similar to HTTP path components. They are rooted (start with `/`), separated by forward slashes, with elements URL-encoded (e.g. `/foo%20bar/baz`)
"""
scalar CollectionPath

type CollectionProjection {
	"""
	Returns an entry at the specified path
	"""
	entry(path: CollectionPath!): CollectionEntry
	"""
	Returns the state of entries as they existed at a specified point in
	time
	"""
	entries(pathPrefix: CollectionPath, maxDepth: Int, page: Int, perPage: Int): CollectionEntryConnection!
	"""
	Find entries that link to specified DIDs
	"""
	entriesByRef(refs: [DatasetID!]!): [CollectionEntry!]!
}

type CollectionUpdateErrorCasFailed implements CollectionUpdateResult {
	expectedHead: Multihash!
	actualHead: Multihash!
	isSuccess: Boolean!
	message: String!
}

type CollectionUpdateErrorNotFound implements CollectionUpdateResult {
	path: CollectionPath!
	isSuccess: Boolean!
	message: String!
}

input CollectionUpdateInput {
	"""
	Inserts a new entry under the specified path. If an entry at the target
	path already exists, it will be retracted.
	"""
	add: CollectionUpdateInputAdd
	"""
	Retracts and appends an entry under the new path. Returns error if from
	path does not exist. If an entry at the target path already exists, it
	will be retracted. Use this to update extra data by specifying the same
	source and target paths.
	"""
	move: CollectionUpdateInputMove
	"""
	Removes the collection entry. Does nothing if entry does not exist.
	"""
	remove: CollectionUpdateInputRemove
}

input CollectionUpdateInputAdd {
	entry: CollectionEntryInput!
}

input CollectionUpdateInputMove {
	pathFrom: CollectionPath!
	pathTo: CollectionPath!
	"""
	Optionally update the extra data
	"""
	extraData: ExtraData
}

input CollectionUpdateInputRemove {
	path: CollectionPath!
}

interface CollectionUpdateResult {
	isSuccess: Boolean!
	message: String!
}

type CollectionUpdateSuccess implements CollectionUpdateResult {
	oldHead: Multihash!
	newHead: Multihash!
	isSuccess: Boolean!
	message: String!
}

type CollectionUpdateUpToDate implements CollectionUpdateResult {
	isSuccess: Boolean!
	message: String!
}

"""
Defines a dataset column
"""
input ColumnInput {
	"""
	Column name
	"""
	name: String!
	"""
	Column data type
	"""
	type: DataTypeInput!
}

interface CommitResult {
	message: String!
}

type CommitResultAppendError implements CommitResult & UpdateReadmeResult {
	message: String!
}

type CommitResultSuccess implements CommitResult & UpdateReadmeResult {
	oldHead: Multihash
	newHead: Multihash!
	message: String!
}

union CompareChainsResult = CompareChainsResultStatus | CompareChainsResultError

type CompareChainsResultError {
	reason: CompareChainsResultReason!
}

type CompareChainsResultReason {
	message: String!
}

type CompareChainsResultStatus {
	message: CompareChainsStatus!
}

enum CompareChainsStatus {
	EQUAL
	BEHIND
	AHEAD
	DIVERGED
}

"""
Defines a compression algorithm.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#compressionformat-schema
"""
enum CompressionFormat {
	GZIP
	ZIP
}

type CreateAccessTokenResultDuplicate implements CreateTokenResult {
	tokenName: String!
	message: String!
}

type CreateAccessTokenResultSuccess implements CreateTokenResult {
	token: CreatedAccessToken!
	message: String!
}

interface CreateAccountResult {
	message: String!
}

type CreateAccountSuccess implements CreateAccountResult {
	account: Account!
	message: String!
}

interface CreateDatasetFromSnapshotResult {
	isSuccess: Boolean!
	message: String!
}

interface CreateDatasetResult {
	isSuccess: Boolean!
	message: String!
}

type CreateDatasetResultInvalidSnapshot implements CreateDatasetFromSnapshotResult {
	message: String!
	isSuccess: Boolean!
}

type CreateDatasetResultMissingInputs implements CreateDatasetFromSnapshotResult {
	missingInputs: [String!]!
	isSuccess: Boolean!
	message: String!
}

type CreateDatasetResultNameCollision implements CreateDatasetResult & CreateDatasetFromSnapshotResult {
	accountName: AccountName
	datasetName: DatasetName!
	isSuccess: Boolean!
	message: String!
}

type CreateDatasetResultSuccess implements CreateDatasetResult & CreateDatasetFromSnapshotResult {
	dataset: Dataset!
	isSuccess: Boolean!
	message: String!
}

interface CreateTokenResult {
	message: String!
}

interface CreateWalletAccountsResult {
	message: String!
}

type CreateWalletAccountsSuccess implements CreateWalletAccountsResult {
	accounts: [Account!]!
	message: String!
}

interface CreateWebhookSubscriptionResult {
	message: String!
}

type CreateWebhookSubscriptionResultSuccess implements CreateWebhookSubscriptionResult {
	subscriptionId: String!
	secret: String!
	message: String!
}

type CreatedAccessToken {
	"""
	Unique identifier of the access token
	"""
	id: AccessTokenID!
	"""
	Name of the access token
	"""
	name: String!
	"""
	Composed original token
	"""
	composed: String!
	"""
	Access token account owner
	"""
	account: Account!
}

type Cron5ComponentExpression {
	cron5ComponentExpression: String!
}

type DataBatch {
	format: DataBatchFormat!
	content: String!
	numRecords: Int!
}

enum DataBatchFormat {
	"""
	Deprecated: Use `JSON_AOS` instead and expect it to become default in
	future versions
	"""
	JSON
	JSON_AOS
	JSON_SOA
	JSON_AOA
	ND_JSON
	CSV
	"""
	Deprecated: Use `ND_JSON` instead
	"""
	JSON_LD
}

type DataQueries {
	"""
	Executes a specified query and returns its result
	"""
	query(query: String!, queryDialect: QueryDialect!, dataFormat: DataBatchFormat, schemaFormat: DataSchemaFormat, skip: Int, limit: Int): DataQueryResult!
	"""
	Lists engines known to the system and recommended for use
	"""
	knownEngines: [EngineDesc!]!
}

union DataQueryResult = DataQueryResultSuccess | DataQueryResultError

type DataQueryResultError {
	errorMessage: String!
	errorKind: DataQueryResultErrorKind!
}

enum DataQueryResultErrorKind {
	INVALID_SQL
}

type DataQueryResultSuccess {
	schema: DataSchema!
	data: DataBatch!
	datasets: [DatasetState!]!
	limit: Int!
}

type DataSchema {
	format: DataSchemaFormat!
	content: String!
}

enum DataSchemaFormat {
	PARQUET
	PARQUET_JSON
	ARROW_JSON
}

"""
Describes a slice of data added to a dataset or produced via transformation

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#dataslice-schema
"""
type DataSlice {
	"""
	Logical hash sum of the data in this slice.
	"""
	logicalHash: Multihash!
	"""
	Hash sum of the data part file.
	"""
	physicalHash: Multihash!
	"""
	Data slice produced by the transaction.
	"""
	offsetInterval: OffsetInterval!
	"""
	Size of data file in bytes.
	"""
	size: Int!
}

input DataTypeInput {
	"""
	Defines type using DDL syntax
	"""
	ddl: String!
}

type Dataset {
	"""
	Unique identifier of the dataset
	"""
	id: DatasetID!
	"""
	Symbolic name of the dataset.
	Name can change over the dataset's lifetime. For unique identifier use
	`id()`.
	"""
	name: DatasetName!
	"""
	Returns the user or organization that owns this dataset
	"""
	owner: Account!
	"""
	Returns dataset alias (user + name)
	"""
	alias: DatasetAlias!
	"""
	Returns the kind of dataset (Root or Derivative)
	"""
	kind: DatasetKind!
	"""
	Returns the visibility of dataset
	"""
	visibility: DatasetVisibilityOutput!
	"""
	Quick access to `head` block hash
	"""
	head: Multihash!
	"""
	Access to the data of the dataset
	"""
	data: DatasetData!
	"""
	Access to the metadata of the dataset
	"""
	metadata: DatasetMetadata!
	"""
	Access to the environment variable of this dataset
	"""
	envVars: DatasetEnvVars!
	"""
	Access to the flow configurations of this dataset
	"""
	flows: DatasetFlows!
	"""
	Creation time of the first metadata block in the chain
	"""
	createdAt: DateTime!
	"""
	Creation time of the most recent metadata block in the chain
	"""
	lastUpdatedAt: DateTime!
	"""
	Permissions of the current user
	"""
	permissions: DatasetPermissions!
	"""
	Current user's role in relation to the dataset
	"""
	role: DatasetAccessRole
	"""
	Access to the dataset collaboration data
	"""
	collaboration: DatasetCollaboration!
	"""
	Access to the dataset's webhooks management functionality
	"""
	webhooks: DatasetWebhooks!
	"""
	Various endpoints for interacting with data
	"""
	endpoints: DatasetEndpoints!
	"""
	Downcast a dataset to a versioned file interface
	"""
	asVersionedFile: VersionedFile
	"""
	Downcast a dataset to a collection interface
	"""
	asCollection: Collection
}

enum DatasetAccessRole {
	"""
	Role opening the possibility for read-only access
	"""
	READER
	"""
	Role allows modifying dataset data
	"""
	EDITOR
	"""
	Role to maintain the dataset
	"""
	MAINTAINER
}

scalar DatasetAlias

type DatasetCollaboration {
	"""
	Accounts (and their roles) that have access to the dataset
	"""
	accountRoles(page: Int, perPage: Int): AccountWithRoleConnection!
}

type DatasetCollaborationMut {
	"""
	Grant account access as the specified role for the dataset
	"""
	setRole(accountId: AccountID!, role: DatasetAccessRole!): SetRoleResult!
	"""
	Revoking account accesses for the dataset
	"""
	unsetRoles(accountIds: [AccountID!]!): UnsetRoleResult!
}

type DatasetCollaborationPermissions {
	canView: Boolean!
	canUpdate: Boolean!
}

type DatasetConnection {
	"""
	A shorthand for `edges { node { ... } }`
	"""
	nodes: [Dataset!]!
	"""
	Approximate number of total nodes
	"""
	totalCount: Int!
	"""
	Page information
	"""
	pageInfo: PageBasedInfo!
	edges: [DatasetEdge!]!
}

type DatasetData {
	"""
	Total number of records in this dataset
	"""
	numRecordsTotal: Int!
	"""
	An estimated size of data on disk not accounting for replication or
	caching
	"""
	estimatedSize: Int!
	"""
	Returns the specified number of the latest records in the dataset
	This is equivalent to SQL query like:
	
	```text
	select * from (
	select
	*
	from dataset
	order by offset desc
	limit lim
	offset skip
	)
	order by offset
	```
	"""
	tail(skip: Int, limit: Int, dataFormat: DataBatchFormat, schemaFormat: DataSchemaFormat): DataQueryResult!
}

type DatasetEdge {
	node: Dataset!
}

type DatasetEndpoints {
	webLink: LinkProtocolDesc!
	cli: CliProtocolDesc!
	rest: RestProtocolDesc!
	flightsql: FlightSqlDesc!
	jdbc: JdbcDesc!
	postgresql: PostgreSqlDesl!
	kafka: KafkaProtocolDesc!
	websocket: WebSocketProtocolDesc!
	odata: OdataProtocolDesc!
}

scalar DatasetEnvVarID

type DatasetEnvVars {
	exposedValue(datasetEnvVarId: DatasetEnvVarID!): String!
	listEnvVariables(page: Int, perPage: Int): ViewDatasetEnvVarConnection!
}

type DatasetEnvVarsMut {
	upsertEnvVariable(key: String!, value: String!, isSecret: Boolean!): UpsertDatasetEnvVarResult!
	deleteEnvVariable(id: DatasetEnvVarID!): DeleteDatasetEnvVarResult!
}

type DatasetEnvVarsPermissions {
	canView: Boolean!
	canUpdate: Boolean!
}

type DatasetFlowConfigs {
	"""
	Returns defined configuration for a flow of specified type
	"""
	byType(datasetFlowType: DatasetFlowType!): FlowConfiguration
}

type DatasetFlowConfigsMut {
	setIngestConfig(ingestConfigInput: FlowConfigIngestInput!, retryPolicyInput: FlowRetryPolicyInput): SetFlowConfigResult!
	setCompactionConfig(compactionConfigInput: FlowConfigCompactionInput!): SetFlowConfigResult!
}

input DatasetFlowFilters {
	byFlowType: DatasetFlowType
	byStatus: FlowStatus
	byInitiator: InitiatorFilterInput
}

type DatasetFlowRuns {
	getFlow(flowId: FlowID!): GetFlowResult!
	listFlows(page: Int, perPage: Int, filters: DatasetFlowFilters): FlowConnection!
	listFlowInitiators: AccountConnection!
}

type DatasetFlowRunsMut {
	triggerIngestFlow(ingestConfigInput: FlowConfigIngestInput): TriggerFlowResult!
	triggerTransformFlow: TriggerFlowResult!
	triggerCompactionFlow(compactionConfigInput: FlowConfigCompactionInput): TriggerFlowResult!
	triggerResetFlow(resetConfigInput: FlowConfigResetInput): TriggerFlowResult!
	cancelScheduledTasks(flowId: FlowID!): CancelScheduledTasksResult!
}

type DatasetFlowTriggers {
	"""
	Returns defined trigger for a flow of specified type
	"""
	byType(datasetFlowType: DatasetFlowType!): FlowTrigger
	"""
	Checks if all triggers of this dataset are disabled
	"""
	allPaused: Boolean!
}

type DatasetFlowTriggersMut {
	setTrigger(datasetFlowType: DatasetFlowType!, paused: Boolean!, triggerInput: FlowTriggerInput!): SetFlowTriggerResult!
	pauseFlow(datasetFlowType: DatasetFlowType!): Boolean!
	pauseFlows: Boolean!
	resumeFlow(datasetFlowType: DatasetFlowType!): Boolean!
	resumeFlows: Boolean!
}

enum DatasetFlowType {
	INGEST
	EXECUTE_TRANSFORM
	HARD_COMPACTION
	RESET
}

type DatasetFlows {
	"""
	Returns interface for flow configurations queries
	"""
	configs: DatasetFlowConfigs!
	"""
	Returns interface for flow triggers queries
	"""
	triggers: DatasetFlowTriggers!
	"""
	Returns interface for flow runs queries
	"""
	runs: DatasetFlowRuns!
}

type DatasetFlowsMut {
	configs: DatasetFlowConfigsMut!
	runs: DatasetFlowRunsMut!
	triggers: DatasetFlowTriggersMut!
}

type DatasetFlowsPermissions {
	canView: Boolean!
	canRun: Boolean!
}

type DatasetGeneralPermissions {
	canRename: Boolean!
	canSetVisibility: Boolean!
	canDelete: Boolean!
}

scalar DatasetID

"""
Represents type of the dataset.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#datasetkind-schema
"""
enum DatasetKind {
	ROOT
	DERIVATIVE
}

type DatasetMetadata {
	"""
	Access to the temporal metadata chain of the dataset
	"""
	chain: MetadataChain!
	"""
	Last recorded watermark
	"""
	currentWatermark: DateTime
	"""
	Latest data schema
	"""
	currentSchema(format: DataSchemaFormat): DataSchema
	"""
	Current upstream dependencies of a dataset
	"""
	currentUpstreamDependencies: [DependencyDatasetResult!]!
	"""
	Current downstream dependencies of a dataset
	"""
	currentDownstreamDependencies: [DependencyDatasetResult!]!
	"""
	Current polling source used by the root dataset
	"""
	currentPollingSource: SetPollingSource
	"""
	Current push sources used by the root dataset
	"""
	currentPushSources: [AddPushSource!]!
	"""
	Sync statuses of push remotes
	"""
	pushSyncStatuses: DatasetPushStatuses!
	"""
	Current transformation used by the derivative dataset
	"""
	currentTransform: SetTransform
	"""
	Current descriptive information about the dataset
	"""
	currentInfo: SetInfo!
	"""
	Current readme file as discovered from attachments associated with the
	dataset
	"""
	currentReadme: String
	"""
	Current license associated with the dataset
	"""
	currentLicense: SetLicense
	"""
	Current vocabulary associated with the dataset
	"""
	currentVocab: SetVocab
}

type DatasetMetadataMut {
	"""
	Access to the mutable metadata chain of the dataset
	"""
	chain: MetadataChainMut!
	"""
	Updates or clears the dataset readme
	"""
	updateReadme(content: String): UpdateReadmeResult!
}

type DatasetMetadataPermissions {
	canCommit: Boolean!
}

type DatasetMut {
	"""
	Access to the mutable metadata of the dataset
	"""
	metadata: DatasetMetadataMut!
	"""
	Access to the mutable flow configurations of this dataset
	"""
	flows: DatasetFlowsMut!
	"""
	Access to the mutable flow configurations of this dataset
	"""
	envVars: DatasetEnvVarsMut!
	"""
	Access to collaboration management methods
	"""
	collaboration: DatasetCollaborationMut!
	"""
	Access to webhooks management methods
	"""
	webhooks: DatasetWebhooksMut!
	"""
	Rename the dataset
	"""
	rename(newName: DatasetName!): RenameResult!
	"""
	Delete the dataset
	"""
	delete: DeleteResult!
	"""
	Manually advances the watermark of a root dataset
	"""
	setWatermark(watermark: DateTime!): SetWatermarkResult!
	"""
	Set visibility for the dataset
	"""
	setVisibility(visibility: DatasetVisibilityInput!): SetDatasetVisibilityResult!
	"""
	Downcast a dataset to a versioned file interface
	"""
	asVersionedFile: VersionedFileMut
	"""
	Downcast a dataset to a collection interface
	"""
	asCollection: CollectionMut
}

scalar DatasetName

type DatasetPermissions {
	collaboration: DatasetCollaborationPermissions!
	envVars: DatasetEnvVarsPermissions!
	flows: DatasetFlowsPermissions!
	webhooks: DatasetWebhooksPermissions!
	general: DatasetGeneralPermissions!
	metadata: DatasetMetadataPermissions!
}

type DatasetPushStatus {
	remote: DatasetRefRemote!
	result: CompareChainsResult!
}

type DatasetPushStatuses {
	statuses: [DatasetPushStatus!]!
}

scalar DatasetRef

scalar DatasetRefRemote

input DatasetRoleOperation @oneOf {
	set: DatasetRoleSetOperation
	unset: DatasetRoleUnsetOperation
}

input DatasetRoleSetOperation {
	role: DatasetAccessRole!
}

input DatasetRoleUnsetOperation {
	dummy: Boolean!
}

type DatasetState {
	"""
	Globally unique identity of the dataset
	"""
	id: DatasetID!
	"""
	Alias to be used in the query
	"""
	alias: String!
	"""
	Last block hash of the input datasets that was or should be considered
	during the query planning
	"""
	blockHash: Multihash
}

enum DatasetVisibility {
	PRIVATE
	PUBLIC
}

input DatasetVisibilityInput @oneOf {
	private: PrivateDatasetVisibilityInput
	public: PublicDatasetVisibilityInput
}

union DatasetVisibilityOutput = PrivateDatasetVisibility | PublicDatasetVisibility

type DatasetWebhooks {
	"""
	Lists all webhook subscriptions for the dataset
	"""
	subscriptions: [WebhookSubscription!]!
	"""
	Returns a webhook subscription by ID
	"""
	subscription(id: WebhookSubscriptionID!): WebhookSubscription
}

type DatasetWebhooksMut {
	createSubscription(input: WebhookSubscriptionInput!): CreateWebhookSubscriptionResult!
	"""
	Returns a webhook subscription management API by ID
	"""
	subscription(id: WebhookSubscriptionID!): WebhookSubscriptionMut
}

type DatasetWebhooksPermissions {
	canView: Boolean!
	canUpdate: Boolean!
}

type Datasets {
	"""
	Returns dataset by its ID
	"""
	byId(datasetId: DatasetID!): Dataset
	"""
	Returns dataset by its owner and name
	"""
	byOwnerAndName(accountName: AccountName!, datasetName: DatasetName!): Dataset
	"""
	Returns datasets belonging to the specified account
	"""
	byAccountId(accountId: AccountID!, page: Int, perPage: Int): DatasetConnection!
	"""
	Returns datasets belonging to the specified account
	"""
	byAccountName(accountName: AccountName!, page: Int, perPage: Int): DatasetConnection!
}

type DatasetsMut {
	"""
	Returns a mutable dataset by its ID
	"""
	byId(datasetId: DatasetID!): DatasetMut
	"""
	Creates a new empty dataset
	"""
	createEmpty(datasetKind: DatasetKind!, datasetAlias: DatasetAlias!, datasetVisibility: DatasetVisibility!): CreateDatasetResult!
	"""
	Creates a new dataset from provided DatasetSnapshot manifest
	"""
	createFromSnapshot(snapshot: String!, snapshotFormat: MetadataManifestFormat!, datasetVisibility: DatasetVisibility!): CreateDatasetFromSnapshotResult!
	"""
	Creates new versioned file dataset.
	Can include schema for extra columns and dataset metadata events (e.g.
	adding description and readme).
	"""
	createVersionedFile(
		"""
		Dataset alias (may include target account)
		"""
		datasetAlias: DatasetAlias!,
		"""
		Additional user-defined columns
		"""
		extraColumns: [ColumnInput!],
		"""
		Extra metadata events (e.g. to populate readme)
		"""
		extraEvents: [String!],
		"""
		How extra events are represented
		"""
		extraEventsFormat: MetadataManifestFormat,
		"""
		Visibility of the dataset
		"""
		datasetVisibility: DatasetVisibility!
	): CreateDatasetFromSnapshotResult!
	"""
	Creates a new collection dataset.
	Can include schema for extra columns, dataset metadata, and initial
	collection entries.
	"""
	createCollection(
		"""
		Dataset alias (may include target account)
		"""
		datasetAlias: DatasetAlias!,
		"""
		Additional user-defined columns
		"""
		extraColumns: [ColumnInput!],
		"""
		Extra metadata events (e.g. to populate readme)
		"""
		extraEvents: [String!],
		"""
		How extra events are represented
		"""
		extraEventsFormat: MetadataManifestFormat,
		"""
		Visibility of the dataset
		"""
		datasetVisibility: DatasetVisibility!
	): CreateDatasetFromSnapshotResult!
}

"""
Implement the DateTime<Utc> scalar

The input/output is a string in RFC3339 format.
"""
scalar DateTime

interface DeleteAccountResult {
	message: String!
}

type DeleteAccountSuccess implements DeleteAccountResult {
	message: String!
}

interface DeleteDatasetEnvVarResult {
	message: String!
}

type DeleteDatasetEnvVarResultNotFound implements DeleteDatasetEnvVarResult {
	envVarId: DatasetEnvVarID!
	message: String!
}

type DeleteDatasetEnvVarResultSuccess implements DeleteDatasetEnvVarResult {
	envVarId: DatasetEnvVarID!
	message: String!
}

interface DeleteResult {
	message: String!
}

type DeleteResultDanglingReference implements DeleteResult {
	notDeletedDataset: DatasetAlias!
	danglingChildRefs: [DatasetRef!]!
	message: String!
}

type DeleteResultSuccess implements DeleteResult {
	deletedDataset: DatasetAlias!
	message: String!
}

interface DependencyDatasetResult {
	message: String!
}

type DependencyDatasetResultAccessible implements DependencyDatasetResult {
	dataset: Dataset!
	message: String!
}

type DependencyDatasetResultNotAccessible implements DependencyDatasetResult {
	id: DatasetID!
	message: String!
}

scalar DeviceCode

"""
Wallet address in did:pkh format.

Example: did:pkh:eip155:1:0xb9c5714089478a327f09197987f16f9e5d936e8a
"""
scalar DidPkh

"""
Disables the previously defined polling source.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#disablepollingsource-schema
"""
type DisablePollingSource {
	dummy: String
}

"""
Disables the previously defined source.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#disablepushsource-schema
"""
type DisablePushSource {
	"""
	Identifies the source to be disabled.
	"""
	sourceName: String!
}

scalar Eip4361AuthNonce

type Eip4361AuthNonceResponse {
	value: Eip4361AuthNonce!
}

scalar Email

type EngineDesc {
	"""
	A short name of the engine, e.g. "Spark", "Flink".
	Intended for use in UI for quick engine identification and selection.
	"""
	name: String!
	"""
	Language and dialect this engine is using for queries
	Indented for configuring code highlighting and completions.
	"""
	dialect: QueryDialect!
	"""
	OCI image repository and a tag of the latest engine image, e.g.
	"ghcr.io/kamu-data/engine-datafusion:0.1.2"
	"""
	latestImage: String!
}

"""
Defines an environment variable passed into some job.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#envvar-schema
"""
type EnvVar {
	"""
	Name of the variable.
	"""
	name: String!
	"""
	Value of the variable.
	"""
	value: String
}

scalar EventID

"""
Defines the external source of data.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#eventtimesource-schema
"""
union EventTimeSource = EventTimeSourceFromMetadata | EventTimeSourceFromPath | EventTimeSourceFromSystemTime

"""
Extracts event time from the source's metadata.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#eventtimesourcefrommetadata-schema
"""
type EventTimeSourceFromMetadata {
	dummy: String
}

"""
Extracts event time from the path component of the source.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#eventtimesourcefrompath-schema
"""
type EventTimeSourceFromPath {
	"""
	Regular expression where first group contains the timestamp string.
	"""
	pattern: String!
	"""
	Format of the expected timestamp in java.text.SimpleDateFormat form.
	"""
	timestampFormat: String
}

"""
Assigns event time from the system time source.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#eventtimesourcefromsystemtime-schema
"""
type EventTimeSourceFromSystemTime {
	dummy: String
}

scalar EvmWalletAddress

"""
Indicates that derivative transformation has been performed.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#executetransform-schema
"""
type ExecuteTransform {
	"""
	Defines inputs used in this transaction. Slices corresponding to every
	input dataset must be present.
	"""
	queryInputs: [ExecuteTransformInput!]!
	"""
	Hash of the checkpoint file used to restore transformation state, if
	any.
	"""
	prevCheckpoint: Multihash
	"""
	Last offset of the previous data slice, if any. Must be equal to the
	last non-empty `newData.offsetInterval.end`.
	"""
	prevOffset: Int
	"""
	Describes output data written during this transaction, if any.
	"""
	newData: DataSlice
	"""
	Describes checkpoint written during this transaction, if any. If an
	engine operation resulted in no updates to the checkpoint, but
	checkpoint is still relevant for subsequent runs - a hash of the
	previous checkpoint should be specified.
	"""
	newCheckpoint: Checkpoint
	"""
	Last watermark of the output data stream, if any. Initial blocks may not
	have watermarks, but once watermark is set - all subsequent blocks
	should either carry the same watermark or specify a new (greater) one.
	Thus, watermarks are monotonically non-decreasing.
	"""
	newWatermark: DateTime
}

"""
Describes a slice of the input dataset used during a transformation

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#executetransforminput-schema
"""
type ExecuteTransformInput {
	"""
	Input dataset identifier.
	"""
	datasetId: DatasetID!
	"""
	Last block of the input dataset that was previously incorporated into
	the derivative transformation, if any. Must be equal to the last
	non-empty `newBlockHash`. Together with `newBlockHash` defines a
	half-open `(prevBlockHash, newBlockHash]` interval of blocks that will
	be considered in this transaction.
	"""
	prevBlockHash: Multihash
	"""
	Hash of the last block that will be incorporated into the derivative
	transformation. When present, defines a half-open `(prevBlockHash,
	newBlockHash]` interval of blocks that will be considered in this
	transaction.
	"""
	newBlockHash: Multihash
	"""
	Last data record offset in the input dataset that was previously
	incorporated into the derivative transformation, if any. Must be equal
	to the last non-empty `newOffset`. Together with `newOffset` defines a
	half-open `(prevOffset, newOffset]` interval of data records that will
	be considered in this transaction.
	"""
	prevOffset: Int
	"""
	Offset of the last data record that will be incorporated into the
	derivative transformation, if any. When present, defines a half-open
	`(prevOffset, newOffset]` interval of data records that will be
	considered in this transaction.
	"""
	newOffset: Int
}

scalar ExtraData

"""
Defines the external source of data.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#fetchstep-schema
"""
union FetchStep = FetchStepUrl | FetchStepFilesGlob | FetchStepContainer | FetchStepMqtt | FetchStepEthereumLogs

"""
Runs the specified OCI container to fetch data from an arbitrary source.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#fetchstepcontainer-schema
"""
type FetchStepContainer {
	"""
	Image name and and an optional tag.
	"""
	image: String!
	"""
	Specifies the entrypoint. Not executed within a shell. The default OCI
	image's ENTRYPOINT is used if this is not provided.
	"""
	command: [String!]
	"""
	Arguments to the entrypoint. The OCI image's CMD is used if this is not
	provided.
	"""
	args: [String!]
	"""
	Environment variables to propagate into or set in the container.
	"""
	env: [EnvVar!]
}

"""
Connects to an Ethereum node to stream transaction logs.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#fetchstepethereumlogs-schema
"""
type FetchStepEthereumLogs {
	"""
	Identifier of the chain to scan logs from. This parameter may be used
	for RPC endpoint lookup as well as asserting that provided `nodeUrl`
	corresponds to the expected chain.
	"""
	chainId: Int
	"""
	Url of the node.
	"""
	nodeUrl: String
	"""
	An SQL WHERE clause that can be used to pre-filter the logs before
	fetching them from the ETH node.
	
	Examples:
	- "block_number > 123 and address =
	X'5fbdb2315678afecb367f032d93f642f64180aa3' and topic1 =
	X'000000000000000000000000f39fd6e51aad88f6f4ce6ab8827279cfffb92266'"
	"""
	filter: String
	"""
	Solidity log event signature to use for decoding. Using this field adds
	`event` to the output containing decoded log as JSON.
	"""
	signature: String
}

"""
Uses glob operator to match files on the local file system.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#fetchstepfilesglob-schema
"""
type FetchStepFilesGlob {
	"""
	Path with a glob pattern.
	"""
	path: String!
	"""
	Describes how event time is extracted from the source metadata.
	"""
	eventTime: EventTimeSource
	"""
	Describes the caching settings used for this source.
	"""
	cache: SourceCaching
	"""
	Specifies how input files should be ordered before ingestion.
	Order is important as every file will be processed individually
	and will advance the dataset's watermark.
	"""
	order: SourceOrdering
}

"""
Connects to an MQTT broker to fetch events from the specified topic.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#fetchstepmqtt-schema
"""
type FetchStepMqtt {
	"""
	Hostname of the MQTT broker.
	"""
	host: String!
	"""
	Port of the MQTT broker.
	"""
	port: Int!
	"""
	Username to use for auth with the broker.
	"""
	username: String
	"""
	Password to use for auth with the broker (can be templated).
	"""
	password: String
	"""
	List of topic subscription parameters.
	"""
	topics: [MqttTopicSubscription!]!
}

"""
Pulls data from one of the supported sources by its URL.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#fetchstepurl-schema
"""
type FetchStepUrl {
	"""
	URL of the data source
	"""
	url: String!
	"""
	Describes how event time is extracted from the source metadata.
	"""
	eventTime: EventTimeSource
	"""
	Describes the caching settings used for this source.
	"""
	cache: SourceCaching
	"""
	Headers to pass during the request (e.g. HTTP Authorization)
	"""
	headers: [RequestHeader!]
}

type FlightSqlDesc {
	url: String!
}

type Flow {
	"""
	Unique identifier of the flow
	"""
	flowId: FlowID!
	"""
	Associated dataset ID, if any
	"""
	datasetId: DatasetID
	"""
	Description of key flow parameters
	"""
	description: FlowDescription!
	"""
	Status of the flow
	"""
	status: FlowStatus!
	"""
	Outcome of the flow (Finished state only)
	"""
	outcome: FlowOutcome
	"""
	Timing records associated with the flow lifecycle
	"""
	timing: FlowTimingRecords!
	"""
	IDs of associated tasks
	"""
	taskIds: [TaskID!]!
	"""
	History of flow events
	"""
	history: [FlowEvent!]!
	"""
	A user, who initiated the flow run. None for system-initiated flows
	"""
	initiator: Account
	"""
	Primary flow activation cause
	"""
	primaryActivationCause: FlowActivationCause!
	"""
	Start condition
	"""
	startCondition: FlowStartCondition
	"""
	Flow config snapshot
	"""
	configSnapshot: FlowConfigRule
	"""
	Flow retry policy
	"""
	retryPolicy: FlowRetryPolicy
}

type FlowAbortedResult {
	message: String!
}

union FlowActivationCause = FlowActivationCauseManual | FlowActivationCauseAutoPolling | FlowActivationCauseDatasetUpdate

type FlowActivationCauseAutoPolling {
	dummy: Boolean!
}

type FlowActivationCauseDatasetUpdate {
	dataset: Dataset!
	source: FlowActivationCauseDatasetUpdateSource!
}

union FlowActivationCauseDatasetUpdateSource = FlowActivationCauseDatasetUpdateSourceUpstreamFlow | FlowActivationCauseDatasetUpdateSourceHttpIngest | FlowActivationCauseDatasetUpdateSourceSmartProtocolPush

type FlowActivationCauseDatasetUpdateSourceHttpIngest {
	sourceName: String
}

type FlowActivationCauseDatasetUpdateSourceSmartProtocolPush {
	accountName: AccountName
	isForce: Boolean!
}

type FlowActivationCauseDatasetUpdateSourceUpstreamFlow {
	flowId: FlowID!
}

type FlowActivationCauseManual {
	initiator: Account!
}

input FlowConfigCompactionInput @oneOf {
	full: FlowConfigInputCompactionFull
	metadataOnly: FlowConfigInputCompactionMetadataOnly
}

union FlowConfigCompactionMode = FlowConfigCompactionModeFull | FlowConfigCompactionModeMetadataOnly

type FlowConfigCompactionModeFull {
	maxSliceSize: Int!
	maxSliceRecords: Int!
	recursive: Boolean!
}

type FlowConfigCompactionModeMetadataOnly {
	recursive: Boolean!
}

input FlowConfigIngestInput {
	"""
	Flag indicates to ignore cache during ingest step for API calls
	"""
	fetchUncacheable: Boolean!
}

input FlowConfigInputCompactionFull {
	maxSliceSize: Int!
	maxSliceRecords: Int!
	recursive: Boolean!
}

input FlowConfigInputCompactionMetadataOnly {
	recursive: Boolean!
}

input FlowConfigInputResetPropagationMode @oneOf {
	custom: FlowConfigInputResetPropagationModeCustom
	toSeed: FlowConfigInputResetPropagationModeToSeed
}

input FlowConfigInputResetPropagationModeCustom {
	newHeadHash: Multihash!
}

input FlowConfigInputResetPropagationModeToSeed {
	dummy: String
}

input FlowConfigResetInput {
	mode: FlowConfigInputResetPropagationMode!
	oldHeadHash: Multihash
	recursive: Boolean!
}

union FlowConfigResetPropagationMode = FlowConfigResetPropagationModeCustom | FlowConfigResetPropagationModeToSeed

type FlowConfigResetPropagationModeCustom {
	newHeadHash: Multihash!
}

type FlowConfigResetPropagationModeToSeed {
	dummy: String
}

union FlowConfigRule = FlowConfigRuleIngest | FlowConfigRuleCompaction | FlowConfigRuleReset

type FlowConfigRuleCompaction {
	compactionMode: FlowConfigCompactionMode!
}

type FlowConfigRuleIngest {
	fetchUncacheable: Boolean!
}

type FlowConfigRuleReset {
	mode: FlowConfigResetPropagationMode!
	oldHeadHash: Multihash
	recursive: Boolean!
}

type FlowConfigSnapshotModified implements FlowEvent {
	eventId: EventID!
	eventTime: DateTime!
	configSnapshot: FlowConfigRule!
}

type FlowConfiguration {
	rule: FlowConfigRule!
	retryPolicy: FlowRetryPolicy
}

type FlowConnection {
	"""
	A shorthand for `edges { node { ... } }`
	"""
	nodes: [Flow!]!
	"""
	Approximate number of total nodes
	"""
	totalCount: Int!
	"""
	Page information
	"""
	pageInfo: PageBasedInfo!
	edges: [FlowEdge!]!
}

union FlowDescription = FlowDescriptionDatasetPollingIngest | FlowDescriptionDatasetPushIngest | FlowDescriptionDatasetExecuteTransform | FlowDescriptionDatasetHardCompaction | FlowDescriptionDatasetReset | FlowDescriptionSystemGC | FlowDescriptionWebhookDeliver

type FlowDescriptionDatasetExecuteTransform {
	transformResult: FlowDescriptionUpdateResult
	transform: SetTransform!
}

type FlowDescriptionDatasetHardCompaction {
	compactionResult: FlowDescriptionDatasetHardCompactionResult
}

union FlowDescriptionDatasetHardCompactionResult = FlowDescriptionHardCompactionNothingToDo | FlowDescriptionHardCompactionSuccess

type FlowDescriptionDatasetPollingIngest {
	ingestResult: FlowDescriptionUpdateResult
	pollingSource: SetPollingSource!
}

type FlowDescriptionDatasetPushIngest {
	sourceName: String
	ingestResult: FlowDescriptionUpdateResult
	message: String!
}

type FlowDescriptionDatasetReset {
	resetResult: FlowDescriptionResetResult
}

type FlowDescriptionHardCompactionNothingToDo {
	dummy: String
	message: String!
}

type FlowDescriptionHardCompactionSuccess {
	originalBlocksCount: Int!
	resultingBlocksCount: Int!
	newHead: Multihash!
}

type FlowDescriptionResetResult {
	newHead: Multihash!
}

type FlowDescriptionSystemGC {
	dummy: Boolean!
}

union FlowDescriptionUpdateResult = FlowDescriptionUpdateResultUpToDate | FlowDescriptionUpdateResultSuccess | FlowDescriptionUpdateResultUnknown

type FlowDescriptionUpdateResultSuccess {
	numBlocks: Int!
	numRecords: Int!
	updatedWatermark: DateTime
}

type FlowDescriptionUpdateResultUnknown {
	message: String!
}

type FlowDescriptionUpdateResultUpToDate {
	"""
	The value indicates whether the api cache was used
	"""
	uncacheable: Boolean!
}

type FlowDescriptionWebhookDeliver {
	targetUrl: Url!
	label: String!
	eventType: String!
}

type FlowEdge {
	node: Flow!
}

interface FlowEvent {
	eventId: EventID!
	eventTime: DateTime!
}

type FlowEventAborted implements FlowEvent {
	eventId: EventID!
	eventTime: DateTime!
}

type FlowEventActivationCauseAdded implements FlowEvent {
	eventId: EventID!
	eventTime: DateTime!
	activationCause: FlowActivationCause!
}

type FlowEventInitiated implements FlowEvent {
	eventId: EventID!
	eventTime: DateTime!
	activationCause: FlowActivationCause!
}

type FlowEventScheduledForActivation implements FlowEvent {
	eventId: EventID!
	eventTime: DateTime!
	scheduledForActivationAt: DateTime!
}

type FlowEventStartConditionUpdated implements FlowEvent {
	eventId: EventID!
	eventTime: DateTime!
	startCondition: FlowStartCondition!
}

type FlowEventTaskChanged implements FlowEvent {
	eventId: EventID!
	eventTime: DateTime!
	taskId: TaskID!
	taskStatus: TaskStatus!
	nextAttemptAt: DateTime
	task: Task!
}

type FlowFailedError {
	reason: TaskFailureReason!
}

scalar FlowID

type FlowIncompatibleDatasetKind implements SetFlowConfigResult & TriggerFlowResult & SetFlowTriggerResult {
	expectedDatasetKind: DatasetKind!
	actualDatasetKind: DatasetKind!
	message: String!
}

type FlowInvalidConfigInputError implements SetFlowConfigResult {
	reason: String!
	message: String!
}

type FlowInvalidRunConfigurations implements TriggerFlowResult {
	error: String!
	message: String!
}

type FlowInvalidTriggerInputError implements SetFlowTriggerResult {
	reason: String!
	message: String!
}

type FlowNotFound implements GetFlowResult & CancelScheduledTasksResult {
	flowId: FlowID!
	message: String!
}

union FlowOutcome = FlowSuccessResult | FlowFailedError | FlowAbortedResult

type FlowPreconditionsNotMet implements SetFlowConfigResult & TriggerFlowResult & SetFlowTriggerResult {
	preconditions: String!
	message: String!
}

enum FlowRetryBackoffType {
	FIXED
	LINEAR
	EXPONENTIAL
	EXPONENTIAL_WITH_JITTER
}

type FlowRetryPolicy {
	maxAttempts: Int!
	minDelay: TimeDelta!
	backoffType: FlowRetryBackoffType!
}

input FlowRetryPolicyInput {
	maxAttempts: Int!
	minDelay: TimeDeltaInput!
	backoffType: FlowRetryBackoffType!
}

union FlowStartCondition = FlowStartConditionSchedule | FlowStartConditionThrottling | FlowStartConditionBatching | FlowStartConditionExecutor

type FlowStartConditionBatching {
	activeBatchingRule: FlowTriggerBatchingRule!
	batchingDeadline: DateTime!
	accumulatedRecordsCount: Int!
	watermarkModified: Boolean!
}

type FlowStartConditionExecutor {
	taskId: TaskID!
}

type FlowStartConditionSchedule {
	wakeUpAt: DateTime!
}

type FlowStartConditionThrottling {
	intervalSec: Int!
	wakeUpAt: DateTime!
	shiftedFrom: DateTime!
}

enum FlowStatus {
	WAITING
	RUNNING
	RETRYING
	FINISHED
}

type FlowSuccessResult {
	message: String!
}

type FlowTimingRecords {
	"""
	Initiation time
	"""
	initiatedAt: DateTime!
	"""
	First scheduling time
	"""
	firstAttemptScheduledAt: DateTime
	"""
	Planned scheduling time (different than first in case of retries)
	"""
	scheduledAt: DateTime
	"""
	Recorded time of last task scheduling
	"""
	awaitingExecutorSince: DateTime
	"""
	Recorded start of running (Running state seen at least once)
	"""
	runningSince: DateTime
	"""
	Recorded time of finish (successful or failed after retry) or abortion
	(Finished state seen at least once)
	"""
	lastAttemptFinishedAt: DateTime
}

type FlowTrigger {
	paused: Boolean!
	schedule: FlowTriggerScheduleRule
	batching: FlowTriggerBatchingRule
}

type FlowTriggerBatchingRule {
	minRecordsToAwait: Int!
	maxBatchingInterval: TimeDelta!
}

input FlowTriggerInput @oneOf {
	schedule: ScheduleInput
	batching: BatchingInput
}

union FlowTriggerScheduleRule = TimeDelta | Cron5ComponentExpression

type FlowTypeIsNotSupported implements SetFlowTriggerResult {
	message: String!
}

interface GetFlowResult {
	message: String!
}

type GetFlowSuccess implements GetFlowResult {
	flow: Flow!
	message: String!
}

input InitiatorFilterInput @oneOf {
	system: Boolean
	accounts: [AccountID!]
}

type JdbcDesc {
	url: String!
}

type KafkaProtocolDesc {
	url: String!
}

"""
Represents base64-encoded binary data using standard encoding
"""
type KeyValue {
	key: String!
	value: String!
}

type LinkProtocolDesc {
	url: String!
}

type LoginResponse {
	accessToken: String!
	account: Account!
}

input LookupFilters {
	byAccount: AccountLookupFilter
}

"""
Merge strategy determines how newly ingested data should be combined with
the data that already exists in the dataset.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#mergestrategy-schema
"""
union MergeStrategy = MergeStrategyAppend | MergeStrategyLedger | MergeStrategySnapshot | MergeStrategyChangelogStream | MergeStrategyUpsertStream

"""
Append merge strategy.

Under this strategy new data will be appended to the dataset in its
entirety, without any deduplication.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#mergestrategyappend-schema
"""
type MergeStrategyAppend {
	dummy: String
}

"""
Changelog stream merge strategy.

This is the native stream format for ODF that accurately describes the
evolution of all event records including appends, retractions, and
corrections as per RFC-015. No pre-processing except for format validation
is done.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#mergestrategychangelogstream-schema
"""
type MergeStrategyChangelogStream {
	"""
	Names of the columns that uniquely identify the record throughout its
	lifetime
	"""
	primaryKey: [String!]!
}

"""
Ledger merge strategy.

This strategy should be used for data sources containing ledgers of events.
Currently this strategy will only perform deduplication of events using
user-specified primary key columns. This means that the source data can
contain partially overlapping set of records and only those records that
were not previously seen will be appended.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#mergestrategyledger-schema
"""
type MergeStrategyLedger {
	"""
	Names of the columns that uniquely identify the record throughout its
	lifetime
	"""
	primaryKey: [String!]!
}

"""
Snapshot merge strategy.

This strategy can be used for data state snapshots that are taken
periodically and contain only the latest state of the observed entity or
system. Over time such snapshots can have new rows added, and old rows
either removed or modified.

This strategy transforms snapshot data into an append-only event stream
where data already added is immutable. It does so by performing Change Data
Capture - essentially diffing the current state of data against the
reconstructed previous state and recording differences as retractions or
corrections. The Operation Type "op" column will contain:
- append (`+A`) when a row appears for the first time
- retraction (`-D`) when row disappears
- correction (`-C`, `+C`) when row data has changed, with `-C` event
carrying the old value of the row and `+C` carrying the new value.

To correctly associate rows between old and new snapshots this strategy
relies on user-specified primary key columns.

To identify whether a row has changed this strategy will compare all other
columns one by one. If the data contains a column that is guaranteed to
change whenever any of the data columns changes (for example a last
modification timestamp, an incremental version, or a data hash), then it can
be specified in `compareColumns` property to speed up the detection of
modified rows.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#mergestrategysnapshot-schema
"""
type MergeStrategySnapshot {
	"""
	Names of the columns that uniquely identify the record throughout its
	lifetime.
	"""
	primaryKey: [String!]!
	"""
	Names of the columns to compared to determine if a row has changed
	between two snapshots.
	"""
	compareColumns: [String!]
}

"""
Upsert stream merge strategy.

This strategy should be used for data sources containing ledgers of
insert-or-update and delete events. Unlike ChangelogStream the
insert-or-update events only carry the new values, so this strategy will use
primary key to re-classify the events into an append or a correction from/to
pair, looking up the previous values.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#mergestrategyupsertstream-schema
"""
type MergeStrategyUpsertStream {
	"""
	Names of the columns that uniquely identify the record throughout its
	lifetime
	"""
	primaryKey: [String!]!
}

type MetadataBlockConnection {
	"""
	A shorthand for `edges { node { ... } }`
	"""
	nodes: [MetadataBlockExtended!]!
	"""
	Approximate number of total nodes
	"""
	totalCount: Int!
	"""
	Page information
	"""
	pageInfo: PageBasedInfo!
	edges: [MetadataBlockEdge!]!
}

type MetadataBlockEdge {
	node: MetadataBlockExtended!
}

type MetadataBlockExtended {
	blockHash: Multihash!
	prevBlockHash: Multihash
	systemTime: DateTime!
	author: Account!
	event: MetadataEvent!
	sequenceNumber: Int!
}

type MetadataChain {
	"""
	Returns all named metadata block references
	"""
	refs: [BlockRef!]!
	"""
	Returns a metadata block corresponding to the specified hash
	"""
	blockByHash(hash: Multihash!): MetadataBlockExtended
	"""
	Returns a metadata block corresponding to the specified hash and encoded
	in desired format
	"""
	blockByHashEncoded(hash: Multihash!, format: MetadataManifestFormat!): String
	"""
	Iterates all metadata blocks in the reverse chronological order
	"""
	blocks(page: Int, perPage: Int): MetadataBlockConnection!
}

type MetadataChainMut {
	"""
	Commits new event to the metadata chain
	"""
	commitEvent(event: String!, eventFormat: MetadataManifestFormat!): CommitResult!
}

"""
Represents a transaction that occurred on a dataset.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#metadataevent-schema
"""
union MetadataEvent = AddData | ExecuteTransform | Seed | SetPollingSource | SetTransform | SetVocab | SetAttachments | SetInfo | SetLicense | SetDataSchema | AddPushSource | DisablePushSource | DisablePollingSource

enum MetadataManifestFormat {
	YAML
}

type MetadataManifestMalformed implements CommitResult & CreateDatasetFromSnapshotResult {
	message: String!
	isSuccess: Boolean!
}

type MetadataManifestUnsupportedVersion implements CommitResult & CreateDatasetFromSnapshotResult {
	isSuccess: Boolean!
	message: String!
}

interface ModifyPasswordResult {
	message: String!
}

type ModifyPasswordSuccess implements ModifyPasswordResult {
	message: String!
}

type ModifyPasswordWrongOldPassword implements ModifyPasswordResult {
	message: String!
}

"""
MQTT quality of service class.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#mqttqos-schema
"""
enum MqttQos {
	AT_MOST_ONCE
	AT_LEAST_ONCE
	EXACTLY_ONCE
}

"""
MQTT topic subscription parameters.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#mqtttopicsubscription-schema
"""
type MqttTopicSubscription {
	"""
	Name of the topic (may include patterns).
	"""
	path: String!
	"""
	Quality of service class.
	
	Defaults to: "AtMostOnce"
	"""
	qos: MqttQos
}

scalar Multihash

type Mutation {
	"""
	Authentication and authorization-related functionality group
	"""
	auth: AuthMut!
	"""
	Dataset-related functionality group.
	
	Datasets are historical streams of events recorded under a certain
	schema.
	"""
	datasets: DatasetsMut!
	"""
	Account-related functionality group.
	
	Accounts can be individual users or organizations registered in the
	system. This groups deals with their identities and permissions.
	"""
	accounts: AccountsMut!
	"""
	Collaboration-related functionality group
	
	Allows setting permissions for multiple datasets in batch mode
	"""
	collaboration: CollaborationMut!
}

union NameLookupResult = Account

type NameLookupResultConnection {
	"""
	A shorthand for `edges { node { ... } }`
	"""
	nodes: [NameLookupResult!]!
	"""
	Approximate number of total nodes
	"""
	totalCount: Int!
	"""
	Page information
	"""
	pageInfo: PageBasedInfo!
	edges: [NameLookupResultEdge!]!
}

type NameLookupResultEdge {
	node: NameLookupResult!
}

type NoChanges implements CommitResult & UpdateReadmeResult {
	message: String!
}

type OdataProtocolDesc {
	serviceUrl: String!
	collectionUrl: String!
}

"""
Describes a range of data as a closed arithmetic interval of offsets

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#offsetinterval-schema
"""
type OffsetInterval {
	"""
	Start of the closed interval [start; end].
	"""
	start: Int!
	"""
	End of the closed interval [start; end].
	"""
	end: Int!
}

type PageBasedInfo {
	"""
	When paginating backwards, are there more items?
	"""
	hasPreviousPage: Boolean!
	"""
	When paginating forwards, are there more items?
	"""
	hasNextPage: Boolean!
	"""
	Index of the current page
	"""
	currentPage: Int!
	"""
	Approximate number of total pages assuming number of nodes per page
	stays the same
	"""
	totalPages: Int
}

interface PauseWebhookSubscriptionResult {
	message: String!
}

type PauseWebhookSubscriptionResultSuccess implements PauseWebhookSubscriptionResult {
	paused: Boolean!
	message: String!
}

type PauseWebhookSubscriptionResultUnexpected implements PauseWebhookSubscriptionResult {
	status: WebhookSubscriptionStatus!
	message: String!
}

type PostgreSqlDesl {
	url: String!
}

"""
Defines the steps to prepare raw data for ingestion.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#prepstep-schema
"""
union PrepStep = PrepStepDecompress | PrepStepPipe

"""
Pulls data from one of the supported sources by its URL.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#prepstepdecompress-schema
"""
type PrepStepDecompress {
	"""
	Name of a compression algorithm used on data.
	"""
	format: CompressionFormat!
	"""
	Path to a data file within a multi-file archive. Can contain glob
	patterns.
	"""
	subPath: String
}

"""
Executes external command to process the data using piped input/output.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#prepsteppipe-schema
"""
type PrepStepPipe {
	"""
	Command to execute and its arguments.
	"""
	command: [String!]!
}

type PrivateDatasetVisibility {
	dummy: String
}

input PrivateDatasetVisibilityInput {
	dummy: String
}

type PublicDatasetVisibility {
	anonymousAvailable: Boolean!
}

input PublicDatasetVisibilityInput {
	anonymousAvailable: Boolean!
}

type Query {
	"""
	Returns the version of the GQL API
	"""
	apiVersion: String!
	"""
	Returns server's version and build configuration information
	"""
	buildInfo: BuildInfo!
	"""
	Authentication and authorization-related functionality group
	"""
	auth: Auth!
	"""
	Dataset-related functionality group.
	
	Datasets are historical streams of events recorded under a certain
	schema.
	"""
	datasets: Datasets!
	"""
	Account-related functionality group.
	
	Accounts can be individual users or organizations registered in the
	system. This groups deals with their identities and permissions.
	"""
	accounts: Accounts!
	"""
	Webhook-related functionality group
	
	Webhooks are used to send notifications about events happening in the
	system. This groups deals with their management and subscriptions.
	"""
	webhooks: Webhooks!
	"""
	Search-related functionality group
	"""
	search: Search!
	"""
	Querying and data manipulations
	"""
	data: DataQueries!
	"""
	Admin-related functionality group
	"""
	admin: Admin!
}

enum QueryDialect {
	SQL_SPARK
	SQL_FLINK
	SQL_DATA_FUSION
	SQL_RISING_WAVE
}

"""
Defines how raw data should be read into the structured form.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#readstep-schema
"""
union ReadStep = ReadStepCsv | ReadStepGeoJson | ReadStepEsriShapefile | ReadStepParquet | ReadStepJson | ReadStepNdJson | ReadStepNdGeoJson

"""
Reader for comma-separated files.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#readstepcsv-schema
"""
type ReadStepCsv {
	"""
	A DDL-formatted schema. Schema can be used to coerce values into more
	appropriate data types.
	
	Examples:
	- ["date TIMESTAMP","city STRING","population INT"]
	"""
	schema: [String!]
	"""
	Sets a single character as a separator for each field and value.
	
	Defaults to: ","
	"""
	separator: String
	"""
	Decodes the CSV files by the given encoding type.
	
	Defaults to: "utf8"
	"""
	encoding: String
	"""
	Sets a single character used for escaping quoted values where the
	separator can be part of the value. Set an empty string to turn off
	quotations.
	
	Defaults to: "\""
	"""
	quote: String
	"""
	Sets a single character used for escaping quotes inside an already
	quoted value.
	
	Defaults to: "\\"
	"""
	escape: String
	"""
	Use the first line as names of columns.
	
	Defaults to: false
	"""
	header: Boolean
	"""
	Infers the input schema automatically from data. It requires one extra
	pass over the data.
	
	Defaults to: false
	"""
	inferSchema: Boolean
	"""
	Sets the string representation of a null value.
	
	Defaults to: ""
	"""
	nullValue: String
	"""
	Sets the string that indicates a date format. The `rfc3339` is the only
	required format, the other format strings are implementation-specific.
	
	Defaults to: "rfc3339"
	"""
	dateFormat: String
	"""
	Sets the string that indicates a timestamp format. The `rfc3339` is the
	only required format, the other format strings are
	implementation-specific.
	
	Defaults to: "rfc3339"
	"""
	timestampFormat: String
}

"""
Reader for ESRI Shapefile format.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#readstepesrishapefile-schema
"""
type ReadStepEsriShapefile {
	"""
	A DDL-formatted schema. Schema can be used to coerce values into more
	appropriate data types.
	"""
	schema: [String!]
	"""
	If the ZIP archive contains multiple shapefiles use this field to
	specify a sub-path to the desired `.shp` file. Can contain glob patterns
	to act as a filter.
	"""
	subPath: String
}

"""
Reader for GeoJSON files. It expects one `FeatureCollection` object in the
root and will create a record per each `Feature` inside it extracting the
properties into individual columns and leaving the feature geometry in its
own column.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#readstepgeojson-schema
"""
type ReadStepGeoJson {
	"""
	A DDL-formatted schema. Schema can be used to coerce values into more
	appropriate data types.
	"""
	schema: [String!]
}

"""
Reader for JSON files that contain an array of objects within them.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#readstepjson-schema
"""
type ReadStepJson {
	"""
	Path in the form of `a.b.c` to a sub-element of the root JSON object
	that is an array or objects. If not specified it is assumed that the
	root element is an array.
	"""
	subPath: String
	"""
	A DDL-formatted schema. Schema can be used to coerce values into more
	appropriate data types.
	"""
	schema: [String!]
	"""
	Sets the string that indicates a date format. The `rfc3339` is the only
	required format, the other format strings are implementation-specific.
	
	Defaults to: "rfc3339"
	"""
	dateFormat: String
	"""
	Allows to forcibly set one of standard basic or extended encodings.
	
	Defaults to: "utf8"
	"""
	encoding: String
	"""
	Sets the string that indicates a timestamp format. The `rfc3339` is the
	only required format, the other format strings are
	implementation-specific.
	
	Defaults to: "rfc3339"
	"""
	timestampFormat: String
}

"""
Reader for Newline-delimited GeoJSON files. It is similar to `GeoJson`
format but instead of `FeatureCollection` object in the root it expects
every individual feature object to appear on its own line.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#readstepndgeojson-schema
"""
type ReadStepNdGeoJson {
	"""
	A DDL-formatted schema. Schema can be used to coerce values into more
	appropriate data types.
	"""
	schema: [String!]
}

"""
Reader for files containing multiple newline-delimited JSON objects with the
same schema.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#readstepndjson-schema
"""
type ReadStepNdJson {
	"""
	A DDL-formatted schema. Schema can be used to coerce values into more
	appropriate data types.
	"""
	schema: [String!]
	"""
	Sets the string that indicates a date format. The `rfc3339` is the only
	required format, the other format strings are implementation-specific.
	
	Defaults to: "rfc3339"
	"""
	dateFormat: String
	"""
	Allows to forcibly set one of standard basic or extended encodings.
	
	Defaults to: "utf8"
	"""
	encoding: String
	"""
	Sets the string that indicates a timestamp format. The `rfc3339` is the
	only required format, the other format strings are
	implementation-specific.
	
	Defaults to: "rfc3339"
	"""
	timestampFormat: String
}

"""
Reader for Apache Parquet format.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#readstepparquet-schema
"""
type ReadStepParquet {
	"""
	A DDL-formatted schema. Schema can be used to coerce values into more
	appropriate data types.
	"""
	schema: [String!]
}

interface RemoveWebhookSubscriptionResult {
	message: String!
}

type RemoveWebhookSubscriptionResultSuccess implements RemoveWebhookSubscriptionResult {
	removed: Boolean!
	message: String!
}

type RenameAccountNameNotUnique implements RenameAccountResult {
	message: String!
}

interface RenameAccountResult {
	message: String!
}

type RenameAccountSuccess implements RenameAccountResult {
	newName: String!
	message: String!
}

interface RenameResult {
	message: String!
}

type RenameResultNameCollision implements RenameResult {
	collidingAlias: DatasetAlias!
	message: String!
}

type RenameResultNoChanges implements RenameResult {
	preservedName: DatasetName!
	message: String!
}

type RenameResultSuccess implements RenameResult {
	oldName: DatasetName!
	newName: DatasetName!
	message: String!
}

"""
Defines a header (e.g. HTTP) to be passed into some request.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#requestheader-schema
"""
type RequestHeader {
	"""
	Name of the header.
	"""
	name: String!
	"""
	Value of the header.
	"""
	value: String!
}

type RestProtocolDesc {
	tailUrl: String!
	queryUrl: String!
	pushUrl: String!
}

interface ResumeWebhookSubscriptionResult {
	message: String!
}

type ResumeWebhookSubscriptionResultSuccess implements ResumeWebhookSubscriptionResult {
	resumed: Boolean!
	message: String!
}

type ResumeWebhookSubscriptionResultUnexpected implements ResumeWebhookSubscriptionResult {
	status: WebhookSubscriptionStatus!
	message: String!
}

interface RevokeResult {
	message: String!
}

type RevokeResultAlreadyRevoked implements RevokeResult {
	tokenId: AccessTokenID!
	message: String!
}

type RevokeResultSuccess implements RevokeResult {
	tokenId: AccessTokenID!
	message: String!
}

input ScheduleInput @oneOf {
	timeDelta: TimeDeltaInput
	"""
	Supported CRON syntax: min hour dayOfMonth month dayOfWeek
	"""
	cron5ComponentExpression: String
}

type Search {
	"""
	This endpoint uses heuristics to infer whether the query string is a DSL
	or a natural language query and is suitable to present the most
	versatile interface to the user consisting of just one input field.
	"""
	query(query: String!, page: Int, perPage: Int): SearchResultConnection!
	"""
	Searches for datasets and other objects managed by the
	current node using a prompt in natural language
	"""
	queryNaturalLanguage(prompt: String!, perPage: Int): SearchResultExConnection!
	"""
	Perform lightweight search among resource names.
	Useful for autocomplete.
	"""
	nameLookup(query: String!, filters: LookupFilters!, page: Int, perPage: Int): NameLookupResultConnection!
}

union SearchResult = Dataset

type SearchResultConnection {
	"""
	A shorthand for `edges { node { ... } }`
	"""
	nodes: [SearchResult!]!
	"""
	Approximate number of total nodes
	"""
	totalCount: Int!
	"""
	Page information
	"""
	pageInfo: PageBasedInfo!
	edges: [SearchResultEdge!]!
}

type SearchResultEdge {
	node: SearchResult!
}

type SearchResultEx {
	item: SearchResult!
	score: Float!
}

type SearchResultExConnection {
	"""
	A shorthand for `edges { node { ... } }`
	"""
	nodes: [SearchResultEx!]!
	"""
	Approximate number of total nodes
	"""
	totalCount: Int!
	"""
	Page information
	"""
	pageInfo: PageBasedInfo!
	edges: [SearchResultExEdge!]!
}

type SearchResultExEdge {
	node: SearchResultEx!
}

"""
Establishes the identity of the dataset. Always the first metadata event in
the chain.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#seed-schema
"""
type Seed {
	"""
	Unique identity of the dataset.
	"""
	datasetId: DatasetID!
	"""
	Type of the dataset.
	"""
	datasetKind: DatasetKind!
}

"""
Associates a set of files with this dataset.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#setattachments-schema
"""
type SetAttachments {
	"""
	One of the supported attachment sources.
	"""
	attachments: Attachments!
}

"""
Specifies the complete schema of Data Slices added to the Dataset following
this event.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#setdataschema-schema
"""
type SetDataSchema {
	schema: DataSchema!
}

interface SetDatasetVisibilityResult {
	message: String!
}

type SetDatasetVisibilityResultSuccess implements SetDatasetVisibilityResult {
	message: String!
}

interface SetFlowConfigResult {
	message: String!
}

type SetFlowConfigSuccess implements SetFlowConfigResult {
	config: FlowConfiguration!
	message: String!
}

interface SetFlowTriggerResult {
	message: String!
}

type SetFlowTriggerSuccess implements SetFlowTriggerResult {
	trigger: FlowTrigger!
	message: String!
}

"""
Provides basic human-readable information about a dataset.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#setinfo-schema
"""
type SetInfo {
	"""
	Brief single-sentence summary of a dataset.
	"""
	description: String
	"""
	Keywords, search terms, or tags used to describe the dataset.
	"""
	keywords: [String!]
}

"""
Defines a license that applies to this dataset.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#setlicense-schema
"""
type SetLicense {
	"""
	Abbreviated name of the license.
	"""
	shortName: String!
	"""
	Full name of the license.
	"""
	name: String!
	"""
	License identifier from the SPDX License List.
	"""
	spdxId: String
	"""
	URL where licensing terms can be found.
	"""
	websiteUrl: String!
}

"""
Contains information on how externally-hosted data can be ingested into the
root dataset.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#setpollingsource-schema
"""
type SetPollingSource {
	"""
	Determines where data is sourced from.
	"""
	fetch: FetchStep!
	"""
	Defines how raw data is prepared before reading.
	"""
	prepare: [PrepStep!]
	"""
	Defines how data is read into structured format.
	"""
	read: ReadStep!
	"""
	Pre-processing query that shapes the data.
	"""
	preprocess: Transform
	"""
	Determines how newly-ingested data should be merged with existing
	history.
	"""
	merge: MergeStrategy!
}

interface SetRoleResult {
	message: String!
}

type SetRoleResultSuccess implements SetRoleResult {
	message: String!
}

"""
Defines a transformation that produces data in a derivative dataset.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#settransform-schema
"""
type SetTransform {
	"""
	Datasets that will be used as sources.
	"""
	inputs: [TransformInput!]!
	"""
	Transformation that will be applied to produce new data.
	"""
	transform: Transform!
}

"""
Lets you manipulate names of the system columns to avoid conflicts.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#setvocab-schema
"""
type SetVocab {
	"""
	Name of the offset column.
	"""
	offsetColumn: String
	"""
	Name of the operation type column.
	"""
	operationTypeColumn: String
	"""
	Name of the system time column.
	"""
	systemTimeColumn: String
	"""
	Name of the event time column.
	"""
	eventTimeColumn: String
}

type SetWatermarkIsDerivative implements SetWatermarkResult {
	message: String!
}

interface SetWatermarkResult {
	message: String!
}

type SetWatermarkUpToDate implements SetWatermarkResult {
	message: String!
}

type SetWatermarkUpdated implements SetWatermarkResult {
	newHead: Multihash!
	message: String!
}

"""
Defines how external data should be cached.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#sourcecaching-schema
"""
union SourceCaching = SourceCachingForever

"""
After source was processed once it will never be ingested again.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#sourcecachingforever-schema
"""
type SourceCachingForever {
	dummy: String
}

"""
Specifies how input files should be ordered before ingestion.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#sourceordering-schema
"""
enum SourceOrdering {
	BY_EVENT_TIME
	BY_NAME
}

"""
The state of the source the data was added from to allow fast resuming.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#sourcestate-schema
"""
type SourceState {
	"""
	Identifies the source that the state corresponds to.
	"""
	sourceName: String!
	"""
	Identifies the type of the state. Standard types include: `odf/etag`,
	`odf/last-modified`.
	"""
	kind: String!
	"""
	Opaque value representing the state.
	"""
	value: String!
}

"""
Defines a query in a multi-step SQL transformation.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#sqlquerystep-schema
"""
type SqlQueryStep {
	"""
	Name of the temporary view that will be created from result of the
	query. Step without this alias will be treated as an output of the
	transformation.
	"""
	alias: String
	"""
	SQL query the result of which will be exposed under the alias.
	"""
	query: String!
}

type StartUploadVersionErrorTooLarge implements StartUploadVersionResult {
	uploadSize: Int!
	uploadLimit: Int!
	isSuccess: Boolean!
	message: String!
}

interface StartUploadVersionResult {
	isSuccess: Boolean!
	message: String!
}

type StartUploadVersionSuccess implements StartUploadVersionResult {
	url: String!
	method: String!
	useMultipart: Boolean!
	headers: [KeyValue!]!
	uploadToken: String!
	isSuccess: Boolean!
	message: String!
}

type Task {
	"""
	Unique and stable identifier of this task
	"""
	taskId: TaskID!
	"""
	Life-cycle status of a task
	"""
	status: TaskStatus!
	"""
	Whether the task was ordered to be cancelled
	"""
	cancellationRequested: Boolean!
	"""
	Describes a certain final outcome of the task once it reaches the
	"finished" status
	"""
	outcome: TaskOutcome
	"""
	Time when task was originally created and placed in a queue
	"""
	createdAt: DateTime!
	"""
	Time when task transitioned into a running state
	"""
	ranAt: DateTime
	"""
	Time when cancellation of task was requested
	"""
	cancellationRequestedAt: DateTime
	"""
	Time when task has reached a final outcome
	"""
	finishedAt: DateTime
}

union TaskFailureReason = TaskFailureReasonGeneral | TaskFailureReasonInputDatasetCompacted

type TaskFailureReasonGeneral {
	message: String!
}

type TaskFailureReasonInputDatasetCompacted {
	inputDataset: Dataset!
	message: String!
}

scalar TaskID

"""
Describes a certain final outcome of the task
"""
union TaskOutcome = TaskOutcomeSuccess | TaskOutcomeFailed | TaskOutcomeCancelled

type TaskOutcomeCancelled {
	message: String!
}

type TaskOutcomeFailed {
	reason: TaskFailureReason!
}

type TaskOutcomeSuccess {
	message: String!
}

"""
Life-cycle status of a task
"""
enum TaskStatus {
	"""
	Task is waiting for capacity to be allocated to it
	"""
	QUEUED
	"""
	Task is being executed
	"""
	RUNNING
	"""
	Task has reached a certain final outcome (see [`TaskOutcome`])
	"""
	FINISHED
}

"""
Temporary Flink-specific extension for creating temporal tables from
streams.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#temporaltable-schema
"""
type TemporalTable {
	"""
	Name of the dataset to be converted into a temporal table.
	"""
	name: String!
	"""
	Column names used as the primary key for creating a table.
	"""
	primaryKey: [String!]!
}

type TimeDelta {
	every: Int!
	unit: TimeUnit!
}

input TimeDeltaInput {
	every: Int!
	unit: TimeUnit!
}

enum TimeUnit {
	MINUTES
	HOURS
	DAYS
	WEEKS
}

"""
Engine-specific processing queries that shape the resulting data.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#transform-schema
"""
union Transform = TransformSql

type TransformInput {
	datasetRef: DatasetRef!
	alias: String!
	inputDataset: TransformInputDataset!
}

"""
Describes a derivative transformation input

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#transforminput-schema
"""
interface TransformInputDataset {
	message: String!
}

type TransformInputDatasetAccessible implements TransformInputDataset {
	dataset: Dataset!
	message: String!
}

type TransformInputDatasetNotAccessible implements TransformInputDataset {
	datasetRef: DatasetRef!
	message: String!
}

"""
Transform using one of the SQL dialects.

See: https://github.com/kamu-data/open-data-fabric/blob/master/open-data-fabric.md#transformsql-schema
"""
type TransformSql {
	engine: String!
	version: String
	queries: [SqlQueryStep!]!
	temporalTables: [TemporalTable!]
}

interface TriggerFlowResult {
	message: String!
}

type TriggerFlowSuccess implements TriggerFlowResult {
	flow: Flow!
	message: String!
}

scalar URL

interface UnsetRoleResult {
	message: String!
}

type UnsetRoleResultSuccess implements UnsetRoleResult {
	message: String!
}

type UpdateEmailNonUnique implements UpdateEmailResult {
	message: String!
}

interface UpdateEmailResult {
	message: String!
}

type UpdateEmailSuccess implements UpdateEmailResult {
	newEmail: String!
	message: String!
}

interface UpdateReadmeResult {
	message: String!
}

type UpdateVersionErrorCasFailed implements UpdateVersionResult {
	expectedHead: Multihash!
	actualHead: Multihash!
	isSuccess: Boolean!
	message: String!
}

type UpdateVersionErrorInvalidExtraData implements UpdateVersionResult {
	message: String!
	isSuccess: Boolean!
}

interface UpdateVersionResult {
	isSuccess: Boolean!
	message: String!
}

type UpdateVersionSuccess implements UpdateVersionResult {
	newVersion: Int!
	oldHead: Multihash!
	newHead: Multihash!
	contentHash: Multihash!
	isSuccess: Boolean!
	message: String!
}

interface UpdateWebhookSubscriptionResult {
	message: String!
}

type UpdateWebhookSubscriptionResultSuccess implements UpdateWebhookSubscriptionResult {
	updated: Boolean!
	message: String!
}

type UpdateWebhookSubscriptionResultUnexpected implements UpdateWebhookSubscriptionResult {
	status: WebhookSubscriptionStatus!
	message: String!
}

interface UpsertDatasetEnvVarResult {
	message: String!
}

type UpsertDatasetEnvVarResultCreated implements UpsertDatasetEnvVarResult {
	envVar: ViewDatasetEnvVar!
	message: String!
}

type UpsertDatasetEnvVarResultUpdated implements UpsertDatasetEnvVarResult {
	envVar: ViewDatasetEnvVar!
	message: String!
}

type UpsertDatasetEnvVarUpToDate implements UpsertDatasetEnvVarResult {
	message: String!
}

"""
URL is a String implementing the [URL Standard](http://url.spec.whatwg.org/)
"""
scalar Url

type VersionedFile {
	"""
	Returns list of versions in reverse chronological order
	"""
	versions(
		"""
		Upper version bound (inclusive)
		"""
		maxVersion: Int,		page: Int,		perPage: Int
	): VersionedFileEntryConnection!
	"""
	Returns the latest version entry, if any
	"""
	latest: VersionedFileEntry
	"""
	Returns the specified entry by block or version number
	"""
	asOf(version: Int, blockHash: Multihash): VersionedFileEntry
}

type VersionedFileContentDownload {
	"""
	Direct download URL
	"""
	url: String!
	"""
	Headers to include in the request
	"""
	headers: [KeyValue!]!
	"""
	Download URL expiration timestamp
	"""
	expiresAt: DateTime
}

type VersionedFileEntry {
	"""
	System time when this version was created/updated
	"""
	systemTime: DateTime!
	"""
	Event time when this version was created/updated
	"""
	eventTime: DateTime!
	"""
	File version
	"""
	version: Int!
	"""
	Media type of the file content
	"""
	contentType: String!
	"""
	Size of the content in bytes
	"""
	contentLength: Int!
	"""
	Multihash of the file content
	"""
	contentHash: Multihash!
	"""
	Extra data associated with this file version
	"""
	extraData: ExtraData!
	"""
	Returns encoded content in-band. Should be used for small files only and
	will return an error if called on large data.
	"""
	content: Base64Usnp!
	"""
	Returns a direct download URL
	"""
	contentUrl: VersionedFileContentDownload!
}

type VersionedFileEntryConnection {
	"""
	A shorthand for `edges { node { ... } }`
	"""
	nodes: [VersionedFileEntry!]!
	"""
	Approximate number of total nodes
	"""
	totalCount: Int!
	"""
	Page information
	"""
	pageInfo: PageBasedInfo!
	edges: [VersionedFileEntryEdge!]!
}

type VersionedFileEntryEdge {
	node: VersionedFileEntry!
}

type VersionedFileMut {
	"""
	Uploads a new version of content in-band. Can be used for very small
	files only.
	"""
	uploadNewVersion(
		"""
		Base64-encoded file content (url-safe, no padding)
		"""
		content: Base64Usnp!,
		"""
		Media type of content (e.g. application/pdf)
		"""
		contentType: String,
		"""
		Json object containing values of extra columns
		"""
		extraData: ExtraData,
		"""
		Expected head block hash to prevent concurrent updates
		"""
		expectedHead: Multihash
	): UpdateVersionResult!
	"""
	Returns a pre-signed URL and upload token for direct uploads of large
	files
	"""
	startUploadNewVersion(
		"""
		Size of the file being uploaded
		"""
		contentLength: Int!,
		"""
		Media type of content (e.g. application/pdf)
		"""
		contentType: String
	): StartUploadVersionResult!
	"""
	Finalizes the content upload by incorporating the content into the
	dataset as a new version
	"""
	finishUploadNewVersion(
		"""
		Token received when starting the upload
		"""
		uploadToken: String!,
		"""
		Json object containing values of extra columns
		"""
		extraData: ExtraData,
		"""
		Expected head block hash to prevent concurrent updates
		"""
		expectedHead: Multihash
	): UpdateVersionResult!
	"""
	Creating a new version with that has updated values of extra columns but
	with the file content unchanged
	"""
	updateExtraData(
		"""
		Json object containing values of extra columns
		"""
		extraData: ExtraData!,
		"""
		Expected head block hash to prevent concurrent updates
		"""
		expectedHead: Multihash
	): UpdateVersionResult!
}

type ViewAccessToken {
	"""
	Unique identifier of the access token
	"""
	id: AccessTokenID!
	"""
	Name of the access token
	"""
	name: String!
	"""
	Date of token creation
	"""
	createdAt: DateTime!
	"""
	Date of token revocation
	"""
	revokedAt: DateTime
	"""
	Access token account owner
	"""
	account: Account!
}

type ViewDatasetEnvVar {
	"""
	Unique identifier of the dataset environment variable
	"""
	id: DatasetEnvVarID!
	"""
	Key of the dataset environment variable
	"""
	key: String!
	"""
	Non secret value of dataset environment variable
	"""
	value: String
	"""
	Date of the dataset environment variable creation
	"""
	createdAt: DateTime!
	isSecret: Boolean!
}

type ViewDatasetEnvVarConnection {
	"""
	A shorthand for `edges { node { ... } }`
	"""
	nodes: [ViewDatasetEnvVar!]!
	"""
	Approximate number of total nodes
	"""
	totalCount: Int!
	"""
	Page information
	"""
	pageInfo: PageBasedInfo!
	edges: [ViewDatasetEnvVarEdge!]!
}

type ViewDatasetEnvVarEdge {
	node: ViewDatasetEnvVar!
}

type WebSocketProtocolDesc {
	url: String!
}

scalar WebhookEventType

type WebhookSubscription {
	"""
	Unique identifier of the webhook subscription
	"""
	id: WebhookSubscriptionID!
	"""
	Optional label for the subscription. Maybe an empty string.
	"""
	label: String!
	"""
	Associated dataset ID
	Not present for system subscriptions
	"""
	datasetId: DatasetID
	"""
	Target URL for the webhook
	"""
	targetUrl: String!
	"""
	List of events that trigger the webhook
	"""
	eventTypes: [String!]!
	"""
	Status of the subscription
	"""
	status: WebhookSubscriptionStatus!
}

type WebhookSubscriptionDuplicateLabel implements CreateWebhookSubscriptionResult & UpdateWebhookSubscriptionResult {
	label: String!
	message: String!
}

scalar WebhookSubscriptionID

input WebhookSubscriptionInput {
	targetUrl: URL!
	eventTypes: [WebhookEventType!]!
	label: WebhookSubscriptionLabel!
}

type WebhookSubscriptionInvalidTargetUrl implements CreateWebhookSubscriptionResult & UpdateWebhookSubscriptionResult {
	innerMessage: String!
	message: String!
}

scalar WebhookSubscriptionLabel

type WebhookSubscriptionMut {
	update(input: WebhookSubscriptionInput!): UpdateWebhookSubscriptionResult!
	pause: PauseWebhookSubscriptionResult!
	resume: ResumeWebhookSubscriptionResult!
	remove: RemoveWebhookSubscriptionResult!
}

type WebhookSubscriptionNoEventTypesProvided implements CreateWebhookSubscriptionResult & UpdateWebhookSubscriptionResult {
	numEventTypes: Int!
	message: String!
}

enum WebhookSubscriptionStatus {
	UNVERIFIED
	ENABLED
	PAUSED
	UNREACHABLE
	REMOVED
}

type Webhooks {
	"""
	List of supported event types
	"""
	eventTypes: [String!]!
}

"""
Directs the executor to include this field or fragment only when the `if` argument is true.
"""
directive @include(if: Boolean!) on FIELD | FRAGMENT_SPREAD | INLINE_FRAGMENT
"""
Indicates that an Input Object is a OneOf Input Object (and thus requires exactly one of its field be provided)
"""
directive @oneOf on INPUT_OBJECT
"""
Directs the executor to skip this field or fragment when the `if` argument is true.
"""
directive @skip(if: Boolean!) on FIELD | FRAGMENT_SPREAD | INLINE_FRAGMENT
"""
Provides a scalar specification URL for specifying the behavior of custom scalar types.
"""
directive @specifiedBy(url: String!) on SCALAR
schema {
	query: Query
	mutation: Mutation
}

# THIS IS AN AUTOGENERATED FILE. DO NOT EDIT THIS FILE DIRECTLY.
#
# To regenerate this schema from existing code, use the following command:
# ```shell
# make codegen-graphql-schema
# ```

directive @oneOf on INPUT_OBJECT

type Account {
	"""
	Unique and stable identifier of this account
	"""
	id: AccountID!
	"""
	Symbolic account name
	"""
	accountName: AccountName!
	"""
	Account name to display
	"""
	displayName: AccountDisplayName!
	"""
	Account type
	"""
	accountType: AccountType!
	"""
	Avatar URL
	"""
	avatarUrl: String
	"""
	Indicates the administrator status
	"""
	isAdmin: Boolean!
}

scalar AccountDisplayName

scalar AccountID

scalar AccountName

enum AccountType {
	USER
	ORGANIZATION
}

type Accounts {
	"""
	Returns account by its ID
	"""
	byId(accountId: AccountID!): Account
	"""
	Returns account by its name
	"""
	byName(name: AccountName!): Account
}

type AddData {
	prevCheckpoint: Multihash
	prevOffset: Int
	newData: DataSlice
	newCheckpoint: Checkpoint
	newWatermark: DateTime
	newSourceState: SourceState
}

type AddPushSource {
	sourceName: String!
	read: ReadStep!
	preprocess: Transform
	merge: MergeStrategy!
}

type Admin {
	selfTest: String!
}

type AttachmentEmbedded {
	path: String!
	content: String!
}

union Attachments = AttachmentsEmbedded

type AttachmentsEmbedded {
	items: [AttachmentEmbedded!]!
}

type Auth {
	enabledLoginMethods: [String!]!
}

type AuthMut {
	login(loginMethod: String!, loginCredentialsJson: String!): LoginResponse!
	accountDetails(accessToken: String!): Account!
}

input BatchingConditionInput {
	minRecordsToAwait: Int!
	maxBatchingInterval: TimeDeltaInput!
}

type BlockRef {
	name: String!
	blockHash: Multihash!
}


interface CancelScheduledTasksResult {
	message: String!
}

type CancelScheduledTasksSuccess implements CancelScheduledTasksResult {
	flow: Flow!
	message: String!
}

type Checkpoint {
	physicalHash: Multihash!
	size: Int!
}

type CliProtocolDesc {
	pullCommand: String!
	pushCommand: String!
}

interface CommitResult {
	message: String!
}

type CommitResultAppendError implements CommitResult & UpdateReadmeResult {
	message: String!
}

type CommitResultSuccess implements CommitResult & UpdateReadmeResult {
	oldHead: Multihash
	newHead: Multihash!
	message: String!
}

enum CompressionFormat {
	GZIP
	ZIP
}

interface CreateDatasetFromSnapshotResult {
	message: String!
}

interface CreateDatasetResult {
	message: String!
}

type CreateDatasetResultInvalidSnapshot implements CreateDatasetFromSnapshotResult {
	message: String!
}

type CreateDatasetResultMissingInputs implements CreateDatasetFromSnapshotResult {
	missingInputs: [String!]!
	message: String!
}

type CreateDatasetResultNameCollision implements CreateDatasetResult & CreateDatasetFromSnapshotResult {
	accountName: AccountName
	datasetName: DatasetName!
	message: String!
}

type CreateDatasetResultSuccess implements CreateDatasetResult & CreateDatasetFromSnapshotResult {
	dataset: Dataset!
	message: String!
}

type Cron5ComponentExpression {
	cron5ComponentExpression: String!
}

type DataBatch {
	format: DataBatchFormat!
	content: String!
	numRecords: Int!
}

enum DataBatchFormat {
	JSON
	JSON_LD
	JSON_SOA
	CSV
}

type DataQueries {
	"""
	Executes a specified query and returns its result
	"""
	query(query: String!, queryDialect: QueryDialect!, dataFormat: DataBatchFormat, schemaFormat: DataSchemaFormat, skip: Int, limit: Int): DataQueryResult!
	"""
	Lists engines known to the system and recommended for use
	"""
	knownEngines: [EngineDesc!]!
}

union DataQueryResult = DataQueryResultSuccess | DataQueryResultError

type DataQueryResultError {
	errorMessage: String!
	errorKind: DataQueryResultErrorKind!
}

enum DataQueryResultErrorKind {
	INVALID_SQL
	UNAUTHORIZED
	INTERNAL_ERROR
}

type DataQueryResultSuccess {
	schema: DataSchema
	data: DataBatch!
	limit: Int!
}

type DataSchema {
	format: DataSchemaFormat!
	content: String!
}

enum DataSchemaFormat {
	PARQUET
	PARQUET_JSON
}

type DataSlice {
	logicalHash: Multihash!
	physicalHash: Multihash!
	offsetInterval: OffsetInterval!
	size: Int!
}

type Dataset {
	"""
	Unique identifier of the dataset
	"""
	id: DatasetID!
	"""
	Symbolic name of the dataset.
	Name can change over the dataset's lifetime. For unique identifier use
	`id()`.
	"""
	name: DatasetName!
	"""
	Returns the user or organization that owns this dataset
	"""
	owner: Account!
	"""
	Returns dataset alias (user + name)
	"""
	alias: DatasetAlias!
	"""
	Returns the kind of a dataset (Root or Derivative)
	"""
	kind: DatasetKind!
	"""
	Access to the data of the dataset
	"""
	data: DatasetData!
	"""
	Access to the metadata of the dataset
	"""
	metadata: DatasetMetadata!
	"""
	Access to the flow configurations of this dataset
	"""
	flows: DatasetFlows!
	"""
	Creation time of the first metadata block in the chain
	"""
	createdAt: DateTime!
	"""
	Creation time of the most recent metadata block in the chain
	"""
	lastUpdatedAt: DateTime!
	"""
	Permissions of the current user
	"""
	permissions: DatasetPermissions!
	"""
	Various endpoints for interacting with data
	"""
	endpoints: DatasetEndpoints!
}

scalar DatasetAlias

type DatasetConnection {
	"""
	A shorthand for `edges { node { ... } }`
	"""
	nodes: [Dataset!]!
	"""
	Approximate number of total nodes
	"""
	totalCount: Int!
	"""
	Page information
	"""
	pageInfo: PageBasedInfo!
	edges: [DatasetEdge!]!
}

type DatasetData {
	"""
	Total number of records in this dataset
	"""
	numRecordsTotal: Int!
	"""
	An estimated size of data on disk not accounting for replication or
	caching
	"""
	estimatedSize: Int!
	"""
	Returns the specified number of the latest records in the dataset
	This is equivalent to SQL query like:
	
	```text
	select * from (
	select
	*
	from dataset
	order by offset desc
	limit lim
	offset skip
	)
	order by offset
	```
	"""
	tail(skip: Int, limit: Int, dataFormat: DataBatchFormat, schemaFormat: DataSchemaFormat): DataQueryResult!
}

type DatasetEdge {
	node: Dataset!
}

type DatasetEndpoints {
	webLink: LinkProtocolDesc!
	cli: CliProtocolDesc!
	rest: RestProtocolDesc!
	flightsql: FlightSqlDesc!
	jdbc: JdbcDesc!
	postgresql: PostgreSqlDesl!
	kafka: KafkaProtocolDesc!
	websocket: WebSocketProtocolDesc!
	odata: OdataProtocolDesc!
}

type DatasetFlowConfigs {
	"""
	Returns defined configuration for a flow of specified type
	"""
	byType(datasetFlowType: DatasetFlowType!): FlowConfiguration
	"""
	Checks if all configs of this dataset are disabled
	"""
	allPaused: Boolean!
}

type DatasetFlowConfigsMut {
	setConfigSchedule(datasetFlowType: DatasetFlowType!, paused: Boolean!, schedule: ScheduleInput!): SetFlowConfigResult!
	setConfigBatching(datasetFlowType: DatasetFlowType!, paused: Boolean!, batching: BatchingConditionInput!): SetFlowConfigResult!
	pauseFlows(datasetFlowType: DatasetFlowType): Boolean!
	resumeFlows(datasetFlowType: DatasetFlowType): Boolean!
}

input DatasetFlowFilters {
	byFlowType: DatasetFlowType
	byStatus: FlowStatus
	byInitiator: InitiatorFilterInput
}

type DatasetFlowRuns {
	getFlow(flowId: FlowID!): GetFlowResult!
	listFlows(page: Int, perPage: Int, filters: DatasetFlowFilters): FlowConnection!
}

type DatasetFlowRunsMut {
	triggerFlow(datasetFlowType: DatasetFlowType!): TriggerFlowResult!
	cancelScheduledTasks(flowId: FlowID!): CancelScheduledTasksResult!
}

enum DatasetFlowType {
	INGEST
	EXECUTE_TRANSFORM
	COMPACTION
}

type DatasetFlows {
	"""
	Returns interface for flow configurations queries
	"""
	configs: DatasetFlowConfigs!
	"""
	Returns interface for flow runs queries
	"""
	runs: DatasetFlowRuns!
}

type DatasetFlowsMut {
	configs: DatasetFlowConfigsMut!
	runs: DatasetFlowRunsMut!
}

scalar DatasetID

enum DatasetKind {
	ROOT
	DERIVATIVE
}

type DatasetMetadata {
	"""
	Access to the temporal metadata chain of the dataset
	"""
	chain: MetadataChain!
	"""
	Last recorded watermark
	"""
	currentWatermark: DateTime
	"""
	Latest data schema
	"""
	currentSchema(format: DataSchemaFormat): DataSchema
	"""
	Current upstream dependencies of a dataset
	"""
	currentUpstreamDependencies: [Dataset!]!
	"""
	Current downstream dependencies of a dataset
	"""
	currentDownstreamDependencies: [Dataset!]!
	"""
	Current polling source used by the root dataset
	"""
	currentPollingSource: SetPollingSource
	"""
	Current push sources used by the root dataset
	"""
	currentPushSources: [AddPushSource!]!
	"""
	Current transformation used by the derivative dataset
	"""
	currentTransform: SetTransform
	"""
	Current descriptive information about the dataset
	"""
	currentInfo: SetInfo!
	"""
	Current readme file as discovered from attachments associated with the
	dataset
	"""
	currentReadme: String
	"""
	Current license associated with the dataset
	"""
	currentLicense: SetLicense
	"""
	Current vocabulary associated with the dataset
	"""
	currentVocab: SetVocab
}

type DatasetMetadataMut {
	"""
	Access to the mutable metadata chain of the dataset
	"""
	chain: MetadataChainMut!
	"""
	Updates or clears the dataset readme
	"""
	updateReadme(content: String): UpdateReadmeResult!
}

type DatasetMut {
	"""
	Access to the mutable metadata of the dataset
	"""
	metadata: DatasetMetadataMut!
	"""
	Access to the mutable flow configurations of this dataset
	"""
	flows: DatasetFlowsMut!
	"""
	Rename the dataset
	"""
	rename(newName: DatasetName!): RenameResult!
	"""
	Delete the dataset
	"""
	delete: DeleteResult!
	"""
	Manually advances the watermark of a root dataset
	"""
	setWatermark(watermark: DateTime!): SetWatermarkResult!
}

scalar DatasetName

type DatasetPermissions {
	canView: Boolean!
	canDelete: Boolean!
	canRename: Boolean!
	canCommit: Boolean!
	canSchedule: Boolean!
}

scalar DatasetRef

type Datasets {
	"""
	Returns dataset by its ID
	"""
	byId(datasetId: DatasetID!): Dataset
	"""
	Returns dataset by its owner and name
	"""
	byOwnerAndName(accountName: AccountName!, datasetName: DatasetName!): Dataset
	"""
	Returns datasets belonging to the specified account
	"""
	byAccountId(accountId: AccountID!, page: Int, perPage: Int): DatasetConnection!
	"""
	Returns datasets belonging to the specified account
	"""
	byAccountName(accountName: AccountName!, page: Int, perPage: Int): DatasetConnection!
}

type DatasetsMut {
	"""
	Returns a mutable dataset by its ID
	"""
	byId(datasetId: DatasetID!): DatasetMut
	"""
	Creates a new empty dataset
	"""
	createEmpty(datasetKind: DatasetKind!, datasetAlias: DatasetAlias!): CreateDatasetResult!
	"""
	Creates a new dataset from provided DatasetSnapshot manifest
	"""
	createFromSnapshot(snapshot: String!, snapshotFormat: MetadataManifestFormat!): CreateDatasetFromSnapshotResult!
}

"""
Implement the DateTime<Utc> scalar

The input/output is a string in RFC3339 format.
"""
scalar DateTime

interface DeleteResult {
	message: String!
}

type DeleteResultDanglingReference implements DeleteResult {
	notDeletedDataset: DatasetAlias!
	danglingChildRefs: [DatasetRef!]!
	message: String!
}

type DeleteResultSuccess implements DeleteResult {
	deletedDataset: DatasetAlias!
	message: String!
}

type DisablePollingSource {
	dummy: String
}

type DisablePushSource {
	sourceName: String!
}

type EngineDesc {
	"""
	A short name of the engine, e.g. "Spark", "Flink".
	Intended for use in UI for quick engine identification and selection.
	"""
	name: String!
	"""
	Language and dialect this engine is using for queries
	Indented for configuring code highlighting and completions.
	"""
	dialect: QueryDialect!
	"""
	OCI image repository and a tag of the latest engine image, e.g.
	"ghcr.io/kamu-data/engine-datafusion:0.1.2"
	"""
	latestImage: String!
}

type EnvVar {
	name: String!
	value: String
}

scalar EventID

union EventTimeSource = EventTimeSourceFromMetadata | EventTimeSourceFromPath | EventTimeSourceFromSystemTime

type EventTimeSourceFromMetadata {
	dummy: String
}

type EventTimeSourceFromPath {
	pattern: String!
	timestampFormat: String
}

type EventTimeSourceFromSystemTime {
	dummy: String
}

type ExecuteTransform {
	queryInputs: [ExecuteTransformInput!]!
	prevCheckpoint: Multihash
	prevOffset: Int
	newData: DataSlice
	newCheckpoint: Checkpoint
	newWatermark: DateTime
}

type ExecuteTransformInput {
	datasetId: DatasetID!
	prevBlockHash: Multihash
	newBlockHash: Multihash
	prevOffset: Int
	newOffset: Int
}

union FetchStep = FetchStepUrl | FetchStepFilesGlob | FetchStepContainer

type FetchStepContainer {
	image: String!
	command: [String!]
	args: [String!]
	env: [EnvVar!]
}

type FetchStepFilesGlob {
	path: String!
	eventTime: EventTimeSource
	cache: SourceCaching
	order: SourceOrdering
}

type FetchStepUrl {
	url: String!
	eventTime: EventTimeSource
	cache: SourceCaching
	headers: [RequestHeader!]
}

type FlightSqlDesc {
	url: String!
}


type Flow {
	"""
	Unique identifier of the flow
	"""
	flowId: FlowID!
	"""
	Description of key flow parameters
	"""
	description: FlowDescription!
	"""
	Status of the flow
	"""
	status: FlowStatus!
	"""
	Outcome of the flow (Finished state only)
	"""
	outcome: FlowOutcome
	"""
	Timing records associated with the flow lifecycle
	"""
	timing: FlowTimingRecords!
	"""
	Associated tasks
	"""
	tasks: [Task!]!
	"""
	History of flow events
	"""
	history: [FlowEvent!]!
	"""
	A user, who initiated the flow run. None for system-initiated flows
	"""
	initiator: Account
	"""
	Primary flow trigger
	"""
	primaryTrigger: FlowTrigger!
	"""
	Start condition
	"""
	startCondition: FlowStartCondition
}

type FlowConfiguration {
	paused: Boolean!
	schedule: FlowConfigurationSchedule
	batching: FlowConfigurationBatching
}

type FlowConfigurationBatching {
	minRecordsToAwait: Int!
	maxBatchingInterval: TimeDelta!
}

union FlowConfigurationSchedule = TimeDelta | Cron5ComponentExpression

type FlowConnection {
	"""
	A shorthand for `edges { node { ... } }`
	"""
	nodes: [Flow!]!
	"""
	Approximate number of total nodes
	"""
	totalCount: Int!
	"""
	Page information
	"""
	pageInfo: PageBasedInfo!
	edges: [FlowEdge!]!
}

union FlowDescription = FlowDescriptionDatasetPollingIngest | FlowDescriptionDatasetPushIngest | FlowDescriptionDatasetExecuteTransform | FlowDescriptionDatasetCompaction | FlowDescriptionSystemGC

type FlowDescriptionDatasetCompaction {
	datasetId: DatasetID!
	originalBlocksCount: Int!
	resultingBlocksCount: Int
}

type FlowDescriptionDatasetExecuteTransform {
	datasetId: DatasetID!
	transformResult: FlowDescriptionUpdateResult
}

type FlowDescriptionDatasetPollingIngest {
	datasetId: DatasetID!
	ingestResult: FlowDescriptionUpdateResult
}

type FlowDescriptionDatasetPushIngest {
	datasetId: DatasetID!
	sourceName: String
	inputRecordsCount: Int!
	ingestResult: FlowDescriptionUpdateResult
}

type FlowDescriptionSystemGC {
	dummy: Boolean!
}

type FlowDescriptionUpdateResult {
	numBlocks: Int!
	numRecords: Int!
	updatedWatermark: DateTime
}

type FlowEdge {
	node: Flow!
}

interface FlowEvent {
	eventId: EventID!
	eventTime: DateTime!
}

type FlowEventAborted implements FlowEvent {
	eventId: EventID!
	eventTime: DateTime!
}

type FlowEventInitiated implements FlowEvent {
	eventId: EventID!
	eventTime: DateTime!
	trigger: FlowTrigger!
}

type FlowEventStartConditionUpdated implements FlowEvent {
	eventId: EventID!
	eventTime: DateTime!
	startCondition: FlowStartCondition!
}

type FlowEventTaskChanged implements FlowEvent {
	eventId: EventID!
	eventTime: DateTime!
	taskId: TaskID!
	taskStatus: TaskStatus!
	task: Task!
}

type FlowEventTriggerAdded implements FlowEvent {
	eventId: EventID!
	eventTime: DateTime!
	trigger: FlowTrigger!
}

scalar FlowID

type FlowIncompatibleDatasetKind implements SetFlowConfigResult & TriggerFlowResult {
	expectedDatasetKind: DatasetKind!
	actualDatasetKind: DatasetKind!
	message: String!
}

type FlowInvalidBatchingConfig implements SetFlowConfigResult {
	reason: String!
	message: String!
}

type FlowNotFound implements GetFlowResult & CancelScheduledTasksResult {
	flowId: FlowID!
	message: String!
}

enum FlowOutcome {
	SUCCESS
	FAILED
	ABORTED
}

union FlowStartCondition = FlowStartConditionSchedule | FlowStartConditionThrottling | FlowStartConditionBatching | FlowStartConditionExecutor

type FlowStartConditionBatching {
	activeBatchingRule: FlowConfigurationBatching!
	batchingDeadline: DateTime!
	accumulatedRecordsCount: Int!
	watermarkModified: Boolean!
}

type FlowStartConditionExecutor {
	taskId: TaskID!
}

type FlowStartConditionSchedule {
	wakeUpAt: DateTime!
}

type FlowStartConditionThrottling {
	intervalSec: Int!
	wakeUpAt: DateTime!
	shiftedFrom: DateTime!
}

enum FlowStatus {
	WAITING
	RUNNING
	FINISHED
}

type FlowTimingRecords {
	"""
	Recorded time of last task scheduling
	"""
	awaitingExecutorSince: DateTime
	"""
	Recorded start of running (Running state seen at least once)
	"""
	runningSince: DateTime
	"""
	Recorded time of finish (successful or failed after retry) or abortion
	(Finished state seen at least once)
	"""
	finishedAt: DateTime
}

union FlowTrigger = FlowTriggerManual | FlowTriggerAutoPolling | FlowTriggerPush | FlowTriggerInputDatasetFlow

type FlowTriggerAutoPolling {
	dummy: Boolean!
}

type FlowTriggerInputDatasetFlow {
	dataset: Dataset!
	flowType: DatasetFlowType!
	flowId: FlowID!
}

type FlowTriggerManual {
	initiator: Account!
}

type FlowTriggerPush {
	dummy: Boolean!
}

interface GetFlowResult {
	message: String!
}

type GetFlowSuccess implements GetFlowResult {
	flow: Flow!
	message: String!
}


input InitiatorFilterInput @oneOf {
	system: Boolean
	account: AccountName
}


type JdbcDesc {
	url: String!
}

type KafkaProtocolDesc {
	url: String!
}

type LinkProtocolDesc {
	url: String!
}

type LoginResponse {
	accessToken: String!
	account: Account!
}

union MergeStrategy = MergeStrategyAppend | MergeStrategyLedger | MergeStrategySnapshot

type MergeStrategyAppend {
	dummy: String
}

type MergeStrategyLedger {
	primaryKey: [String!]!
}

type MergeStrategySnapshot {
	primaryKey: [String!]!
	compareColumns: [String!]
}

type MetadataBlockConnection {
	"""
	A shorthand for `edges { node { ... } }`
	"""
	nodes: [MetadataBlockExtended!]!
	"""
	Approximate number of total nodes
	"""
	totalCount: Int!
	"""
	Page information
	"""
	pageInfo: PageBasedInfo!
	edges: [MetadataBlockEdge!]!
}

type MetadataBlockEdge {
	node: MetadataBlockExtended!
}

type MetadataBlockExtended {
	blockHash: Multihash!
	prevBlockHash: Multihash
	systemTime: DateTime!
	author: Account!
	event: MetadataEvent!
	sequenceNumber: Int!
}

type MetadataChain {
	"""
	Returns all named metadata block references
	"""
	refs: [BlockRef!]!
	"""
	Returns a metadata block corresponding to the specified hash
	"""
	blockByHash(hash: Multihash!): MetadataBlockExtended
	"""
	Returns a metadata block corresponding to the specified hash and encoded
	in desired format
	"""
	blockByHashEncoded(hash: Multihash!, format: MetadataManifestFormat!): String
	"""
	Iterates all metadata blocks in the reverse chronological order
	"""
	blocks(page: Int, perPage: Int): MetadataBlockConnection!
}

type MetadataChainMut {
	"""
	Commits new event to the metadata chain
	"""
	commitEvent(event: String!, eventFormat: MetadataManifestFormat!): CommitResult!
}

union MetadataEvent = AddData | ExecuteTransform | Seed | SetPollingSource | SetTransform | SetVocab | SetAttachments | SetInfo | SetLicense | SetDataSchema | AddPushSource | DisablePushSource | DisablePollingSource

enum MetadataManifestFormat {
	YAML
}

type MetadataManifestMalformed implements CommitResult & CreateDatasetFromSnapshotResult {
	message: String!
}

type MetadataManifestUnsupportedVersion implements CommitResult & CreateDatasetFromSnapshotResult {
	message: String!
}

scalar Multihash

type Mutation {
	"""
	Authentication and authorization-related functionality group
	"""
	auth: AuthMut!
	"""
	Dataset-related functionality group.
	
	Datasets are historical streams of events recorded under a certain
	schema.
	"""
	datasets: DatasetsMut!
	"""
	Tasks-related functionality group.
	
	Tasks are units of work scheduled and executed by the system to query
	and process data.
	"""
	tasks: TasksMut!
}

type NoChanges implements CommitResult & UpdateReadmeResult {
	message: String!
}

type OdataProtocolDesc {
	serviceUrl: String!
	collectionUrl: String!
}

type OffsetInterval {
	start: Int!
	end: Int!
}

type PageBasedInfo {
	"""
	When paginating backwards, are there more items?
	"""
	hasPreviousPage: Boolean!
	"""
	When paginating forwards, are there more items?
	"""
	hasNextPage: Boolean!
	"""
	Index of the current page
	"""
	currentPage: Int!
	"""
	Approximate number of total pages assuming number of nodes per page
	stays the same
	"""
	totalPages: Int
}

type PostgreSqlDesl {
	url: String!
}

union PrepStep = PrepStepDecompress | PrepStepPipe

type PrepStepDecompress {
	format: CompressionFormat!
	subPath: String
}

type PrepStepPipe {
	command: [String!]!
}

type Query {
	"""
	Returns the version of the GQL API
	"""
	apiVersion: String!
	"""
	Authentication and authorization-related functionality group
	"""
	auth: Auth!
	"""
	Dataset-related functionality group.
	
	Datasets are historical streams of events recorded under a certain
	schema.
	"""
	datasets: Datasets!
	"""
	Account-related functionality group.
	
	Accounts can be individual users or organizations registered in the
	system. This groups deals with their identities and permissions.
	"""
	accounts: Accounts!
	"""
	Task-related functionality group.
	
	Tasks are units of scheduling that can perform many functions like
	ingesting new data, running dataset transformations, answering ad-hoc
	queries etc.
	"""
	tasks: Tasks!
	"""
	Search-related functionality group
	"""
	search: Search!
	"""
	Querying and data manipulations
	"""
	data: DataQueries!
	"""
	Admin-related functionality group
	"""
	admin: Admin!
}

enum QueryDialect {
	SQL_SPARK
	SQL_FLINK
	SQL_DATA_FUSION
}

union ReadStep = ReadStepCsv | ReadStepGeoJson | ReadStepEsriShapefile | ReadStepParquet | ReadStepJson | ReadStepNdJson | ReadStepNdGeoJson

type ReadStepCsv {
	schema: [String!]
	separator: String
	encoding: String
	quote: String
	escape: String
	header: Boolean
	inferSchema: Boolean
	nullValue: String
	dateFormat: String
	timestampFormat: String
}

type ReadStepEsriShapefile {
	schema: [String!]
	subPath: String
}

type ReadStepGeoJson {
	schema: [String!]
}

type ReadStepJson {
	subPath: String
	schema: [String!]
	dateFormat: String
	encoding: String
	timestampFormat: String
}

type ReadStepNdGeoJson {
	schema: [String!]
}

type ReadStepNdJson {
	schema: [String!]
	dateFormat: String
	encoding: String
	timestampFormat: String
}

type ReadStepParquet {
	schema: [String!]
}

interface RenameResult {
	message: String!
}

type RenameResultNameCollision implements RenameResult {
	collidingAlias: DatasetAlias!
	message: String!
}

type RenameResultNoChanges implements RenameResult {
	preservedName: DatasetName!
	message: String!
}

type RenameResultSuccess implements RenameResult {
	oldName: DatasetName!
	newName: DatasetName!
	message: String!
}

type RequestHeader {
	name: String!
	value: String!
}

type RestProtocolDesc {
	tailUrl: String!
	queryUrl: String!
	pushUrl: String!
}

input ScheduleInput @oneOf {
	timeDelta: TimeDeltaInput
	cron5ComponentExpression: String
}

type Search {
	"""
	Perform search across all resources
	"""
	query(query: String!, page: Int, perPage: Int): SearchResultConnection!
}

union SearchResult = Dataset

type SearchResultConnection {
	"""
	A shorthand for `edges { node { ... } }`
	"""
	nodes: [SearchResult!]!
	"""
	Approximate number of total nodes
	"""
	totalCount: Int!
	"""
	Page information
	"""
	pageInfo: PageBasedInfo!
	edges: [SearchResultEdge!]!
}

type SearchResultEdge {
	node: SearchResult!
}

type Seed {
	datasetId: DatasetID!
	datasetKind: DatasetKind!
}

type SetAttachments {
	attachments: Attachments!
}

type SetDataSchema {
	schema: DataSchema!
}

interface SetFlowConfigResult {
	message: String!
}

type SetFlowConfigSuccess implements SetFlowConfigResult {
	config: FlowConfiguration!
	message: String!
}

type SetInfo {
	description: String
	keywords: [String!]
}

type SetLicense {
	shortName: String!
	name: String!
	spdxId: String
	websiteUrl: String!
}

type SetPollingSource {
	fetch: FetchStep!
	prepare: [PrepStep!]
	read: ReadStep!
	preprocess: Transform
	merge: MergeStrategy!
}

type SetTransform {
	inputs: [TransformInput!]!
	transform: Transform!
}

type SetVocab {
	offsetColumn: String
	operationTypeColumn: String
	systemTimeColumn: String
	eventTimeColumn: String
}

type SetWatermarkIsDerivative implements SetWatermarkResult {
	message: String!
}

interface SetWatermarkResult {
	message: String!
}

type SetWatermarkUpToDate implements SetWatermarkResult {
	dummy: String!
	message: String!
}

type SetWatermarkUpdated implements SetWatermarkResult {
	newHead: Multihash!
	message: String!
}

union SourceCaching = SourceCachingForever

type SourceCachingForever {
	dummy: String
}

enum SourceOrdering {
	BY_EVENT_TIME
	BY_NAME
}

type SourceState {
	sourceName: String!
	kind: String!
	value: String!
}

type SqlQueryStep {
	alias: String
	query: String!
}


type Task {
	"""
	Unique and stable identifier of this task
	"""
	taskId: TaskID!
	"""
	Life-cycle status of a task
	"""
	status: TaskStatus!
	"""
	Whether the task was ordered to be cancelled
	"""
	cancellationRequested: Boolean!
	"""
	Describes a certain final outcome of the task once it reaches the
	"finished" status
	"""
	outcome: TaskOutcome
	"""
	Time when task was originally created and placed in a queue
	"""
	createdAt: DateTime!
	"""
	Time when task transitioned into a running state
	"""
	ranAt: DateTime
	"""
	Time when cancellation of task was requested
	"""
	cancellationRequestedAt: DateTime
	"""
	Time when task has reached a final outcome
	"""
	finishedAt: DateTime
}

type TaskConnection {
	"""
	A shorthand for `edges { node { ... } }`
	"""
	nodes: [Task!]!
	"""
	Approximate number of total nodes
	"""
	totalCount: Int!
	"""
	Page information
	"""
	pageInfo: PageBasedInfo!
	edges: [TaskEdge!]!
}

type TaskEdge {
	node: Task!
}

scalar TaskID

"""
Describes a certain final outcome of the task
"""
enum TaskOutcome {
	"""
	Task succeeded
	"""
	SUCCESS
	"""
	Task failed to complete
	"""
	FAILED
	"""
	Task was cancelled by a user
	"""
	CANCELLED
}

"""
Life-cycle status of a task
"""
enum TaskStatus {
	"""
	Task is waiting for capacity to be allocated to it
	"""
	QUEUED
	"""
	Task is being executed
	"""
	RUNNING
	"""
	Task has reached a certain final outcome (see [TaskOutcome])
	"""
	FINISHED
}

type Tasks {
	"""
	Returns current state of a given task
	"""
	getTask(taskId: TaskID!): Task
	"""
	Returns states of tasks associated with a given dataset ordered by
	creation time from newest to oldest
	"""
	listTasksByDataset(datasetId: DatasetID!, page: Int, perPage: Int): TaskConnection!
}

type TasksMut {
	"""
	Requests cancellation of the specified task
	"""
	cancelTask(taskId: TaskID!): Task!
	"""
	Schedules a task to update the specified dataset by performing polling
	ingest or a derivative transformation
	"""
	createUpdateDatasetTask(datasetId: DatasetID!): Task!
	"""
	Schedules a task to update the specified dataset by performing polling
	ingest or a derivative transformation
	"""
	createProbeTask(datasetId: DatasetID, busyTimeMs: Int, endWithOutcome: TaskOutcome): Task!
}

type TemporalTable {
	name: String!
	primaryKey: [String!]!
}

type TimeDelta {
	every: Int!
	unit: TimeUnit!
}

input TimeDeltaInput {
	every: Int!
	unit: TimeUnit!
}

enum TimeUnit {
	MINUTES
	HOURS
	DAYS
	WEEKS
}

union Transform = TransformSql

type TransformInput {
	datasetRef: DatasetRef!
	alias: String!
	dataset: Dataset!
}

type TransformSql {
	engine: String!
	version: String
	queries: [SqlQueryStep!]!
	temporalTables: [TemporalTable!]
}

interface TriggerFlowResult {
	message: String!
}

type TriggerFlowSuccess implements TriggerFlowResult {
	flow: Flow!
	message: String!
}

interface UpdateReadmeResult {
	message: String!
}

type WebSocketProtocolDesc {
	url: String!
}

schema {
	query: Query
	mutation: Mutation
}
